py : MXFP4 quantization requires triton >= 3.4.0 and kernels installed, we will default to dequantizing the model to bf16
At line:1 char:1
+ py -m cli.main eval *>&1 | Out-File -FilePath 'eval_run.log' -Encodin ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : NotSpecified: (MXFP4 quantizat...e model to bf16:String) [], RemoteException
    + FullyQualifiedErrorId : NativeCommandError
 

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|###3      | 1/3 [00:05<00:11,  5.94s/it]
Loading checkpoint shards:  67%|######6   | 2/3 [00:11<00:05,  5.75s/it]
Loading checkpoint shards: 100%|##########| 3/3 [00:13<00:00,  4.14s/it]
Loading checkpoint shards: 100%|##########| 3/3 [00:13<00:00,  4.59s/it]
2025-10-29 11:42:27,915 | INFO | gpt_oss_ws | GPT-OSS workspace model initialized
+--------------------- Traceback (most recent call last) ---------------------+
| G:\OSSC\cli\main.py:53 in eval                                              |
|                                                                             |
|   50   cfg = load_config(str(cfg_path))                                     |
|   51   from evals import report                                             |
|   52                                                                        |
| > 53   asyncio.run(report.run(task, cfg))                                   |
|   54                                                                        |
|   55                                                                        |
|   56 @app.command(name="fluency-guard")                                     |
|                                                                             |
| +------------------------------- locals -------------------------------+    |
| |      cfg = WorkspaceConfig(                                          |    |
| |                model_name='openai/gpt-oss-20b',                      |    |
| |                quantization='Mxfp4',                                 |    |
| |                device_map='auto',                                    |    |
| |                hooked_layers=[1, 5, 9, 13, 17, 21],                  |    |
| |                nvirt=4,                                              |    |
| |                residual_rank=8,                                      |    |
| |                slot_count=6,                                         |    |
| |                slot_dim=256,                                         |    |
| |                slot_iterations=1,                                    |    |
| |                enable_kv_append=True,                                |    |
| |                enable_residual_delta=True,                           |    |
| |                enable_read_probes=True,                              |    |
| |                enable_broadcast=True,                                |    |
| |                workspace_device='cpu',                               |    |
| |                retention=RetentionConfig(                            |    |
| |                    virt_kv_max_tokens_per_layer=1024,                |    |
| |                    virt_kv_ttl_steps=2048,                           |    |
| |                    spill_to_cpu=True,                                |    |
| |                    prefetch_margin=16                                |    |
| |                ),                                                    |    |
| |                log_level='INFO',                                     |    |
| |                api_host='0.0.0.0',                                   |    |
| |                api_port=8000,                                        |    |
| |                controller_entropy_floor=2.5,                         |    |
| |                controller_norm_cap=4.5,                              |    |
| |                sqlite_path='workspace_memory.sqlite',                |    |
| |                faiss_index_path='workspace_memory.faiss',            |    |
| |                memory_embedding_dim=384,                             |    |
| |                max_context_tokens=8192,                              |    |
| |                bf16_fallback=True                                    |    |
| |            )                                                         |    |
| | cfg_path = WindowsPath('configs/server.yaml')                        |    |
| |   config = None                                                      |    |
| |   report = <module 'evals.report' from 'G:\\OSSC\\evals\\report.py'> |    |
| |     task = 'fluency'                                                 |    |
| +----------------------------------------------------------------------+    |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\asyncio\runners.p |
| y:195 in run                                                                |
|                                                                             |
|   192             "asyncio.run() cannot be called from a running event loop |
|   193                                                                       |
|   194     with Runner(debug=debug, loop_factory=loop_factory) as runner:    |
| > 195         return runner.run(main)                                       |
|   196                                                                       |
|   197                                                                       |
|   198 def _cancel_all_tasks(loop):                                          |
|                                                                             |
| +------------------------------- locals -------------------------------+    |
| |        debug = None                                                  |    |
| | loop_factory = None                                                  |    |
| |         main = <coroutine object run at 0x000001EE03167480>          |    |
| |       runner = <asyncio.runners.Runner object at 0x000001EF318A8050> |    |
| +----------------------------------------------------------------------+    |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\asyncio\runners.p |
| y:118 in run                                                                |
|                                                                             |
|   115                                                                       |
|   116         self._interrupt_count = 0                                     |
|   117         try:                                                          |
| > 118             return self._loop.run_until_complete(task)                |
|   119         except exceptions.CancelledError:                             |
|   120             if self._interrupt_count > 0:                             |
|   121                 uncancel = getattr(task, "uncancel", None)            |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |        context = <_contextvars.Context object at 0x000001EF30B56880>    | |
| |           coro = <coroutine object run at 0x000001EE03167480>           | |
| |           self = <asyncio.runners.Runner object at 0x000001EF318A8050>  | |
| | sigint_handler = functools.partial(<bound method Runner._on_sigint of   | |
| |                  <asyncio.runners.Runner object at                      | |
| |                  0x000001EF318A8050>>, main_task=<Task finished         | |
| |                  name='Task-1' coro=<run() done, defined at             | |
| |                  G:\OSSC\evals\report.py:17>                            | |
| |                  exception=RuntimeError('expected m1 and m2 to have the | |
| |                  same dtype, but got: float != struct c10::BFloat16')>) | |
| |           task = <Task finished name='Task-1' coro=<run() done, defined | |
| |                  at G:\OSSC\evals\report.py:17>                         | |
| |                  exception=RuntimeError('expected m1 and m2 to have the | |
| |                  same dtype, but got: float != struct c10::BFloat16')>  | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\asyncio\base_even |
| ts.py:725 in run_until_complete                                             |
|                                                                             |
|    722         if not future.done():                                        |
|    723             raise RuntimeError('Event loop stopped before Future com |
|    724                                                                      |
| >  725         return future.result()                                       |
|    726                                                                      |
|    727     def stop(self):                                                  |
|    728         """Stop running the event loop.                              |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |   future = <Task finished name='Task-1' coro=<run() done, defined at    | |
| |            G:\OSSC\evals\report.py:17> exception=RuntimeError('expected | |
| |            m1 and m2 to have the same dtype, but got: float != struct   | |
| |            c10::BFloat16')>                                             | |
| | new_task = False                                                        | |
| |     self = <ProactorEventLoop running=False closed=True debug=False>    | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| G:\OSSC\evals\report.py:27 in run                                           |
|                                                                             |
|   24   request_ctx = GenerationRequestContext(request_id=str(time.time()),  |
|   25   for prompt in prompts:                                               |
|   26     input_ids = model.tokenizer_encode(prompt)                         |
| > 27     outputs = model.generate(request_ctx, input_ids=input_ids, max_new |
|   28     text = model.tokenizer_decode(outputs[0])                          |
|   29     print(json.dumps({"prompt": prompt, "completion": text}))          |
|   30   model.close()                                                        |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |      config = WorkspaceConfig(                                          | |
| |                   model_name='openai/gpt-oss-20b',                      | |
| |                   quantization='Mxfp4',                                 | |
| |                   device_map='auto',                                    | |
| |                   hooked_layers=[1, 5, 9, 13, 17, 21],                  | |
| |                   nvirt=4,                                              | |
| |                   residual_rank=8,                                      | |
| |                   slot_count=6,                                         | |
| |                   slot_dim=256,                                         | |
| |                   slot_iterations=1,                                    | |
| |                   enable_kv_append=True,                                | |
| |                   enable_residual_delta=True,                           | |
| |                   enable_read_probes=True,                              | |
| |                   enable_broadcast=True,                                | |
| |                   workspace_device='cpu',                               | |
| |                   retention=RetentionConfig(                            | |
| |                       virt_kv_max_tokens_per_layer=1024,                | |
| |                       virt_kv_ttl_steps=2048,                           | |
| |                       spill_to_cpu=True,                                | |
| |                       prefetch_margin=16                                | |
| |                   ),                                                    | |
| |                   log_level='INFO',                                     | |
| |                   api_host='0.0.0.0',                                   | |
| |                   api_port=8000,                                        | |
| |                   controller_entropy_floor=2.5,                         | |
| |                   controller_norm_cap=4.5,                              | |
| |                   sqlite_path='workspace_memory.sqlite',                | |
| |                   faiss_index_path='workspace_memory.faiss',            | |
| |                   memory_embedding_dim=384,                             | |
| |                   max_context_tokens=8192,                              | |
| |                   bf16_fallback=True                                    | |
| |               )                                                         | |
| |   input_ids = tensor([[64614,   277,   750,   290, 43952, 49803,  2608, | |
| |               1072,   220,    23,                                       | |
| |                        18304,    13]])                                  | |
| |       model = <gpt_oss_ws.model_wrapper.GPTOSSHookedModel object at     | |
| |               0x000001EF318A8590>                                       | |
| |      prompt = 'Summarize the persistent workspace state over 8 turns.'  | |
| |     prompts = [                                                         | |
| |                   'Summarize the persistent workspace state over 8      | |
| |               turns.',                                                  | |
| |                   'Describe retention policy effects on current virtual | |
| |               KV.'                                                      | |
| |               ]                                                         | |
| | request_ctx = GenerationRequestContext(                                 | |
| |                   request_id='1761752547.915142',                       | |
| |                   toggles=HookToggles(                                  | |
| |                       kv_append=True,                                   | |
| |                       residual_delta=True,                              | |
| |                       read_probes=True,                                 | |
| |                       broadcast=True                                    | |
| |                   ),                                                    | |
| |                   retention_overrides=None                              | |
| |               )                                                         | |
| |        task = 'fluency'                                                 | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| G:\OSSC\gpt_oss_ws\model_wrapper.py:144 in generate                         |
|                                                                             |
|   141   def generate(self, request: GenerationRequestContext, **kwargs) ->  |
|   142     from .generation import generate_with_workspace                   |
|   143                                                                       |
| > 144     return generate_with_workspace(self, request, **kwargs)           |
|   145                                                                       |
|   146   def tokenizer_encode(self, text: str, **kwargs) -> torch.Tensor:    |
|   147     return self.tokenizer(text, return_tensors="pt", **kwargs)["input |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |  kwargs = {                                                             | |
| |               'input_ids': tensor([[64614,   277,   750,   290, 43952,  | |
| |           49803,  2608,  1072,   220,    23,                            | |
| |                    18304,    13]]),                                     | |
| |               'max_new_tokens': 128                                     | |
| |           }                                                             | |
| | request = GenerationRequestContext(                                     | |
| |               request_id='1761752547.915142',                           | |
| |               toggles=HookToggles(                                      | |
| |                   kv_append=True,                                       | |
| |                   residual_delta=True,                                  | |
| |                   read_probes=True,                                     | |
| |                   broadcast=True                                        | |
| |               ),                                                        | |
| |               retention_overrides=None                                  | |
| |           )                                                             | |
| |    self = <gpt_oss_ws.model_wrapper.GPTOSSHookedModel object at         | |
| |           0x000001EF318A8590>                                           | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| G:\OSSC\gpt_oss_ws\generation.py:63 in generate_with_workspace              |
|                                                                             |
|   60     if cache is not None and isinstance(cache, tuple):                 |
|   61       cache = DynamicCache.from_legacy_cache(cache)                    |
|   62     with model.runtime_context(request.toggles):                       |
| > 63       outputs = model.model(                                           |
|   64         input_ids=step_input,                                          |
|   65         attention_mask=attention_mask,                                 |
|   66         past_key_values=cache,                                         |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |        attention_mask = tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,    | |
| |                         1]])                                            | |
| |                 cache = DynamicCache(layers=[DynamicSlidingWindowLayer, | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer])                                  | |
| |        cache_position = tensor([12])                                    | |
| |              decision = ControllerOutput(                               | |
| |                             broadcast=True,                             | |
| |                             retrieve=True,                              | |
| |                             write_memory=True,                          | |
| |                             halt=False                                  | |
| |                         )                                               | |
| |                device = device(type='cpu')                              | |
| |          eos_token_id = 200002                                          | |
| |            full_input = tensor([[64614,   277,   750,   290, 43952,     | |
| |                         49803,  2608,  1072,   220,    23,              | |
| |                                  18304,    13,   350]])                 | |
| |      generated_tokens = [tensor([350])]                                 | |
| |             input_ids = tensor([[64614,   277,   750,   290, 43952,     | |
| |                         49803,  2608,  1072,   220,    23,              | |
| |                                  18304,    13]])                        | |
| |                logits = tensor([[ 2.1094,  4.4375,  1.5547,  ...,       | |
| |                         -0.0679, -0.0757,  0.0767]],                    | |
| |                                dtype=torch.bfloat16,                    | |
| |                         grad_fn=<SliceBackward0>)                       | |
| |        max_new_tokens = 128                                             | |
| |                 model = <gpt_oss_ws.model_wrapper.GPTOSSHookedModel     | |
| |                         object at 0x000001EF318A8590>                   | |
| |             new_cache = DynamicCache(layers=[DynamicSlidingWindowLayer, | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer])                                  | |
| |            next_token = tensor([350])                                   | |
| | next_token_unsqueezed = tensor([[350]])                                 | |
| |               outputs = MoeCausalLMOutputWithPast(                      | |
| |                             loss=None,                                  | |
| |                             aux_loss=None,                              | |
| |                             logits=tensor([[[ 5.2188,  8.1875,  6.7500, | |
| |                         ..., -0.0579, -0.0835,  0.0972],                | |
| |                                  [ 5.4688,  7.5000,  4.4375,  ...,      | |
| |                         0.0281, -0.0189,  0.0410],                      | |
| |                                  [ 3.9375,  8.5000,  3.3594,  ...,      | |
| |                         0.0535, -0.0942,  0.1138],                      | |
| |                                  ...,                                   | |
| |                                  [ 5.0938,  6.4688,  3.7188,  ...,      | |
| |                         -0.0223,  0.0315, -0.0229],                     | |
| |                                  [ 7.3125, 11.2500,  4.0938,  ...,      | |
| |                         -0.1221, -0.1060,  0.0986],                     | |
| |                                  [ 2.1094,  4.4375,  1.5547,  ...,      | |
| |                         -0.0679, -0.0757,  0.0767]]],                   | |
| |                                dtype=torch.bfloat16,                    | |
| |                         grad_fn=<UnsafeViewBackward0>),                 | |
| |                             past_key_values=DynamicCache(layers=[Dynamà | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer]),                                 | |
| |                             hidden_states=None,                         | |
| |                             attentions=None,                            | |
| |                             router_logits=None                          | |
| |                         )                                               | |
| |        position_start = 12                                              | |
| |               request = GenerationRequestContext(                       | |
| |                             request_id='1761752547.915142',             | |
| |                             toggles=HookToggles(                        | |
| |                                 kv_append=True,                         | |
| |                                 residual_delta=True,                    | |
| |                                 read_probes=True,                       | |
| |                                 broadcast=True                          | |
| |                             ),                                          | |
| |                             retention_overrides=None                    | |
| |                         )                                               | |
| |                  step = 1                                               | |
| |            step_input = tensor([[350]])                                 | |
| |           step_length = 1                                               | |
| |       stream_callback = None                                            | |
| |           temperature = 0.8                                             | |
| |                 top_p = 0.95                                            | |
| |          total_length = 13                                              | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\tor |
| ch\nn\modules\module.py:1773 in _wrapped_call_impl                          |
|                                                                             |
|   1770         if self._compiled_call_impl is not None:                     |
|   1771             return self._compiled_call_impl(*args, **kwargs)  # type |
|   1772         else:                                                        |
| > 1773             return self._call_impl(*args, **kwargs)                  |
|   1774                                                                      |
|   1775     # torchrec tests the code consistency with the following code    |
|   1776     # fmt: off                                                       |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |   args = ()                                                             | |
| | kwargs = {                                                              | |
| |              'input_ids': tensor([[350]]),                              | |
| |              'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1,   | |
| |          1, 1, 1]]),                                                    | |
| |              'past_key_values':                                         | |
| |          DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer,  | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer]),                     | |
| |              'cache_position': tensor([12]),                            | |
| |              'use_cache': True                                          | |
| |          }                                                              | |
| |   self = GptOssForCausalLM(                                             | |
| |            (model): GptOssModel(                                        | |
| |              (embed_tokens): Embedding(201088, 2880,                    | |
| |          padding_idx=199999)                                            | |
| |              (layers): ModuleList(                                      | |
| |                (0-23): 24 x GptOssDecoderLayer(                         | |
| |                  (self_attn): GptOssAttention(                          | |
| |                    (q_proj): Linear(in_features=2880,                   | |
| |          out_features=4096, bias=True)                                  | |
| |                    (k_proj): Linear(in_features=2880, out_features=512, | |
| |          bias=True)                                                     | |
| |                    (v_proj): Linear(in_features=2880, out_features=512, | |
| |          bias=True)                                                     | |
| |                    (o_proj): Linear(in_features=4096,                   | |
| |          out_features=2880, bias=True)                                  | |
| |                  )                                                      | |
| |                  (mlp): GptOssMLP(                                      | |
| |                    (router): GptOssTopKRouter()                         | |
| |                    (experts): GptOssExperts()                           | |
| |                  )                                                      | |
| |                  (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)   | |
| |                  (post_attention_layernorm): GptOssRMSNorm((2880,),     | |
| |          eps=1e-05)                                                     | |
| |                )                                                        | |
| |              )                                                          | |
| |              (norm): GptOssRMSNorm((2880,), eps=1e-05)                  | |
| |              (rotary_emb): GptOssRotaryEmbedding()                      | |
| |            )                                                            | |
| |            (lm_head): Linear(in_features=2880, out_features=201088,     | |
| |          bias=False)                                                    | |
| |          )                                                              | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\tor |
| ch\nn\modules\module.py:1784 in _call_impl                                  |
|                                                                             |
|   1781         if not (self._backward_hooks or self._backward_pre_hooks or  |
|   1782                 or _global_backward_pre_hooks or _global_backward_ho |
|   1783                 or _global_forward_hooks or _global_forward_pre_hook |
| > 1784             return forward_call(*args, **kwargs)                     |
|   1785                                                                      |
|   1786         result = None                                                |
|   1787         called_always_called_hooks = set()                           |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |         args = ()                                                       | |
| | forward_call = <bound method GptOssForCausalLM.forward of               | |
| |                GptOssForCausalLM(                                       | |
| |                  (model): GptOssModel(                                  | |
| |                    (embed_tokens): Embedding(201088, 2880,              | |
| |                padding_idx=199999)                                      | |
| |                    (layers): ModuleList(                                | |
| |                      (0-23): 24 x GptOssDecoderLayer(                   | |
| |                        (self_attn): GptOssAttention(                    | |
| |                          (q_proj): Linear(in_features=2880,             | |
| |                out_features=4096, bias=True)                            | |
| |                          (k_proj): Linear(in_features=2880,             | |
| |                out_features=512, bias=True)                             | |
| |                          (v_proj): Linear(in_features=2880,             | |
| |                out_features=512, bias=True)                             | |
| |                          (o_proj): Linear(in_features=4096,             | |
| |                out_features=2880, bias=True)                            | |
| |                        )                                                | |
| |                        (mlp): GptOssMLP(                                | |
| |                          (router): GptOssTopKRouter()                   | |
| |                          (experts): GptOssExperts()                     | |
| |                        )                                                | |
| |                        (input_layernorm): GptOssRMSNorm((2880,),        | |
| |                eps=1e-05)                                               | |
| |                        (post_attention_layernorm):                      | |
| |                GptOssRMSNorm((2880,), eps=1e-05)                        | |
| |                      )                                                  | |
| |                    )                                                    | |
| |                    (norm): GptOssRMSNorm((2880,), eps=1e-05)            | |
| |                    (rotary_emb): GptOssRotaryEmbedding()                | |
| |                  )                                                      | |
| |                  (lm_head): Linear(in_features=2880,                    | |
| |                out_features=201088, bias=False)                         | |
| |                )>                                                       | |
| |       kwargs = {                                                        | |
| |                    'input_ids': tensor([[350]]),                        | |
| |                    'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1,   | |
| |                1, 1, 1, 1, 1]]),                                        | |
| |                    'past_key_values':                                   | |
| |                DynamicCache(layers=[DynamicSlidingWindowLayer,          | |
| |                DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,   | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer]),               | |
| |                    'cache_position': tensor([12]),                      | |
| |                    'use_cache': True                                    | |
| |                }                                                        | |
| |         self = GptOssForCausalLM(                                       | |
| |                  (model): GptOssModel(                                  | |
| |                    (embed_tokens): Embedding(201088, 2880,              | |
| |                padding_idx=199999)                                      | |
| |                    (layers): ModuleList(                                | |
| |                      (0-23): 24 x GptOssDecoderLayer(                   | |
| |                        (self_attn): GptOssAttention(                    | |
| |                          (q_proj): Linear(in_features=2880,             | |
| |                out_features=4096, bias=True)                            | |
| |                          (k_proj): Linear(in_features=2880,             | |
| |                out_features=512, bias=True)                             | |
| |                          (v_proj): Linear(in_features=2880,             | |
| |                out_features=512, bias=True)                             | |
| |                          (o_proj): Linear(in_features=4096,             | |
| |                out_features=2880, bias=True)                            | |
| |                        )                                                | |
| |                        (mlp): GptOssMLP(                                | |
| |                          (router): GptOssTopKRouter()                   | |
| |                          (experts): GptOssExperts()                     | |
| |                        )                                                | |
| |                        (input_layernorm): GptOssRMSNorm((2880,),        | |
| |                eps=1e-05)                                               | |
| |                        (post_attention_layernorm):                      | |
| |                GptOssRMSNorm((2880,), eps=1e-05)                        | |
| |                      )                                                  | |
| |                    )                                                    | |
| |                    (norm): GptOssRMSNorm((2880,), eps=1e-05)            | |
| |                    (rotary_emb): GptOssRotaryEmbedding()                | |
| |                  )                                                      | |
| |                  (lm_head): Linear(in_features=2880,                    | |
| |                out_features=201088, bias=False)                         | |
| |                )                                                        | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\tra |
| nsformers\utils\generic.py:940 in wrapper                                   |
|                                                                             |
|    937         return_dict_passed = kwargs.pop("return_dict", return_dict)  |
|    938         if return_dict_passed is not None:                           |
|    939             return_dict = return_dict_passed                         |
| >  940         output = func(self, *args, **kwargs)                         |
|    941         if not return_dict and not isinstance(output, tuple):        |
|    942             output = output.to_tuple()                               |
|    943         return output                                                |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |               args = ()                                                 | |
| |             kwargs = {                                                  | |
| |                          'input_ids': tensor([[350]]),                  | |
| |                          'attention_mask': tensor([[1, 1, 1, 1, 1, 1,   | |
| |                      1, 1, 1, 1, 1, 1, 1]]),                            | |
| |                          'past_key_values':                             | |
| |                      DynamicCache(layers=[DynamicSlidingWindowLayer,    | |
| |                      DynamicLayer, DynamicSlidingWindowLayer,           | |
| |                      DynamicLayer, DynamicSlidingWindowLayer,           | |
| |                      DynamicLayer, DynamicSlidingWindowLayer,           | |
| |                      DynamicLayer, DynamicSlidingWindowLayer,           | |
| |                      DynamicLayer, DynamicSlidingWindowLayer,           | |
| |                      DynamicLayer, DynamicSlidingWindowLayer,           | |
| |                      DynamicLayer, DynamicSlidingWindowLayer,           | |
| |                      DynamicLayer, DynamicSlidingWindowLayer,           | |
| |                      DynamicLayer, DynamicSlidingWindowLayer,           | |
| |                      DynamicLayer, DynamicSlidingWindowLayer,           | |
| |                      DynamicLayer, DynamicSlidingWindowLayer,           | |
| |                      DynamicLayer]),                                    | |
| |                          'cache_position': tensor([12]),                | |
| |                          'use_cache': True                              | |
| |                      }                                                  | |
| |        return_dict = True                                               | |
| | return_dict_passed = True                                               | |
| |               self = GptOssForCausalLM(                                 | |
| |                        (model): GptOssModel(                            | |
| |                          (embed_tokens): Embedding(201088, 2880,        | |
| |                      padding_idx=199999)                                | |
| |                          (layers): ModuleList(                          | |
| |                            (0-23): 24 x GptOssDecoderLayer(             | |
| |                              (self_attn): GptOssAttention(              | |
| |                                (q_proj): Linear(in_features=2880,       | |
| |                      out_features=4096, bias=True)                      | |
| |                                (k_proj): Linear(in_features=2880,       | |
| |                      out_features=512, bias=True)                       | |
| |                                (v_proj): Linear(in_features=2880,       | |
| |                      out_features=512, bias=True)                       | |
| |                                (o_proj): Linear(in_features=4096,       | |
| |                      out_features=2880, bias=True)                      | |
| |                              )                                          | |
| |                              (mlp): GptOssMLP(                          | |
| |                                (router): GptOssTopKRouter()             | |
| |                                (experts): GptOssExperts()               | |
| |                              )                                          | |
| |                              (input_layernorm): GptOssRMSNorm((2880,),  | |
| |                      eps=1e-05)                                         | |
| |                              (post_attention_layernorm):                | |
| |                      GptOssRMSNorm((2880,), eps=1e-05)                  | |
| |                            )                                            | |
| |                          )                                              | |
| |                          (norm): GptOssRMSNorm((2880,), eps=1e-05)      | |
| |                          (rotary_emb): GptOssRotaryEmbedding()          | |
| |                        )                                                | |
| |                        (lm_head): Linear(in_features=2880,              | |
| |                      out_features=201088, bias=False)                   | |
| |                      )                                                  | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\tra |
| nsformers\models\gpt_oss\modeling_gpt_oss.py:663 in forward                 |
|                                                                             |
|   660         )                                                             |
|   661                                                                       |
|   662         # decoder outputs consists of (dec_features, layer_state, dec |
| > 663         outputs: MoeModelOutputWithPast = self.model(                 |
|   664             input_ids=input_ids,                                      |
|   665             attention_mask=attention_mask,                            |
|   666             position_ids=position_ids,                                |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |       attention_mask = tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,     | |
| |                        1]])                                             | |
| |       cache_position = tensor([12])                                     | |
| |            input_ids = tensor([[350]])                                  | |
| |        inputs_embeds = None                                             | |
| |               kwargs = {}                                               | |
| |               labels = None                                             | |
| |       logits_to_keep = 0                                                | |
| | output_router_logits = False                                            | |
| |      past_key_values = DynamicCache(layers=[DynamicSlidingWindowLayer,  | |
| |                        DynamicLayer, DynamicSlidingWindowLayer,         | |
| |                        DynamicLayer, DynamicSlidingWindowLayer,         | |
| |                        DynamicLayer, DynamicSlidingWindowLayer,         | |
| |                        DynamicLayer, DynamicSlidingWindowLayer,         | |
| |                        DynamicLayer, DynamicSlidingWindowLayer,         | |
| |                        DynamicLayer, DynamicSlidingWindowLayer,         | |
| |                        DynamicLayer, DynamicSlidingWindowLayer,         | |
| |                        DynamicLayer, DynamicSlidingWindowLayer,         | |
| |                        DynamicLayer, DynamicSlidingWindowLayer,         | |
| |                        DynamicLayer, DynamicSlidingWindowLayer,         | |
| |                        DynamicLayer, DynamicSlidingWindowLayer,         | |
| |                        DynamicLayer])                                   | |
| |         position_ids = None                                             | |
| |                 self = GptOssForCausalLM(                               | |
| |                          (model): GptOssModel(                          | |
| |                            (embed_tokens): Embedding(201088, 2880,      | |
| |                        padding_idx=199999)                              | |
| |                            (layers): ModuleList(                        | |
| |                              (0-23): 24 x GptOssDecoderLayer(           | |
| |                                (self_attn): GptOssAttention(            | |
| |                                  (q_proj): Linear(in_features=2880,     | |
| |                        out_features=4096, bias=True)                    | |
| |                                  (k_proj): Linear(in_features=2880,     | |
| |                        out_features=512, bias=True)                     | |
| |                                  (v_proj): Linear(in_features=2880,     | |
| |                        out_features=512, bias=True)                     | |
| |                                  (o_proj): Linear(in_features=4096,     | |
| |                        out_features=2880, bias=True)                    | |
| |                                )                                        | |
| |                                (mlp): GptOssMLP(                        | |
| |                                  (router): GptOssTopKRouter()           | |
| |                                  (experts): GptOssExperts()             | |
| |                                )                                        | |
| |                                (input_layernorm):                       | |
| |                        GptOssRMSNorm((2880,), eps=1e-05)                | |
| |                                (post_attention_layernorm):              | |
| |                        GptOssRMSNorm((2880,), eps=1e-05)                | |
| |                              )                                          | |
| |                            )                                            | |
| |                            (norm): GptOssRMSNorm((2880,), eps=1e-05)    | |
| |                            (rotary_emb): GptOssRotaryEmbedding()        | |
| |                          )                                              | |
| |                          (lm_head): Linear(in_features=2880,            | |
| |                        out_features=201088, bias=False)                 | |
| |                        )                                                | |
| |            use_cache = True                                             | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\tor |
| ch\nn\modules\module.py:1773 in _wrapped_call_impl                          |
|                                                                             |
|   1770         if self._compiled_call_impl is not None:                     |
|   1771             return self._compiled_call_impl(*args, **kwargs)  # type |
|   1772         else:                                                        |
| > 1773             return self._call_impl(*args, **kwargs)                  |
|   1774                                                                      |
|   1775     # torchrec tests the code consistency with the following code    |
|   1776     # fmt: off                                                       |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |   args = ()                                                             | |
| | kwargs = {                                                              | |
| |              'input_ids': tensor([[350]]),                              | |
| |              'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1,   | |
| |          1, 1, 1]]),                                                    | |
| |              'position_ids': None,                                      | |
| |              'past_key_values':                                         | |
| |          DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer,  | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer]),                     | |
| |              'inputs_embeds': None,                                     | |
| |              'use_cache': True,                                         | |
| |              'output_router_logits': False,                             | |
| |              'cache_position': tensor([12])                             | |
| |          }                                                              | |
| |   self = GptOssModel(                                                   | |
| |            (embed_tokens): Embedding(201088, 2880, padding_idx=199999)  | |
| |            (layers): ModuleList(                                        | |
| |              (0-23): 24 x GptOssDecoderLayer(                           | |
| |                (self_attn): GptOssAttention(                            | |
| |                  (q_proj): Linear(in_features=2880, out_features=4096,  | |
| |          bias=True)                                                     | |
| |                  (k_proj): Linear(in_features=2880, out_features=512,   | |
| |          bias=True)                                                     | |
| |                  (v_proj): Linear(in_features=2880, out_features=512,   | |
| |          bias=True)                                                     | |
| |                  (o_proj): Linear(in_features=4096, out_features=2880,  | |
| |          bias=True)                                                     | |
| |                )                                                        | |
| |                (mlp): GptOssMLP(                                        | |
| |                  (router): GptOssTopKRouter()                           | |
| |                  (experts): GptOssExperts()                             | |
| |                )                                                        | |
| |                (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)     | |
| |                (post_attention_layernorm): GptOssRMSNorm((2880,),       | |
| |          eps=1e-05)                                                     | |
| |              )                                                          | |
| |            )                                                            | |
| |            (norm): GptOssRMSNorm((2880,), eps=1e-05)                    | |
| |            (rotary_emb): GptOssRotaryEmbedding()                        | |
| |          )                                                              | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\tor |
| ch\nn\modules\module.py:1784 in _call_impl                                  |
|                                                                             |
|   1781         if not (self._backward_hooks or self._backward_pre_hooks or  |
|   1782                 or _global_backward_pre_hooks or _global_backward_ho |
|   1783                 or _global_forward_hooks or _global_forward_pre_hook |
| > 1784             return forward_call(*args, **kwargs)                     |
|   1785                                                                      |
|   1786         result = None                                                |
|   1787         called_always_called_hooks = set()                           |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |         args = ()                                                       | |
| | forward_call = <bound method GptOssModel.forward of GptOssModel(        | |
| |                  (embed_tokens): Embedding(201088, 2880,                | |
| |                padding_idx=199999)                                      | |
| |                  (layers): ModuleList(                                  | |
| |                    (0-23): 24 x GptOssDecoderLayer(                     | |
| |                      (self_attn): GptOssAttention(                      | |
| |                        (q_proj): Linear(in_features=2880,               | |
| |                out_features=4096, bias=True)                            | |
| |                        (k_proj): Linear(in_features=2880,               | |
| |                out_features=512, bias=True)                             | |
| |                        (v_proj): Linear(in_features=2880,               | |
| |                out_features=512, bias=True)                             | |
| |                        (o_proj): Linear(in_features=4096,               | |
| |                out_features=2880, bias=True)                            | |
| |                      )                                                  | |
| |                      (mlp): GptOssMLP(                                  | |
| |                        (router): GptOssTopKRouter()                     | |
| |                        (experts): GptOssExperts()                       | |
| |                      )                                                  | |
| |                      (input_layernorm): GptOssRMSNorm((2880,),          | |
| |                eps=1e-05)                                               | |
| |                      (post_attention_layernorm): GptOssRMSNorm((2880,), | |
| |                eps=1e-05)                                               | |
| |                    )                                                    | |
| |                  )                                                      | |
| |                  (norm): GptOssRMSNorm((2880,), eps=1e-05)              | |
| |                  (rotary_emb): GptOssRotaryEmbedding()                  | |
| |                )>                                                       | |
| |       kwargs = {                                                        | |
| |                    'input_ids': tensor([[350]]),                        | |
| |                    'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1,   | |
| |                1, 1, 1, 1, 1]]),                                        | |
| |                    'position_ids': None,                                | |
| |                    'past_key_values':                                   | |
| |                DynamicCache(layers=[DynamicSlidingWindowLayer,          | |
| |                DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,   | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer]),               | |
| |                    'inputs_embeds': None,                               | |
| |                    'use_cache': True,                                   | |
| |                    'output_router_logits': False,                       | |
| |                    'cache_position': tensor([12])                       | |
| |                }                                                        | |
| |         self = GptOssModel(                                             | |
| |                  (embed_tokens): Embedding(201088, 2880,                | |
| |                padding_idx=199999)                                      | |
| |                  (layers): ModuleList(                                  | |
| |                    (0-23): 24 x GptOssDecoderLayer(                     | |
| |                      (self_attn): GptOssAttention(                      | |
| |                        (q_proj): Linear(in_features=2880,               | |
| |                out_features=4096, bias=True)                            | |
| |                        (k_proj): Linear(in_features=2880,               | |
| |                out_features=512, bias=True)                             | |
| |                        (v_proj): Linear(in_features=2880,               | |
| |                out_features=512, bias=True)                             | |
| |                        (o_proj): Linear(in_features=4096,               | |
| |                out_features=2880, bias=True)                            | |
| |                      )                                                  | |
| |                      (mlp): GptOssMLP(                                  | |
| |                        (router): GptOssTopKRouter()                     | |
| |                        (experts): GptOssExperts()                       | |
| |                      )                                                  | |
| |                      (input_layernorm): GptOssRMSNorm((2880,),          | |
| |                eps=1e-05)                                               | |
| |                      (post_attention_layernorm): GptOssRMSNorm((2880,), | |
| |                eps=1e-05)                                               | |
| |                    )                                                    | |
| |                  )                                                      | |
| |                  (norm): GptOssRMSNorm((2880,), eps=1e-05)              | |
| |                  (rotary_emb): GptOssRotaryEmbedding()                  | |
| |                )                                                        | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\tra |
| nsformers\utils\generic.py:1064 in wrapper                                  |
|                                                                             |
|   1061                         module.forward = make_capture_wrapper(module |
|   1062                         monkey_patched_layers.append((module, origin |
|   1063                                                                      |
| > 1064         outputs = func(self, *args, **kwargs)                        |
|   1065         # Restore original forward methods                           |
|   1066         for module, original_forward in monkey_patched_layers:       |
|   1067             module.forward = original_forward                        |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |              all_args = {                                               | |
| |                             'input_ids': tensor([[350]]),               | |
| |                             'attention_mask': tensor([[1, 1, 1, 1, 1,   | |
| |                         1, 1, 1, 1, 1, 1, 1, 1]]),                      | |
| |                             'position_ids': None,                       | |
| |                             'past_key_values':                          | |
| |                         DynamicCache(layers=[DynamicSlidingWindowLayer, | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer]),                                 | |
| |                             'inputs_embeds': None,                      | |
| |                             'use_cache': True,                          | |
| |                             'output_router_logits': False,              | |
| |                             'cache_position': tensor([12])              | |
| |                         }                                               | |
| |                  args = ()                                              | |
| |         capture_flags = {                                               | |
| |                             'router_logits': OutputRecorder(            | |
| |                                 target_class=<class                     | |
| |                         'transformers.models.gpt_oss.modeling_gpt_oss.à | |
| |                                 index=0,                                | |
| |                                 layer_name=None,                        | |
| |                                 class_name=None                         | |
| |                             ),                                          | |
| |                             'hidden_states': <class                     | |
| |                         'transformers.models.gpt_oss.modeling_gpt_oss.à | |
| |                             'attentions': <class                        | |
| |                         'transformers.models.gpt_oss.modeling_gpt_oss.à | |
| |                         }                                               | |
| |     collected_outputs = defaultdict(<class 'tuple'>, {})                | |
| |                kwargs = {                                               | |
| |                             'input_ids': tensor([[350]]),               | |
| |                             'attention_mask': tensor([[1, 1, 1, 1, 1,   | |
| |                         1, 1, 1, 1, 1, 1, 1, 1]]),                      | |
| |                             'position_ids': None,                       | |
| |                             'past_key_values':                          | |
| |                         DynamicCache(layers=[DynamicSlidingWindowLayer, | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer, DynamicSlidingWindowLayer,        | |
| |                         DynamicLayer]),                                 | |
| |                             'inputs_embeds': None,                      | |
| |                             'use_cache': True,                          | |
| |                             'output_router_logits': False,              | |
| |                             'cache_position': tensor([12])              | |
| |                         }                                               | |
| | monkey_patched_layers = []                                              | |
| |       recordable_keys = {                                               | |
| |                             'output_router_logits': False,              | |
| |                             'output_hidden_states': False,              | |
| |                             'output_attentions': False                  | |
| |                         }                                               | |
| |           return_dict = True                                            | |
| |                  self = GptOssModel(                                    | |
| |                           (embed_tokens): Embedding(201088, 2880,       | |
| |                         padding_idx=199999)                             | |
| |                           (layers): ModuleList(                         | |
| |                             (0-23): 24 x GptOssDecoderLayer(            | |
| |                               (self_attn): GptOssAttention(             | |
| |                                 (q_proj): Linear(in_features=2880,      | |
| |                         out_features=4096, bias=True)                   | |
| |                                 (k_proj): Linear(in_features=2880,      | |
| |                         out_features=512, bias=True)                    | |
| |                                 (v_proj): Linear(in_features=2880,      | |
| |                         out_features=512, bias=True)                    | |
| |                                 (o_proj): Linear(in_features=4096,      | |
| |                         out_features=2880, bias=True)                   | |
| |                               )                                         | |
| |                               (mlp): GptOssMLP(                         | |
| |                                 (router): GptOssTopKRouter()            | |
| |                                 (experts): GptOssExperts()              | |
| |                               )                                         | |
| |                               (input_layernorm): GptOssRMSNorm((2880,), | |
| |                         eps=1e-05)                                      | |
| |                               (post_attention_layernorm):               | |
| |                         GptOssRMSNorm((2880,), eps=1e-05)               | |
| |                             )                                           | |
| |                           )                                             | |
| |                           (norm): GptOssRMSNorm((2880,), eps=1e-05)     | |
| |                           (rotary_emb): GptOssRotaryEmbedding()         | |
| |                         )                                               | |
| |             use_cache = True                                            | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\tra |
| nsformers\models\gpt_oss\modeling_gpt_oss.py:502 in forward                 |
|                                                                             |
|   499         position_embeddings = self.rotary_emb(hidden_states, position |
|   500                                                                       |
|   501         for decoder_layer in self.layers:                             |
| > 502             hidden_states = decoder_layer(                            |
|   503                 hidden_states,                                        |
|   504                 attention_mask=causal_mask_mapping[decoder_layer.atte |
|   505                 position_ids=position_ids,                            |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |      attention_mask = tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) | |
| |      cache_position = tensor([12])                                      | |
| | causal_mask_mapping = {                                                 | |
| |                           'full_attention': tensor([[[[0., 0., 0., 0.,  | |
| |                       0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],           | |
| |                              dtype=torch.bfloat16),                     | |
| |                           'sliding_attention': tensor([[[[0., 0., 0.,   | |
| |                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],       | |
| |                              dtype=torch.bfloat16)                      | |
| |                       }                                                 | |
| |       decoder_layer = GptOssDecoderLayer(                               | |
| |                         (self_attn): GptOssAttention(                   | |
| |                           (q_proj): Linear(in_features=2880,            | |
| |                       out_features=4096, bias=True)                     | |
| |                           (k_proj): Linear(in_features=2880,            | |
| |                       out_features=512, bias=True)                      | |
| |                           (v_proj): Linear(in_features=2880,            | |
| |                       out_features=512, bias=True)                      | |
| |                           (o_proj): Linear(in_features=4096,            | |
| |                       out_features=2880, bias=True)                     | |
| |                         )                                               | |
| |                         (mlp): GptOssMLP(                               | |
| |                           (router): GptOssTopKRouter()                  | |
| |                           (experts): GptOssExperts()                    | |
| |                         )                                               | |
| |                         (input_layernorm): GptOssRMSNorm((2880,),       | |
| |                       eps=1e-05)                                        | |
| |                         (post_attention_layernorm):                     | |
| |                       GptOssRMSNorm((2880,), eps=1e-05)                 | |
| |                       )                                                 | |
| |       hidden_states = tensor([[[-0.5664, -0.1055,  1.3594,  ...,        | |
| |                       0.0913,  0.0187,  0.0125]]],                      | |
| |                              dtype=torch.bfloat16,                      | |
| |                       grad_fn=<AddBackward0>)                           | |
| |           input_ids = tensor([[350]])                                   | |
| |       inputs_embeds = tensor([[[-0.7070,  0.5938,  0.6836,  ...,        | |
| |                       0.0835,  0.0391,  0.1050]]],                      | |
| |                              dtype=torch.bfloat16,                      | |
| |                       grad_fn=<EmbeddingBackward0>)                     | |
| |              kwargs = {'output_router_logits': False}                   | |
| |         mask_kwargs = {                                                 | |
| |                           'config': GptOssConfig {                      | |
| |                         "architectures": [                              | |
| |                           "GptOssForCausalLM"                           | |
| |                         ],                                              | |
| |                         "attention_bias": true,                         | |
| |                         "attention_dropout": 0.0,                       | |
| |                         "dtype": "bfloat16",                            | |
| |                         "eos_token_id": 200002,                         | |
| |                         "experts_per_token": 4,                         | |
| |                         "head_dim": 64,                                 | |
| |                         "hidden_act": "silu",                           | |
| |                         "hidden_size": 2880,                            | |
| |                         "initial_context_length": 4096,                 | |
| |                         "initializer_range": 0.02,                      | |
| |                         "intermediate_size": 2880,                      | |
| |                         "layer_types": [                                | |
| |                           "sliding_attention",                          | |
| |                           "full_attention",                             | |
| |                           "sliding_attention",                          | |
| |                           "full_attention",                             | |
| |                           "sliding_attention",                          | |
| |                           "full_attention",                             | |
| |                           "sliding_attention",                          | |
| |                           "full_attention",                             | |
| |                           "sliding_attention",                          | |
| |                           "full_attention",                             | |
| |                           "sliding_attention",                          | |
| |                           "full_attention",                             | |
| |                           "sliding_attention",                          | |
| |                           "full_attention",                             | |
| |                           "sliding_attention",                          | |
| |                           "full_attention",                             | |
| |                           "sliding_attention",                          | |
| |                           "full_attention",                             | |
| |                           "sliding_attention",                          | |
| |                           "full_attention",                             | |
| |                           "sliding_attention",                          | |
| |                           "full_attention",                             | |
| |                           "sliding_attention",                          | |
| |                           "full_attention"                              | |
| |                         ],                                              | |
| |                         "max_position_embeddings": 131072,              | |
| |                         "model_type": "gpt_oss",                        | |
| |                         "num_attention_heads": 64,                      | |
| |                         "num_experts_per_tok": 4,                       | |
| |                         "num_hidden_layers": 24,                        | |
| |                         "num_key_value_heads": 8,                       | |
| |                         "num_local_experts": 32,                        | |
| |                         "output_router_logits": false,                  | |
| |                         "pad_token_id": 199999,                         | |
| |                         "rms_norm_eps": 1e-05,                          | |
| |                         "rope_scaling": {                               | |
| |                           "beta_fast": 32.0,                            | |
| |                           "beta_slow": 1.0,                             | |
| |                           "factor": 32.0,                               | |
| |                           "original_max_position_embeddings": 4096,     | |
| |                           "rope_type": "yarn",                          | |
| |                           "truncate": false                             | |
| |                         },                                              | |
| |                         "rope_theta": 150000,                           | |
| |                         "router_aux_loss_coef": 0.9,                    | |
| |                         "sliding_window": 128,                          | |
| |                         "swiglu_limit": 7.0,                            | |
| |                         "tie_word_embeddings": false,                   | |
| |                         "transformers_version": "4.56.2",               | |
| |                         "use_cache": true,                              | |
| |                         "vocab_size": 201088                            | |
| |                       }                                                 | |
| |                       ,                                                 | |
| |                           'input_embeds': tensor([[[-0.7070,  0.5938,   | |
| |                       0.6836,  ...,  0.0835,  0.0391,  0.1050]]],       | |
| |                              dtype=torch.bfloat16,                      | |
| |                       grad_fn=<EmbeddingBackward0>),                    | |
| |                           'attention_mask': tensor([[1, 1, 1, 1, 1, 1,  | |
| |                       1, 1, 1, 1, 1, 1, 1]]),                           | |
| |                           'cache_position': tensor([12]),               | |
| |                           'past_key_values':                            | |
| |                       DynamicCache(layers=[DynamicSlidingWindowLayer,   | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer])                                    | |
| |                       }                                                 | |
| |     past_key_values = DynamicCache(layers=[DynamicSlidingWindowLayer,   | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer])                                    | |
| | position_embeddings = (                                                 | |
| |                           tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, | |
| |                       -1.2188, -0.3887,  0.3809,                        | |
| |                                  0.8516,  1.1016,  1.2500,  1.3125,     | |
| |                       1.3359,  1.3438,  1.3438,                         | |
| |                                  1.3438,  1.3438,  1.3438,  1.3438,     | |
| |                       1.3438,  1.3438,  1.3438,                         | |
| |                                  1.3438,  1.3438,  1.3438,  1.3438,     | |
| |                       1.3438,  1.3438,  1.3438,                         | |
| |                                  1.3438,  1.3438,  1.3438,  1.3438]]],  | |
| |                       dtype=torch.bfloat16),                            | |
| |                           tensor([[[-7.2266e-01,  1.2344e+00,           | |
| |                       -7.4609e-01, -9.4922e-01,  5.7031e-01,            | |
| |                                  1.2891e+00,  1.2891e+00,  1.0391e+00,  | |
| |                       7.6953e-01,  5.0000e-01,                          | |
| |                                  3.1055e-01,  1.8652e-01,  1.0986e-01,  | |
| |                       6.2256e-02,  3.3936e-02,                          | |
| |                                  1.6968e-02,  7.3853e-03,  2.0905e-03,  | |
| |                       6.1798e-04,  4.2725e-04,                          | |
| |                                  2.9373e-04,  2.0218e-04,  1.3924e-04,  | |
| |                       9.6321e-05,  6.6280e-05,                          | |
| |                                  4.5538e-05,  3.1471e-05,  2.1696e-05,  | |
| |                       1.4961e-05,  1.0312e-05,                          | |
| |                                  7.0930e-06,  4.8876e-06]]],            | |
| |                       dtype=torch.bfloat16)                             | |
| |                       )                                                 | |
| |        position_ids = tensor([[12]])                                    | |
| |                self = GptOssModel(                                      | |
| |                         (embed_tokens): Embedding(201088, 2880,         | |
| |                       padding_idx=199999)                               | |
| |                         (layers): ModuleList(                           | |
| |                           (0-23): 24 x GptOssDecoderLayer(              | |
| |                             (self_attn): GptOssAttention(               | |
| |                               (q_proj): Linear(in_features=2880,        | |
| |                       out_features=4096, bias=True)                     | |
| |                               (k_proj): Linear(in_features=2880,        | |
| |                       out_features=512, bias=True)                      | |
| |                               (v_proj): Linear(in_features=2880,        | |
| |                       out_features=512, bias=True)                      | |
| |                               (o_proj): Linear(in_features=4096,        | |
| |                       out_features=2880, bias=True)                     | |
| |                             )                                           | |
| |                             (mlp): GptOssMLP(                           | |
| |                               (router): GptOssTopKRouter()              | |
| |                               (experts): GptOssExperts()                | |
| |                             )                                           | |
| |                             (input_layernorm): GptOssRMSNorm((2880,),   | |
| |                       eps=1e-05)                                        | |
| |                             (post_attention_layernorm):                 | |
| |                       GptOssRMSNorm((2880,), eps=1e-05)                 | |
| |                           )                                             | |
| |                         )                                               | |
| |                         (norm): GptOssRMSNorm((2880,), eps=1e-05)       | |
| |                         (rotary_emb): GptOssRotaryEmbedding()           | |
| |                       )                                                 | |
| |           use_cache = True                                              | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\tra |
| nsformers\modeling_layers.py:94 in __call__                                 |
|                                                                             |
|    91                 logger.warning_once(message)                          |
|    92                                                                       |
|    93             return self._gradient_checkpointing_func(partial(super(). |
| >  94         return super().__call__(*args, **kwargs)                      |
|    95                                                                       |
|    96                                                                       |
|    97 @auto_docstring                                                       |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |   args = (                                                              | |
| |              tensor([[[-0.5664, -0.1055,  1.3594,  ...,  0.0913,        | |
| |          0.0187,  0.0125]]],                                            | |
| |                 dtype=torch.bfloat16, grad_fn=<AddBackward0>),          | |
| |          )                                                              | |
| | kwargs = {                                                              | |
| |              'attention_mask': tensor([[[[0., 0., 0., 0., 0., 0., 0.,   | |
| |          0., 0., 0., 0., 0., 0.]]]],                                    | |
| |                 dtype=torch.bfloat16),                                  | |
| |              'position_ids': tensor([[12]]),                            | |
| |              'past_key_values':                                         | |
| |          DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer,  | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer]),                     | |
| |              'use_cache': True,                                         | |
| |              'cache_position': tensor([12]),                            | |
| |              'position_embeddings': (                                   | |
| |                  tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, -1.2188, | |
| |          -0.3887,  0.3809,                                              | |
| |                     0.8516,  1.1016,  1.2500,  1.3125,  1.3359,         | |
| |          1.3438,  1.3438,                                               | |
| |                     1.3438,  1.3438,  1.3438,  1.3438,  1.3438,         | |
| |          1.3438,  1.3438,                                               | |
| |                     1.3438,  1.3438,  1.3438,  1.3438,  1.3438,         | |
| |          1.3438,  1.3438,                                               | |
| |                     1.3438,  1.3438,  1.3438,  1.3438]]],               | |
| |          dtype=torch.bfloat16),                                         | |
| |                  tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01,       | |
| |          -9.4922e-01,  5.7031e-01,                                      | |
| |                     1.2891e+00,  1.2891e+00,  1.0391e+00,  7.6953e-01,  | |
| |          5.0000e-01,                                                    | |
| |                     3.1055e-01,  1.8652e-01,  1.0986e-01,  6.2256e-02,  | |
| |          3.3936e-02,                                                    | |
| |                     1.6968e-02,  7.3853e-03,  2.0905e-03,  6.1798e-04,  | |
| |          4.2725e-04,                                                    | |
| |                     2.9373e-04,  2.0218e-04,  1.3924e-04,  9.6321e-05,  | |
| |          6.6280e-05,                                                    | |
| |                     4.5538e-05,  3.1471e-05,  2.1696e-05,  1.4961e-05,  | |
| |          1.0312e-05,                                                    | |
| |                     7.0930e-06,  4.8876e-06]]], dtype=torch.bfloat16)   | |
| |              ),                                                         | |
| |              'output_router_logits': False                              | |
| |          }                                                              | |
| |   self = GptOssDecoderLayer(                                            | |
| |            (self_attn): GptOssAttention(                                | |
| |              (q_proj): Linear(in_features=2880, out_features=4096,      | |
| |          bias=True)                                                     | |
| |              (k_proj): Linear(in_features=2880, out_features=512,       | |
| |          bias=True)                                                     | |
| |              (v_proj): Linear(in_features=2880, out_features=512,       | |
| |          bias=True)                                                     | |
| |              (o_proj): Linear(in_features=4096, out_features=2880,      | |
| |          bias=True)                                                     | |
| |            )                                                            | |
| |            (mlp): GptOssMLP(                                            | |
| |              (router): GptOssTopKRouter()                               | |
| |              (experts): GptOssExperts()                                 | |
| |            )                                                            | |
| |            (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)         | |
| |            (post_attention_layernorm): GptOssRMSNorm((2880,),           | |
| |          eps=1e-05)                                                     | |
| |          )                                                              | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\tor |
| ch\nn\modules\module.py:1773 in _wrapped_call_impl                          |
|                                                                             |
|   1770         if self._compiled_call_impl is not None:                     |
|   1771             return self._compiled_call_impl(*args, **kwargs)  # type |
|   1772         else:                                                        |
| > 1773             return self._call_impl(*args, **kwargs)                  |
|   1774                                                                      |
|   1775     # torchrec tests the code consistency with the following code    |
|   1776     # fmt: off                                                       |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |   args = (                                                              | |
| |              tensor([[[-0.5664, -0.1055,  1.3594,  ...,  0.0913,        | |
| |          0.0187,  0.0125]]],                                            | |
| |                 dtype=torch.bfloat16, grad_fn=<AddBackward0>),          | |
| |          )                                                              | |
| | kwargs = {                                                              | |
| |              'attention_mask': tensor([[[[0., 0., 0., 0., 0., 0., 0.,   | |
| |          0., 0., 0., 0., 0., 0.]]]],                                    | |
| |                 dtype=torch.bfloat16),                                  | |
| |              'position_ids': tensor([[12]]),                            | |
| |              'past_key_values':                                         | |
| |          DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer,  | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer]),                     | |
| |              'use_cache': True,                                         | |
| |              'cache_position': tensor([12]),                            | |
| |              'position_embeddings': (                                   | |
| |                  tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, -1.2188, | |
| |          -0.3887,  0.3809,                                              | |
| |                     0.8516,  1.1016,  1.2500,  1.3125,  1.3359,         | |
| |          1.3438,  1.3438,                                               | |
| |                     1.3438,  1.3438,  1.3438,  1.3438,  1.3438,         | |
| |          1.3438,  1.3438,                                               | |
| |                     1.3438,  1.3438,  1.3438,  1.3438,  1.3438,         | |
| |          1.3438,  1.3438,                                               | |
| |                     1.3438,  1.3438,  1.3438,  1.3438]]],               | |
| |          dtype=torch.bfloat16),                                         | |
| |                  tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01,       | |
| |          -9.4922e-01,  5.7031e-01,                                      | |
| |                     1.2891e+00,  1.2891e+00,  1.0391e+00,  7.6953e-01,  | |
| |          5.0000e-01,                                                    | |
| |                     3.1055e-01,  1.8652e-01,  1.0986e-01,  6.2256e-02,  | |
| |          3.3936e-02,                                                    | |
| |                     1.6968e-02,  7.3853e-03,  2.0905e-03,  6.1798e-04,  | |
| |          4.2725e-04,                                                    | |
| |                     2.9373e-04,  2.0218e-04,  1.3924e-04,  9.6321e-05,  | |
| |          6.6280e-05,                                                    | |
| |                     4.5538e-05,  3.1471e-05,  2.1696e-05,  1.4961e-05,  | |
| |          1.0312e-05,                                                    | |
| |                     7.0930e-06,  4.8876e-06]]], dtype=torch.bfloat16)   | |
| |              ),                                                         | |
| |              'output_router_logits': False                              | |
| |          }                                                              | |
| |   self = GptOssDecoderLayer(                                            | |
| |            (self_attn): GptOssAttention(                                | |
| |              (q_proj): Linear(in_features=2880, out_features=4096,      | |
| |          bias=True)                                                     | |
| |              (k_proj): Linear(in_features=2880, out_features=512,       | |
| |          bias=True)                                                     | |
| |              (v_proj): Linear(in_features=2880, out_features=512,       | |
| |          bias=True)                                                     | |
| |              (o_proj): Linear(in_features=4096, out_features=2880,      | |
| |          bias=True)                                                     | |
| |            )                                                            | |
| |            (mlp): GptOssMLP(                                            | |
| |              (router): GptOssTopKRouter()                               | |
| |              (experts): GptOssExperts()                                 | |
| |            )                                                            | |
| |            (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)         | |
| |            (post_attention_layernorm): GptOssRMSNorm((2880,),           | |
| |          eps=1e-05)                                                     | |
| |          )                                                              | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\tor |
| ch\nn\modules\module.py:1784 in _call_impl                                  |
|                                                                             |
|   1781         if not (self._backward_hooks or self._backward_pre_hooks or  |
|   1782                 or _global_backward_pre_hooks or _global_backward_ho |
|   1783                 or _global_forward_hooks or _global_forward_pre_hook |
| > 1784             return forward_call(*args, **kwargs)                     |
|   1785                                                                      |
|   1786         result = None                                                |
|   1787         called_always_called_hooks = set()                           |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |         args = (                                                        | |
| |                    tensor([[[-0.5664, -0.1055,  1.3594,  ...,  0.0913,  | |
| |                0.0187,  0.0125]]],                                      | |
| |                       dtype=torch.bfloat16, grad_fn=<AddBackward0>),    | |
| |                )                                                        | |
| | forward_call = <bound method GptOssDecoderLayer.forward of              | |
| |                GptOssDecoderLayer(                                      | |
| |                  (self_attn): GptOssAttention(                          | |
| |                    (q_proj): Linear(in_features=2880,                   | |
| |                out_features=4096, bias=True)                            | |
| |                    (k_proj): Linear(in_features=2880, out_features=512, | |
| |                bias=True)                                               | |
| |                    (v_proj): Linear(in_features=2880, out_features=512, | |
| |                bias=True)                                               | |
| |                    (o_proj): Linear(in_features=4096,                   | |
| |                out_features=2880, bias=True)                            | |
| |                  )                                                      | |
| |                  (mlp): GptOssMLP(                                      | |
| |                    (router): GptOssTopKRouter()                         | |
| |                    (experts): GptOssExperts()                           | |
| |                  )                                                      | |
| |                  (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)   | |
| |                  (post_attention_layernorm): GptOssRMSNorm((2880,),     | |
| |                eps=1e-05)                                               | |
| |                )>                                                       | |
| |       kwargs = {                                                        | |
| |                    'attention_mask': tensor([[[[0., 0., 0., 0., 0., 0., | |
| |                0., 0., 0., 0., 0., 0., 0.]]]],                          | |
| |                       dtype=torch.bfloat16),                            | |
| |                    'position_ids': tensor([[12]]),                      | |
| |                    'past_key_values':                                   | |
| |                DynamicCache(layers=[DynamicSlidingWindowLayer,          | |
| |                DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,   | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer,                 | |
| |                DynamicSlidingWindowLayer, DynamicLayer]),               | |
| |                    'use_cache': True,                                   | |
| |                    'cache_position': tensor([12]),                      | |
| |                    'position_embeddings': (                             | |
| |                        tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531,    | |
| |                -1.2188, -0.3887,  0.3809,                               | |
| |                           0.8516,  1.1016,  1.2500,  1.3125,  1.3359,   | |
| |                1.3438,  1.3438,                                         | |
| |                           1.3438,  1.3438,  1.3438,  1.3438,  1.3438,   | |
| |                1.3438,  1.3438,                                         | |
| |                           1.3438,  1.3438,  1.3438,  1.3438,  1.3438,   | |
| |                1.3438,  1.3438,                                         | |
| |                           1.3438,  1.3438,  1.3438,  1.3438]]],         | |
| |                dtype=torch.bfloat16),                                   | |
| |                        tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01, | |
| |                -9.4922e-01,  5.7031e-01,                                | |
| |                           1.2891e+00,  1.2891e+00,  1.0391e+00,         | |
| |                7.6953e-01,  5.0000e-01,                                 | |
| |                           3.1055e-01,  1.8652e-01,  1.0986e-01,         | |
| |                6.2256e-02,  3.3936e-02,                                 | |
| |                           1.6968e-02,  7.3853e-03,  2.0905e-03,         | |
| |                6.1798e-04,  4.2725e-04,                                 | |
| |                           2.9373e-04,  2.0218e-04,  1.3924e-04,         | |
| |                9.6321e-05,  6.6280e-05,                                 | |
| |                           4.5538e-05,  3.1471e-05,  2.1696e-05,         | |
| |                1.4961e-05,  1.0312e-05,                                 | |
| |                           7.0930e-06,  4.8876e-06]]],                   | |
| |                dtype=torch.bfloat16)                                    | |
| |                    ),                                                   | |
| |                    'output_router_logits': False                        | |
| |                }                                                        | |
| |         self = GptOssDecoderLayer(                                      | |
| |                  (self_attn): GptOssAttention(                          | |
| |                    (q_proj): Linear(in_features=2880,                   | |
| |                out_features=4096, bias=True)                            | |
| |                    (k_proj): Linear(in_features=2880, out_features=512, | |
| |                bias=True)                                               | |
| |                    (v_proj): Linear(in_features=2880, out_features=512, | |
| |                bias=True)                                               | |
| |                    (o_proj): Linear(in_features=4096,                   | |
| |                out_features=2880, bias=True)                            | |
| |                  )                                                      | |
| |                  (mlp): GptOssMLP(                                      | |
| |                    (router): GptOssTopKRouter()                         | |
| |                    (experts): GptOssExperts()                           | |
| |                  )                                                      | |
| |                  (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)   | |
| |                  (post_attention_layernorm): GptOssRMSNorm((2880,),     | |
| |                eps=1e-05)                                               | |
| |                )                                                        | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\tra |
| nsformers\utils\deprecation.py:172 in wrapped_func                          |
|                                                                             |
|   169                 # DeprecationWarning is ignored by default, so we use |
|   170                 warnings.warn(message, FutureWarning, stacklevel=2)   |
|   171                                                                       |
| > 172             return func(*args, **kwargs)                              |
|   173                                                                       |
|   174         return wrapped_func                                           |
|   175                                                                       |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |                additional_message = None                                | |
| |                              args = (                                   | |
| |                                         GptOssDecoderLayer(             | |
| |                                       (self_attn): GptOssAttention(     | |
| |                                         (q_proj):                       | |
| |                                     Linear(in_features=2880,            | |
| |                                     out_features=4096, bias=True)       | |
| |                                         (k_proj):                       | |
| |                                     Linear(in_features=2880,            | |
| |                                     out_features=512, bias=True)        | |
| |                                         (v_proj):                       | |
| |                                     Linear(in_features=2880,            | |
| |                                     out_features=512, bias=True)        | |
| |                                         (o_proj):                       | |
| |                                     Linear(in_features=4096,            | |
| |                                     out_features=2880, bias=True)       | |
| |                                       )                                 | |
| |                                       (mlp): GptOssMLP(                 | |
| |                                         (router): GptOssTopKRouter()    | |
| |                                         (experts): GptOssExperts()      | |
| |                                       )                                 | |
| |                                       (input_layernorm):                | |
| |                                     GptOssRMSNorm((2880,), eps=1e-05)   | |
| |                                       (post_attention_layernorm):       | |
| |                                     GptOssRMSNorm((2880,), eps=1e-05)   | |
| |                                     ),                                  | |
| |                                         tensor([[[-0.5664, -0.1055,     | |
| |                                     1.3594,  ...,  0.0913,  0.0187,     | |
| |                                     0.0125]]],                          | |
| |                                            dtype=torch.bfloat16,        | |
| |                                     grad_fn=<AddBackward0>)             | |
| |                                     )                                   | |
| |                         func_name = 'GptOssDecoderLayer.forward'        | |
| |                   is_class_method = False                               | |
| |       is_greater_or_equal_version = False                               | |
| |                is_instance_method = True                                | |
| |                            kwargs = {                                   | |
| |                                         'attention_mask':               | |
| |                                     tensor([[[[0., 0., 0., 0., 0., 0.,  | |
| |                                     0., 0., 0., 0., 0., 0., 0.]]]],     | |
| |                                            dtype=torch.bfloat16),       | |
| |                                         'position_ids': tensor([[12]]), | |
| |                                         'past_key_values':              | |
| |                                     DynamicCache(layers=[DynamicSlidinà | |
| |                                     DynamicLayer,                       | |
| |                                     DynamicSlidingWindowLayer,          | |
| |                                     DynamicLayer,                       | |
| |                                     DynamicSlidingWindowLayer,          | |
| |                                     DynamicLayer,                       | |
| |                                     DynamicSlidingWindowLayer,          | |
| |                                     DynamicLayer,                       | |
| |                                     DynamicSlidingWindowLayer,          | |
| |                                     DynamicLayer,                       | |
| |                                     DynamicSlidingWindowLayer,          | |
| |                                     DynamicLayer,                       | |
| |                                     DynamicSlidingWindowLayer,          | |
| |                                     DynamicLayer,                       | |
| |                                     DynamicSlidingWindowLayer,          | |
| |                                     DynamicLayer,                       | |
| |                                     DynamicSlidingWindowLayer,          | |
| |                                     DynamicLayer,                       | |
| |                                     DynamicSlidingWindowLayer,          | |
| |                                     DynamicLayer,                       | |
| |                                     DynamicSlidingWindowLayer,          | |
| |                                     DynamicLayer,                       | |
| |                                     DynamicSlidingWindowLayer,          | |
| |                                     DynamicLayer]),                     | |
| |                                         'use_cache': True,              | |
| |                                         'cache_position': tensor([12]), | |
| |                                         'position_embeddings': (        | |
| |                                             tensor([[[ 1.1328, -0.5430, | |
| |                                     1.1250, -0.9531, -1.2188, -0.3887,  | |
| |                                     0.3809,                             | |
| |                                                0.8516,  1.1016,         | |
| |                                     1.2500,  1.3125,  1.3359,  1.3438,  | |
| |                                     1.3438,                             | |
| |                                                1.3438,  1.3438,         | |
| |                                     1.3438,  1.3438,  1.3438,  1.3438,  | |
| |                                     1.3438,                             | |
| |                                                1.3438,  1.3438,         | |
| |                                     1.3438,  1.3438,  1.3438,  1.3438,  | |
| |                                     1.3438,                             | |
| |                                                1.3438,  1.3438,         | |
| |                                     1.3438,  1.3438]]],                 | |
| |                                     dtype=torch.bfloat16),              | |
| |                                             tensor([[[-7.2266e-01,      | |
| |                                     1.2344e+00, -7.4609e-01,            | |
| |                                     -9.4922e-01,  5.7031e-01,           | |
| |                                                1.2891e+00,  1.2891e+00, | |
| |                                     1.0391e+00,  7.6953e-01,            | |
| |                                     5.0000e-01,                         | |
| |                                                3.1055e-01,  1.8652e-01, | |
| |                                     1.0986e-01,  6.2256e-02,            | |
| |                                     3.3936e-02,                         | |
| |                                                1.6968e-02,  7.3853e-03, | |
| |                                     2.0905e-03,  6.1798e-04,            | |
| |                                     4.2725e-04,                         | |
| |                                                2.9373e-04,  2.0218e-04, | |
| |                                     1.3924e-04,  9.6321e-05,            | |
| |                                     6.6280e-05,                         | |
| |                                                4.5538e-05,  3.1471e-05, | |
| |                                     2.1696e-05,  1.4961e-05,            | |
| |                                     1.0312e-05,                         | |
| |                                                7.0930e-06,              | |
| |                                     4.8876e-06]]],                      | |
| |                                     dtype=torch.bfloat16)               | |
| |                                         ),                              | |
| |                                         'output_router_logits': False   | |
| |                                     }                                   | |
| |                           message = None                                | |
| |                    minimum_action = <Action.NONE: 'none'>               | |
| |                          new_name = 'past_key_values'                   | |
| |                          old_name = 'past_key_value'                    | |
| |               raise_if_both_names = False                               | |
| | raise_if_greater_or_equal_version = False                               | |
| |                   version_message = 'and will be removed in version     | |
| |                                     4.58'                               | |
| |  warn_if_greater_or_equal_version = False                               | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\tra |
| nsformers\models\gpt_oss\modeling_gpt_oss.py:366 in forward                 |
|                                                                             |
|   363         residual = hidden_states                                      |
|   364         hidden_states = self.input_layernorm(hidden_states)           |
|   365         # Self Attention                                              |
| > 366         hidden_states, _ = self.self_attn(                            |
|   367             hidden_states=hidden_states,                              |
|   368             attention_mask=attention_mask,                            |
|   369             position_ids=position_ids,                                |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |      attention_mask = tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0.,    | |
| |                       0., 0., 0., 0.]]]],                               | |
| |                              dtype=torch.bfloat16)                      | |
| |      cache_position = tensor([12])                                      | |
| |       hidden_states = tensor([[[-0.3418, -0.0552,  0.5977,  ...,        | |
| |                       0.1157,  0.0238,  0.0096]]],                      | |
| |                              dtype=torch.bfloat16,                      | |
| |                       grad_fn=<ToCopyBackward0>)                        | |
| |              kwargs = {'output_router_logits': False}                   | |
| |     past_key_values = DynamicCache(layers=[DynamicSlidingWindowLayer,   | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer, DynamicSlidingWindowLayer,          | |
| |                       DynamicLayer])                                    | |
| | position_embeddings = (                                                 | |
| |                           tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, | |
| |                       -1.2188, -0.3887,  0.3809,                        | |
| |                                  0.8516,  1.1016,  1.2500,  1.3125,     | |
| |                       1.3359,  1.3438,  1.3438,                         | |
| |                                  1.3438,  1.3438,  1.3438,  1.3438,     | |
| |                       1.3438,  1.3438,  1.3438,                         | |
| |                                  1.3438,  1.3438,  1.3438,  1.3438,     | |
| |                       1.3438,  1.3438,  1.3438,                         | |
| |                                  1.3438,  1.3438,  1.3438,  1.3438]]],  | |
| |                       dtype=torch.bfloat16),                            | |
| |                           tensor([[[-7.2266e-01,  1.2344e+00,           | |
| |                       -7.4609e-01, -9.4922e-01,  5.7031e-01,            | |
| |                                  1.2891e+00,  1.2891e+00,  1.0391e+00,  | |
| |                       7.6953e-01,  5.0000e-01,                          | |
| |                                  3.1055e-01,  1.8652e-01,  1.0986e-01,  | |
| |                       6.2256e-02,  3.3936e-02,                          | |
| |                                  1.6968e-02,  7.3853e-03,  2.0905e-03,  | |
| |                       6.1798e-04,  4.2725e-04,                          | |
| |                                  2.9373e-04,  2.0218e-04,  1.3924e-04,  | |
| |                       9.6321e-05,  6.6280e-05,                          | |
| |                                  4.5538e-05,  3.1471e-05,  2.1696e-05,  | |
| |                       1.4961e-05,  1.0312e-05,                          | |
| |                                  7.0930e-06,  4.8876e-06]]],            | |
| |                       dtype=torch.bfloat16)                             | |
| |                       )                                                 | |
| |        position_ids = tensor([[12]])                                    | |
| |            residual = tensor([[[-0.5664, -0.1055,  1.3594,  ...,        | |
| |                       0.0913,  0.0187,  0.0125]]],                      | |
| |                              dtype=torch.bfloat16,                      | |
| |                       grad_fn=<AddBackward0>)                           | |
| |                self = GptOssDecoderLayer(                               | |
| |                         (self_attn): GptOssAttention(                   | |
| |                           (q_proj): Linear(in_features=2880,            | |
| |                       out_features=4096, bias=True)                     | |
| |                           (k_proj): Linear(in_features=2880,            | |
| |                       out_features=512, bias=True)                      | |
| |                           (v_proj): Linear(in_features=2880,            | |
| |                       out_features=512, bias=True)                      | |
| |                           (o_proj): Linear(in_features=4096,            | |
| |                       out_features=2880, bias=True)                     | |
| |                         )                                               | |
| |                         (mlp): GptOssMLP(                               | |
| |                           (router): GptOssTopKRouter()                  | |
| |                           (experts): GptOssExperts()                    | |
| |                         )                                               | |
| |                         (input_layernorm): GptOssRMSNorm((2880,),       | |
| |                       eps=1e-05)                                        | |
| |                         (post_attention_layernorm):                     | |
| |                       GptOssRMSNorm((2880,), eps=1e-05)                 | |
| |                       )                                                 | |
| |           use_cache = True                                              | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\tor |
| ch\nn\modules\module.py:1773 in _wrapped_call_impl                          |
|                                                                             |
|   1770         if self._compiled_call_impl is not None:                     |
|   1771             return self._compiled_call_impl(*args, **kwargs)  # type |
|   1772         else:                                                        |
| > 1773             return self._call_impl(*args, **kwargs)                  |
|   1774                                                                      |
|   1775     # torchrec tests the code consistency with the following code    |
|   1776     # fmt: off                                                       |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |   args = ()                                                             | |
| | kwargs = {                                                              | |
| |              'hidden_states': tensor([[[-0.3418, -0.0552,  0.5977,      | |
| |          ...,  0.1157,  0.0238,  0.0096]]],                             | |
| |                 dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>),       | |
| |              'attention_mask': tensor([[[[0., 0., 0., 0., 0., 0., 0.,   | |
| |          0., 0., 0., 0., 0., 0.]]]],                                    | |
| |                 dtype=torch.bfloat16),                                  | |
| |              'position_ids': tensor([[12]]),                            | |
| |              'past_key_values':                                         | |
| |          DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer,  | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer]),                     | |
| |              'use_cache': True,                                         | |
| |              'cache_position': tensor([12]),                            | |
| |              'position_embeddings': (                                   | |
| |                  tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, -1.2188, | |
| |          -0.3887,  0.3809,                                              | |
| |                     0.8516,  1.1016,  1.2500,  1.3125,  1.3359,         | |
| |          1.3438,  1.3438,                                               | |
| |                     1.3438,  1.3438,  1.3438,  1.3438,  1.3438,         | |
| |          1.3438,  1.3438,                                               | |
| |                     1.3438,  1.3438,  1.3438,  1.3438,  1.3438,         | |
| |          1.3438,  1.3438,                                               | |
| |                     1.3438,  1.3438,  1.3438,  1.3438]]],               | |
| |          dtype=torch.bfloat16),                                         | |
| |                  tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01,       | |
| |          -9.4922e-01,  5.7031e-01,                                      | |
| |                     1.2891e+00,  1.2891e+00,  1.0391e+00,  7.6953e-01,  | |
| |          5.0000e-01,                                                    | |
| |                     3.1055e-01,  1.8652e-01,  1.0986e-01,  6.2256e-02,  | |
| |          3.3936e-02,                                                    | |
| |                     1.6968e-02,  7.3853e-03,  2.0905e-03,  6.1798e-04,  | |
| |          4.2725e-04,                                                    | |
| |                     2.9373e-04,  2.0218e-04,  1.3924e-04,  9.6321e-05,  | |
| |          6.6280e-05,                                                    | |
| |                     4.5538e-05,  3.1471e-05,  2.1696e-05,  1.4961e-05,  | |
| |          1.0312e-05,                                                    | |
| |                     7.0930e-06,  4.8876e-06]]], dtype=torch.bfloat16)   | |
| |              ),                                                         | |
| |              'output_router_logits': False                              | |
| |          }                                                              | |
| |   self = GptOssAttention(                                               | |
| |            (q_proj): Linear(in_features=2880, out_features=4096,        | |
| |          bias=True)                                                     | |
| |            (k_proj): Linear(in_features=2880, out_features=512,         | |
| |          bias=True)                                                     | |
| |            (v_proj): Linear(in_features=2880, out_features=512,         | |
| |          bias=True)                                                     | |
| |            (o_proj): Linear(in_features=4096, out_features=2880,        | |
| |          bias=True)                                                     | |
| |          )                                                              | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\tor |
| ch\nn\modules\module.py:1784 in _call_impl                                  |
|                                                                             |
|   1781         if not (self._backward_hooks or self._backward_pre_hooks or  |
|   1782                 or _global_backward_pre_hooks or _global_backward_ho |
|   1783                 or _global_forward_hooks or _global_forward_pre_hook |
| > 1784             return forward_call(*args, **kwargs)                     |
|   1785                                                                      |
|   1786         result = None                                                |
|   1787         called_always_called_hooks = set()                           |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |   args = ()                                                             | |
| | kwargs = {                                                              | |
| |              'hidden_states': tensor([[[-0.3418, -0.0552,  0.5977,      | |
| |          ...,  0.1157,  0.0238,  0.0096]]],                             | |
| |                 dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>),       | |
| |              'attention_mask': tensor([[[[0., 0., 0., 0., 0., 0., 0.,   | |
| |          0., 0., 0., 0., 0., 0.]]]],                                    | |
| |                 dtype=torch.bfloat16),                                  | |
| |              'position_ids': tensor([[12]]),                            | |
| |              'past_key_values':                                         | |
| |          DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer,  | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer,                       | |
| |          DynamicSlidingWindowLayer, DynamicLayer]),                     | |
| |              'use_cache': True,                                         | |
| |              'cache_position': tensor([12]),                            | |
| |              'position_embeddings': (                                   | |
| |                  tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, -1.2188, | |
| |          -0.3887,  0.3809,                                              | |
| |                     0.8516,  1.1016,  1.2500,  1.3125,  1.3359,         | |
| |          1.3438,  1.3438,                                               | |
| |                     1.3438,  1.3438,  1.3438,  1.3438,  1.3438,         | |
| |          1.3438,  1.3438,                                               | |
| |                     1.3438,  1.3438,  1.3438,  1.3438,  1.3438,         | |
| |          1.3438,  1.3438,                                               | |
| |                     1.3438,  1.3438,  1.3438,  1.3438]]],               | |
| |          dtype=torch.bfloat16),                                         | |
| |                  tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01,       | |
| |          -9.4922e-01,  5.7031e-01,                                      | |
| |                     1.2891e+00,  1.2891e+00,  1.0391e+00,  7.6953e-01,  | |
| |          5.0000e-01,                                                    | |
| |                     3.1055e-01,  1.8652e-01,  1.0986e-01,  6.2256e-02,  | |
| |          3.3936e-02,                                                    | |
| |                     1.6968e-02,  7.3853e-03,  2.0905e-03,  6.1798e-04,  | |
| |          4.2725e-04,                                                    | |
| |                     2.9373e-04,  2.0218e-04,  1.3924e-04,  9.6321e-05,  | |
| |          6.6280e-05,                                                    | |
| |                     4.5538e-05,  3.1471e-05,  2.1696e-05,  1.4961e-05,  | |
| |          1.0312e-05,                                                    | |
| |                     7.0930e-06,  4.8876e-06]]], dtype=torch.bfloat16)   | |
| |              ),                                                         | |
| |              'output_router_logits': False                              | |
| |          }                                                              | |
| |   self = GptOssAttention(                                               | |
| |            (q_proj): Linear(in_features=2880, out_features=4096,        | |
| |          bias=True)                                                     | |
| |            (k_proj): Linear(in_features=2880, out_features=512,         | |
| |          bias=True)                                                     | |
| |            (v_proj): Linear(in_features=2880, out_features=512,         | |
| |          bias=True)                                                     | |
| |            (o_proj): Linear(in_features=4096, out_features=2880,        | |
| |          bias=True)                                                     | |
| |          )                                                              | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| G:\OSSC\gpt_oss_ws\attention_patch.py:166 in patched_forward                |
|                                                                             |
|   163         attention_mask = bound.arguments.get("attention_mask")        |
|   164         bound.arguments["attention_mask"] = _extend_mask(attention_ma |
|   165                                                                       |
| > 166       outputs = original_forward(*bound.args, **bound.kwargs)         |
|   167       model_output = HFModelOutput is not None and isinstance(outputs |
|   168       if not model_output and not isinstance(outputs, tuple):         |
|   169         return outputs                                                |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| | accepts_attention_mask = True                                           | |
| |     accepts_past_value = False                                          | |
| |    accepts_past_values = True                                           | |
| |                   args = ()                                             | |
| |         attention_mask = tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., | |
| |                          0., 0., 0., 0.]]]],                            | |
| |                                 dtype=torch.bfloat16)                   | |
| |             base_cache = DynamicCache(layers=[DynamicSlidingWindowLayeà | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer])                                 | |
| |                  bound = <BoundArguments                                | |
| |                          (hidden_states=tensor([[[-0.3418, -0.0552,     | |
| |                          0.5977,  ...,  0.1157,  0.0238,  0.0096]]],    | |
| |                                 dtype=torch.bfloat16,                   | |
| |                          grad_fn=<ToCopyBackward0>),                    | |
| |                          position_embeddings=(tensor([[[ 1.1328,        | |
| |                          -0.5430,  1.1250, -0.9531, -1.2188, -0.3887,   | |
| |                          0.3809,                                        | |
| |                                     0.8516,  1.1016,  1.2500,  1.3125,  | |
| |                          1.3359,  1.3438,  1.3438,                      | |
| |                                     1.3438,  1.3438,  1.3438,  1.3438,  | |
| |                          1.3438,  1.3438,  1.3438,                      | |
| |                                     1.3438,  1.3438,  1.3438,  1.3438,  | |
| |                          1.3438,  1.3438,  1.3438,                      | |
| |                                     1.3438,  1.3438,  1.3438,           | |
| |                          1.3438]]], dtype=torch.bfloat16),              | |
| |                          tensor([[[-7.2266e-01,  1.2344e+00,            | |
| |                          -7.4609e-01, -9.4922e-01,  5.7031e-01,         | |
| |                                     1.2891e+00,  1.2891e+00,            | |
| |                          1.0391e+00,  7.6953e-01,  5.0000e-01,          | |
| |                                     3.1055e-01,  1.8652e-01,            | |
| |                          1.0986e-01,  6.2256e-02,  3.3936e-02,          | |
| |                                     1.6968e-02,  7.3853e-03,            | |
| |                          2.0905e-03,  6.1798e-04,  4.2725e-04,          | |
| |                                     2.9373e-04,  2.0218e-04,            | |
| |                          1.3924e-04,  9.6321e-05,  6.6280e-05,          | |
| |                                     4.5538e-05,  3.1471e-05,            | |
| |                          2.1696e-05,  1.4961e-05,  1.0312e-05,          | |
| |                                     7.0930e-06,  4.8876e-06]]],         | |
| |                          dtype=torch.bfloat16)),                        | |
| |                          attention_mask=tensor([[[[0., 0., 0., 0., 0.,  | |
| |                          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,    | |
| |                          0.]]]]),                                       | |
| |                          past_key_values=<gpt_oss_ws.attention_patch._à | |
| |                          object at 0x000001F1379D5E80>,                 | |
| |                          cache_position=tensor([12]),                   | |
| |                          kwargs={'position_ids': tensor([[12]]),        | |
| |                          'use_cache': True, 'output_router_logits':     | |
| |                          False})>                                       | |
| |                 device = 'cpu'                                          | |
| |                fetched = (                                              | |
| |                              tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],  | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   ...,                                  | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]]]],   | |
| |                          dtype=torch.bfloat16,                          | |
| |                                 grad_fn=<CatBackward0>),                | |
| |                              tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],  | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   ...,                                  | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]]]],   | |
| |                          dtype=torch.bfloat16,                          | |
| |                                 grad_fn=<CatBackward0>)                 | |
| |                          )                                              | |
| |                 kwargs = {                                              | |
| |                              'hidden_states': tensor([[[-0.3418,        | |
| |                          -0.0552,  0.5977,  ...,  0.1157,  0.0238,      | |
| |                          0.0096]]],                                     | |
| |                                 dtype=torch.bfloat16,                   | |
| |                          grad_fn=<ToCopyBackward0>),                    | |
| |                              'attention_mask': tensor([[[[0., 0., 0.,   | |
| |                          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],    | |
| |                                 dtype=torch.bfloat16),                  | |
| |                              'position_ids': tensor([[12]]),            | |
| |                              'past_key_values':                         | |
| |                          DynamicCache(layers=[DynamicSlidingWindowLayeà | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer]),                                | |
| |                              'use_cache': True,                         | |
| |                              'cache_position': tensor([12]),            | |
| |                              'position_embeddings': (                   | |
| |                                  tensor([[[ 1.1328, -0.5430,  1.1250,   | |
| |                          -0.9531, -1.2188, -0.3887,  0.3809,            | |
| |                                     0.8516,  1.1016,  1.2500,  1.3125,  | |
| |                          1.3359,  1.3438,  1.3438,                      | |
| |                                     1.3438,  1.3438,  1.3438,  1.3438,  | |
| |                          1.3438,  1.3438,  1.3438,                      | |
| |                                     1.3438,  1.3438,  1.3438,  1.3438,  | |
| |                          1.3438,  1.3438,  1.3438,                      | |
| |                                     1.3438,  1.3438,  1.3438,           | |
| |                          1.3438]]], dtype=torch.bfloat16),              | |
| |                                  tensor([[[-7.2266e-01,  1.2344e+00,    | |
| |                          -7.4609e-01, -9.4922e-01,  5.7031e-01,         | |
| |                                     1.2891e+00,  1.2891e+00,            | |
| |                          1.0391e+00,  7.6953e-01,  5.0000e-01,          | |
| |                                     3.1055e-01,  1.8652e-01,            | |
| |                          1.0986e-01,  6.2256e-02,  3.3936e-02,          | |
| |                                     1.6968e-02,  7.3853e-03,            | |
| |                          2.0905e-03,  6.1798e-04,  4.2725e-04,          | |
| |                                     2.9373e-04,  2.0218e-04,            | |
| |                          1.3924e-04,  9.6321e-05,  6.6280e-05,          | |
| |                                     4.5538e-05,  3.1471e-05,            | |
| |                          2.1696e-05,  1.4961e-05,  1.0312e-05,          | |
| |                                     7.0930e-06,  4.8876e-06]]],         | |
| |                          dtype=torch.bfloat16)                          | |
| |                              ),                                         | |
| |                              'output_router_logits': False              | |
| |                          }                                              | |
| |       original_forward = <bound method GptOssAttention.forward of       | |
| |                          GptOssAttention(                               | |
| |                            (q_proj): Linear(in_features=2880,           | |
| |                          out_features=4096, bias=True)                  | |
| |                            (k_proj): Linear(in_features=2880,           | |
| |                          out_features=512, bias=True)                   | |
| |                            (v_proj): Linear(in_features=2880,           | |
| |                          out_features=512, bias=True)                   | |
| |                            (o_proj): Linear(in_features=4096,           | |
| |                          out_features=2880, bias=True)                  | |
| |                          )>                                             | |
| |                   past = DynamicCache(layers=[DynamicSlidingWindowLayeà | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer, DynamicSlidingWindowLayer,       | |
| |                          DynamicLayer])                                 | |
| |          past_arg_name = 'past_key_values'                              | |
| |                runtime = WorkspaceRuntimeState(                         | |
| |                              toggles=HookToggles(                       | |
| |                                  kv_append=True,                        | |
| |                                  residual_delta=True,                   | |
| |                                  read_probes=True,                      | |
| |                                  broadcast=True                         | |
| |                              ),                                         | |
| |                              layer_map={},                              | |
| |                              kv_fetch=<bound method                     | |
| |                          GPTOSSHookedModel._kv_fetch of                 | |
| |                          <gpt_oss_ws.model_wrapper.GPTOSSHookedModel    | |
| |                          object at 0x000001EF318A8590>>,                | |
| |                              residual_delta=<bound method               | |
| |                          GPTOSSHookedModel._residual_delta of           | |
| |                          <gpt_oss_ws.model_wrapper.GPTOSSHookedModel    | |
| |                          object at 0x000001EF318A8590>>,                | |
| |                              record_residual=<bound method              | |
| |                          GPTOSSHookedModel._record_residual of          | |
| |                          <gpt_oss_ws.model_wrapper.GPTOSSHookedModel    | |
| |                          object at 0x000001EF318A8590>>,                | |
| |                              post_attention_hook=None,                  | |
| |                              device='cpu',                              | |
| |                              slots=tensor([[[ 1.1581, -0.9775, -0.9338, | |
| |                          ...,  0.3286,  0.4728,  0.0351],               | |
| |                                   [ 0.8579,  2.5151,  0.5711,  ...,     | |
| |                          -0.9415,  1.0328,  0.6320],                    | |
| |                                   [ 0.6978, -0.1756,  0.0935,  ...,     | |
| |                          -1.2386,  0.0314,  0.7207],                    | |
| |                                   [ 0.8340, -0.1881, -1.0944,  ...,     | |
| |                          1.9440,  0.4003,  1.3985],                     | |
| |                                   [ 0.9573, -1.0251, -0.9583,  ...,     | |
| |                          0.2315,  1.0611, -2.5258],                     | |
| |                                   [ 0.0124, -0.0330,  0.2220,  ...,     | |
| |                          -0.6761, -0.2052, -0.3255]]],                  | |
| |                                 grad_fn=<AddBackward0>),                | |
| |                              entropy=5.34375                            | |
| |                          )                                              | |
| |                   self = <gpt_oss_ws.attention_patch.AttentionPatcher   | |
| |                          object at 0x000001F1379D5400>                  | |
| |              signature = <Signature (hidden_states: torch.Tensor,       | |
| |                          position_embeddings: tuple[torch.Tensor,       | |
| |                          torch.Tensor], attention_mask:                 | |
| |                          Optional[torch.Tensor], past_key_values:       | |
| |                          Optional[transformers.cache_utils.Cache] =     | |
| |                          None, cache_position:                          | |
| |                          Optional[torch.LongTensor] = None, **kwargs:   | |
| |                          Unpack[transformers.utils.generic.Transformerà | |
| |                          -> tuple[torch.Tensor, torch.Tensor]>          | |
| |             target_key = 'past_key_values'                              | |
| |                toggles = HookToggles(                                   | |
| |                              kv_append=True,                            | |
| |                              residual_delta=True,                       | |
| |                              read_probes=True,                          | |
| |                              broadcast=True                             | |
| |                          )                                              | |
| |              virtual_k = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   ...,                                  | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]]]],   | |
| |                          dtype=torch.bfloat16,                          | |
| |                                 grad_fn=<CatBackward0>)                 | |
| |              virtual_v = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   ...,                                  | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]],     | |
| |                                                                         | |
| |                                   [[0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.],      | |
| |                                    [0., 0., 0.,  ..., 0., 0., 0.]]]],   | |
| |                          dtype=torch.bfloat16,                          | |
| |                                 grad_fn=<CatBackward0>)                 | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\tra |
| nsformers\utils\deprecation.py:172 in wrapped_func                          |
|                                                                             |
|   169                 # DeprecationWarning is ignored by default, so we use |
|   170                 warnings.warn(message, FutureWarning, stacklevel=2)   |
|   171                                                                       |
| > 172             return func(*args, **kwargs)                              |
|   173                                                                       |
|   174         return wrapped_func                                           |
|   175                                                                       |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |                additional_message = None                                | |
| |                              args = (                                   | |
| |                                         GptOssAttention(                | |
| |                                       (q_proj):                         | |
| |                                     Linear(in_features=2880,            | |
| |                                     out_features=4096, bias=True)       | |
| |                                       (k_proj):                         | |
| |                                     Linear(in_features=2880,            | |
| |                                     out_features=512, bias=True)        | |
| |                                       (v_proj):                         | |
| |                                     Linear(in_features=2880,            | |
| |                                     out_features=512, bias=True)        | |
| |                                       (o_proj):                         | |
| |                                     Linear(in_features=4096,            | |
| |                                     out_features=2880, bias=True)       | |
| |                                     ),                                  | |
| |                                         tensor([[[-0.3418, -0.0552,     | |
| |                                     0.5977,  ...,  0.1157,  0.0238,     | |
| |                                     0.0096]]],                          | |
| |                                            dtype=torch.bfloat16,        | |
| |                                     grad_fn=<ToCopyBackward0>),         | |
| |                                         (                               | |
| |                                             tensor([[[ 1.1328, -0.5430, | |
| |                                     1.1250, -0.9531, -1.2188, -0.3887,  | |
| |                                     0.3809,                             | |
| |                                                0.8516,  1.1016,         | |
| |                                     1.2500,  1.3125,  1.3359,  1.3438,  | |
| |                                     1.3438,                             | |
| |                                                1.3438,  1.3438,         | |
| |                                     1.3438,  1.3438,  1.3438,  1.3438,  | |
| |                                     1.3438,                             | |
| |                                                1.3438,  1.3438,         | |
| |                                     1.3438,  1.3438,  1.3438,  1.3438,  | |
| |                                     1.3438,                             | |
| |                                                1.3438,  1.3438,         | |
| |                                     1.3438,  1.3438]]],                 | |
| |                                     dtype=torch.bfloat16),              | |
| |                                             tensor([[[-7.2266e-01,      | |
| |                                     1.2344e+00, -7.4609e-01,            | |
| |                                     -9.4922e-01,  5.7031e-01,           | |
| |                                                1.2891e+00,  1.2891e+00, | |
| |                                     1.0391e+00,  7.6953e-01,            | |
| |                                     5.0000e-01,                         | |
| |                                                3.1055e-01,  1.8652e-01, | |
| |                                     1.0986e-01,  6.2256e-02,            | |
| |                                     3.3936e-02,                         | |
| |                                                1.6968e-02,  7.3853e-03, | |
| |                                     2.0905e-03,  6.1798e-04,            | |
| |                                     4.2725e-04,                         | |
| |                                                2.9373e-04,  2.0218e-04, | |
| |                                     1.3924e-04,  9.6321e-05,            | |
| |                                     6.6280e-05,                         | |
| |                                                4.5538e-05,  3.1471e-05, | |
| |                                     2.1696e-05,  1.4961e-05,            | |
| |                                     1.0312e-05,                         | |
| |                                                7.0930e-06,              | |
| |                                     4.8876e-06]]],                      | |
| |                                     dtype=torch.bfloat16)               | |
| |                                         ),                              | |
| |                                         tensor([[[[0., 0., 0., 0., 0.,  | |
| |                                     0., 0., 0., 0., 0., 0., 0., 0., 0., | |
| |                                     0., 0., 0.]]]]),                    | |
| |                                         <gpt_oss_ws.attention_patch._Và | |
| |                                     object at 0x000001F1379D5E80>,      | |
| |                                         tensor([12])                    | |
| |                                     )                                   | |
| |                         func_name = 'GptOssAttention.forward'           | |
| |                   is_class_method = False                               | |
| |       is_greater_or_equal_version = False                               | |
| |                is_instance_method = True                                | |
| |                            kwargs = {                                   | |
| |                                         'position_ids': tensor([[12]]), | |
| |                                         'use_cache': True,              | |
| |                                         'output_router_logits': False   | |
| |                                     }                                   | |
| |                           message = None                                | |
| |                    minimum_action = <Action.NONE: 'none'>               | |
| |                          new_name = 'past_key_values'                   | |
| |                          old_name = 'past_key_value'                    | |
| |               raise_if_both_names = False                               | |
| | raise_if_greater_or_equal_version = False                               | |
| |                   version_message = 'and will be removed in version     | |
| |                                     4.58'                               | |
| |  warn_if_greater_or_equal_version = False                               | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\tra |
| nsformers\models\gpt_oss\modeling_gpt_oss.py:323 in forward                 |
|                                                                             |
|   320         if self.config._attn_implementation != "eager":               |
|   321             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config |
|   322                                                                       |
| > 323         attn_output, attn_weights = attention_interface(              |
|   324             self,                                                     |
|   325             query_states,                                             |
|   326             key_states,                                               |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |      attention_mask = tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0.,    | |
| |                       0., 0., 0., 0., 0., 0., 0., 0.]]]])               | |
| |        cache_kwargs = {'cache_position': tensor([12])}                  | |
| |      cache_position = tensor([12])                                      | |
| |                 cos = tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531,     | |
| |                       -1.2188, -0.3887,  0.3809,                        | |
| |                                  0.8516,  1.1016,  1.2500,  1.3125,     | |
| |                       1.3359,  1.3438,  1.3438,                         | |
| |                                  1.3438,  1.3438,  1.3438,  1.3438,     | |
| |                       1.3438,  1.3438,  1.3438,                         | |
| |                                  1.3438,  1.3438,  1.3438,  1.3438,     | |
| |                       1.3438,  1.3438,  1.3438,                         | |
| |                                  1.3438,  1.3438,  1.3438,  1.3438]]],  | |
| |                       dtype=torch.bfloat16)                             | |
| |        hidden_shape = (1, 1, -1, 64)                                    | |
| |       hidden_states = tensor([[[-0.3418, -0.0552,  0.5977,  ...,        | |
| |                       0.1157,  0.0238,  0.0096]]],                      | |
| |                              dtype=torch.bfloat16,                      | |
| |                       grad_fn=<ToCopyBackward0>)                        | |
| |         input_shape = torch.Size([1, 1])                                | |
| |          key_states = tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 ...,                                    | |
| |                                 [-2.1250e+00,  1.0391e+00, -2.9062e+00, | |
| |                       ..., -7.4158e-03,                                 | |
| |                                  -1.6797e+00, -1.0469e+00],             | |
| |                                 [-8.2812e-01,  4.8828e-02, -5.9766e-01, | |
| |                       ..., -1.2812e+00,                                 | |
| |                                   1.0859e+00,  2.2344e+00],             | |
| |                                 [-7.5781e-01,  1.1484e+00, -1.0000e+00, | |
| |                       ..., -2.1250e+00,                                 | |
| |                                  -1.7188e-01, -8.4375e-01]],            | |
| |                                                                         | |
| |                                [[ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 ...,                                    | |
| |                                 [ 1.2500e+01,  2.7188e+00,  4.7070e-01, | |
| |                       ..., -3.4375e+00,                                 | |
| |                                  -7.0312e-01, -1.9297e+00],             | |
| |                                 [-3.9648e-01, -2.2812e+00, -2.9102e-01, | |
| |                       ..., -1.6875e+00,                                 | |
| |                                  -1.0078e+00,  6.2891e-01],             | |
| |                                 [-1.2500e+01, -3.5781e+00, -5.5859e-01, | |
| |                       ..., -1.8047e+00,                                 | |
| |                                   5.2734e-01,  1.1094e+00]],            | |
| |                                                                         | |
| |                                [[ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 ...,                                    | |
| |                                 [-2.6367e-01,  2.1680e-01, -1.1328e+00, | |
| |                       ..., -8.3594e-01,                                 | |
| |                                  -2.5469e+00,  8.5156e-01],             | |
| |                                 [-1.0132e-02,  6.3965e-02, -1.8799e-02, | |
| |                       ...,  2.5938e+00,                                 | |
| |                                   2.1875e+00, -1.6357e-02],             | |
| |                                 [-8.0078e-01, -7.3828e-01, -9.2969e-01, | |
| |                       ..., -3.8086e-01,                                 | |
| |                                   5.2344e-01, -2.9219e+00]],            | |
| |                                                                         | |
| |                                ...,                                     | |
| |                                                                         | |
| |                                [[ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 ...,                                    | |
| |                                 [-1.3672e+00, -1.7969e+00, -2.2344e+00, | |
| |                       ..., -8.1250e-01,                                 | |
| |                                  -1.7734e+00, -5.6641e-01],             | |
| |                                 [-2.0996e-01,  1.1816e-01, -4.7852e-02, | |
| |                       ...,  6.2109e-01,                                 | |
| |                                  -1.5625e+00, -2.7344e+00],             | |
| |                                 [-2.0469e+00,  3.5547e-01, -2.1875e+00, | |
| |                       ..., -7.0801e-03,                                 | |
| |                                   9.0234e-01, -2.5781e+00]],            | |
| |                                                                         | |
| |                                [[ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 ...,                                    | |
| |                                 [ 3.9062e-03, -1.2812e+00, -6.9141e-01, | |
| |                       ...,  3.9844e+00,                                 | |
| |                                  -4.7812e+00,  2.6875e+00],             | |
| |                                 [ 5.6641e-01, -9.4238e-02, -1.4258e-01, | |
| |                       ..., -1.0000e+00,                                 | |
| |                                   5.0000e-01,  1.4609e+00],             | |
| |                                 [ 8.0469e-01,  2.0020e-01,  3.7695e-01, | |
| |                       ...,  3.4844e+00,                                 | |
| |                                  -3.0664e-01, -4.3750e+00]],            | |
| |                                                                         | |
| |                                [[ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 ...,                                    | |
| |                                 [-1.7734e+00,  2.3438e-01,  2.5000e+00, | |
| |                       ..., -1.2891e+00,                                 | |
| |                                  -1.6504e-01,  1.2109e+00],             | |
| |                                 [ 1.2344e+00, -3.6133e-01,  7.0312e-01, | |
| |                       ...,  6.4844e-01,                                 | |
| |                                  -1.1523e-01, -1.6094e+00],             | |
| |                                 [ 2.3906e+00, -2.2656e+00,  8.5547e-01, | |
| |                       ..., -2.8125e+00,                                 | |
| |                                   2.8281e+00, -6.8359e-01]]]],          | |
| |                       dtype=torch.bfloat16,                             | |
| |                              grad_fn=<CatBackward0>)                    | |
| |              kwargs = {                                                 | |
| |                           'position_ids': tensor([[12]]),               | |
| |                           'use_cache': True,                            | |
| |                           'output_router_logits': False                 | |
| |                       }                                                 | |
| |     past_key_values = <gpt_oss_ws.attention_patch._VirtualCacheProxy    | |
| |                       object at 0x000001F1379D5E80>                     | |
| | position_embeddings = (                                                 | |
| |                           tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, | |
| |                       -1.2188, -0.3887,  0.3809,                        | |
| |                                  0.8516,  1.1016,  1.2500,  1.3125,     | |
| |                       1.3359,  1.3438,  1.3438,                         | |
| |                                  1.3438,  1.3438,  1.3438,  1.3438,     | |
| |                       1.3438,  1.3438,  1.3438,                         | |
| |                                  1.3438,  1.3438,  1.3438,  1.3438,     | |
| |                       1.3438,  1.3438,  1.3438,                         | |
| |                                  1.3438,  1.3438,  1.3438,  1.3438]]],  | |
| |                       dtype=torch.bfloat16),                            | |
| |                           tensor([[[-7.2266e-01,  1.2344e+00,           | |
| |                       -7.4609e-01, -9.4922e-01,  5.7031e-01,            | |
| |                                  1.2891e+00,  1.2891e+00,  1.0391e+00,  | |
| |                       7.6953e-01,  5.0000e-01,                          | |
| |                                  3.1055e-01,  1.8652e-01,  1.0986e-01,  | |
| |                       6.2256e-02,  3.3936e-02,                          | |
| |                                  1.6968e-02,  7.3853e-03,  2.0905e-03,  | |
| |                       6.1798e-04,  4.2725e-04,                          | |
| |                                  2.9373e-04,  2.0218e-04,  1.3924e-04,  | |
| |                       9.6321e-05,  6.6280e-05,                          | |
| |                                  4.5538e-05,  3.1471e-05,  2.1696e-05,  | |
| |                       1.4961e-05,  1.0312e-05,                          | |
| |                                  7.0930e-06,  4.8876e-06]]],            | |
| |                       dtype=torch.bfloat16)                             | |
| |                       )                                                 | |
| |        query_states = tensor([[[[-0.1602, -0.1787, -0.2695,  ...,       | |
| |                       1.4453,  0.6602, -0.9023]],                       | |
| |                                                                         | |
| |                                [[ 0.0107,  0.0908, -0.0137,  ...,       | |
| |                       -0.0259,  1.4453,  0.3867]],                      | |
| |                                                                         | |
| |                                [[-0.0099,  0.0486,  0.0063,  ...,       | |
| |                       -1.8672,  0.2949, -0.0635]],                      | |
| |                                                                         | |
| |                                ...,                                     | |
| |                                                                         | |
| |                                [[-0.0986,  0.1348, -0.0198,  ...,       | |
| |                       5.6250,  0.6836,  1.8047]],                       | |
| |                                                                         | |
| |                                [[ 0.0811, -0.1250, -0.0508,  ...,       | |
| |                       0.5156,  0.0811,  1.3438]],                       | |
| |                                                                         | |
| |                                [[ 0.5742, -0.1641, -0.1572,  ...,       | |
| |                       2.5312, -0.4277,  1.1719]]]],                     | |
| |                              dtype=torch.bfloat16,                      | |
| |                       grad_fn=<CatBackward0>)                           | |
| |                self = GptOssAttention(                                  | |
| |                         (q_proj): Linear(in_features=2880,              | |
| |                       out_features=4096, bias=True)                     | |
| |                         (k_proj): Linear(in_features=2880,              | |
| |                       out_features=512, bias=True)                      | |
| |                         (v_proj): Linear(in_features=2880,              | |
| |                       out_features=512, bias=True)                      | |
| |                         (o_proj): Linear(in_features=4096,              | |
| |                       out_features=2880, bias=True)                     | |
| |                       )                                                 | |
| |                 sin = tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01,  | |
| |                       -9.4922e-01,  5.7031e-01,                         | |
| |                                  1.2891e+00,  1.2891e+00,  1.0391e+00,  | |
| |                       7.6953e-01,  5.0000e-01,                          | |
| |                                  3.1055e-01,  1.8652e-01,  1.0986e-01,  | |
| |                       6.2256e-02,  3.3936e-02,                          | |
| |                                  1.6968e-02,  7.3853e-03,  2.0905e-03,  | |
| |                       6.1798e-04,  4.2725e-04,                          | |
| |                                  2.9373e-04,  2.0218e-04,  1.3924e-04,  | |
| |                       9.6321e-05,  6.6280e-05,                          | |
| |                                  4.5538e-05,  3.1471e-05,  2.1696e-05,  | |
| |                       1.4961e-05,  1.0312e-05,                          | |
| |                                  7.0930e-06,  4.8876e-06]]],            | |
| |                       dtype=torch.bfloat16)                             | |
| |        value_states = tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 ...,                                    | |
| |                                 [ 9.2578e-01, -5.0391e-01, -1.3203e+00, | |
| |                       ...,  2.1562e+00,                                 | |
| |                                  -1.1953e+00, -1.7656e+00],             | |
| |                                 [-1.4844e+00, -7.2656e-01,  7.1484e-01, | |
| |                       ...,  7.1289e-02,                                 | |
| |                                  -6.4062e-01, -1.1169e-02],             | |
| |                                 [ 3.5352e-01,  2.9102e-01,  5.5078e-01, | |
| |                       ..., -6.7578e-01,                                 | |
| |                                   1.3867e-01, -7.3438e-01]],            | |
| |                                                                         | |
| |                                [[ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 ...,                                    | |
| |                                 [ 3.3447e-02,  1.7500e+00, -5.7031e-01, | |
| |                       ..., -3.1250e-01,                                 | |
| |                                  -3.3008e-01,  6.6797e-01],             | |
| |                                 [-1.0938e+00,  4.9023e-01, -6.2500e-01, | |
| |                       ..., -1.2256e-01,                                 | |
| |                                  -9.7656e-01,  2.0312e-01],             | |
| |                                 [ 6.7383e-02, -1.1084e-01,  1.3750e+00, | |
| |                       ..., -1.2891e+00,                                 | |
| |                                  -4.0430e-01,  1.6719e+00]],            | |
| |                                                                         | |
| |                                [[ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 ...,                                    | |
| |                                 [ 7.2754e-02, -3.5938e-01,  9.8438e-01, | |
| |                       ..., -2.0312e+00,                                 | |
| |                                  -1.2578e+00, -8.6719e-01],             | |
| |                                 [ 1.2891e+00, -2.5781e-01, -1.7480e-01, | |
| |                       ..., -6.1719e-01,                                 | |
| |                                   4.1797e-01, -3.2617e-01],             | |
| |                                 [-2.4121e-01,  1.3203e+00, -4.1797e-01, | |
| |                       ..., -1.9297e+00,                                 | |
| |                                   1.3379e-01, -2.7734e-01]],            | |
| |                                                                         | |
| |                                ...,                                     | |
| |                                                                         | |
| |                                [[ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 ...,                                    | |
| |                                 [ 5.6250e-01,  1.6406e-01,  2.6855e-03, | |
| |                       ...,  4.6484e-01,                                 | |
| |                                   8.2031e-01, -1.0938e+00],             | |
| |                                 [ 1.0156e-01, -2.2852e-01,  8.9844e-01, | |
| |                       ..., -2.8711e-01,                                 | |
| |                                   1.3770e-01, -8.1250e-01],             | |
| |                                 [-1.7383e-01, -8.8672e-01, -1.0986e-01, | |
| |                       ..., -8.4766e-01,                                 | |
| |                                  -2.8711e-01, -2.3438e-01]],            | |
| |                                                                         | |
| |                                [[ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 ...,                                    | |
| |                                 [ 2.7969e+00,  1.0693e-01, -1.3672e+00, | |
| |                       ..., -2.8125e+00,                                 | |
| |                                   1.5747e-02,  1.7422e+00],             | |
| |                                 [-4.6484e-01,  1.5137e-01, -4.9609e-01, | |
| |                       ...,  2.8906e-01,                                 | |
| |                                   3.3789e-01, -6.8750e-01],             | |
| |                                 [ 1.9062e+00, -4.0039e-02, -1.4609e+00, | |
| |                       ...,  1.7383e-01,                                 | |
| |                                   3.1055e-01,  6.5625e-01]],            | |
| |                                                                         | |
| |                                [[ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00, | |
| |                       ...,  0.0000e+00,                                 | |
| |                                   0.0000e+00,  0.0000e+00],             | |
| |                                 ...,                                    | |
| |                                 [ 1.1094e+00, -1.9409e-02, -5.1953e-01, | |
| |                       ...,  1.6211e-01,                                 | |
| |                                   1.2109e+00,  3.8281e-01],             | |
| |                                 [-1.2500e+00, -4.7852e-01,  1.3477e-01, | |
| |                       ...,  1.3379e-01,                                 | |
| |                                  -7.1094e-01,  2.1582e-01],             | |
| |                                 [ 1.3125e+00,  7.5781e-01,  6.7578e-01, | |
| |                       ..., -3.6377e-02,                                 | |
| |                                  -1.2061e-01,  2.6367e-01]]]],          | |
| |                       dtype=torch.bfloat16,                             | |
| |                              grad_fn=<CatBackward0>)                    | |
| +-------------------------------------------------------------------------+ |
|                                                                             |
| C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\tra |
| nsformers\models\gpt_oss\modeling_gpt_oss.py:263 in eager_attention_forward |
|                                                                             |
|   260     probs = F.softmax(combined_logits, dim=-1, dtype=combined_logits. |
|   261     scores = probs[..., :-1]  # we drop the sink here                 |
|   262     attn_weights = nn.functional.dropout(scores, p=dropout, training= |
| > 263     attn_output = torch.matmul(attn_weights, value_states)            |
|   264     attn_output = attn_output.transpose(1, 2).contiguous()            |
|   265     return attn_output, attn_weights                                  |
|   266                                                                       |
|                                                                             |
| +-------------------------------- locals ---------------------------------+ |
| |  attention_mask = tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,    | |
| |                   0., 0., 0., 0., 0., 0., 0.]]]])                       | |
| |    attn_weights = tensor([[[[8.0390e-03, 8.0390e-03, 8.0390e-03,  ...,  | |
| |                   1.9897e-02,                                           | |
| |                              2.2196e-02, 5.0413e-02]],                  | |
| |                                                                         | |
| |                            [[2.2359e-02, 2.2359e-02, 2.2359e-02,  ...,  | |
| |                   1.4602e-04,                                           | |
| |                              2.9916e-01, 1.7740e-02]],                  | |
| |                                                                         | |
| |                            [[2.3835e-02, 2.3835e-02, 2.3835e-02,  ...,  | |
| |                   1.6478e-02,                                           | |
| |                              1.8171e-01, 1.9342e-01]],                  | |
| |                                                                         | |
| |                            ...,                                         | |
| |                                                                         | |
| |                            [[5.8761e-03, 5.8761e-03, 5.8761e-03,  ...,  | |
| |                   1.5906e-04,                                           | |
| |                              6.4168e-02, 2.6224e-04]],                  | |
| |                                                                         | |
| |                            [[1.3439e-02, 1.3439e-02, 1.3439e-02,  ...,  | |
| |                   7.8554e-02,                                           | |
| |                              6.5124e-02, 1.5408e-02]],                  | |
| |                                                                         | |
| |                            [[2.7297e-02, 2.7297e-02, 2.7297e-02,  ...,  | |
| |                   4.3871e-03,                                           | |
| |                              8.0231e-02, 4.3871e-03]]]],                | |
| |                   grad_fn=<SliceBackward0>)                             | |
| |     causal_mask = tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,    | |
| |                   0., 0., 0., 0., 0., 0., 0.]]]])                       | |
| | combined_logits = tensor([[[[-3.9844, -3.9844, -3.9844,  ..., -2.9688,  | |
| |                   -2.1484,  0.0000]],                                   | |
| |                                                                         | |
| |                            [[-3.2656, -3.2656, -3.2656,  ..., -0.6719,  | |
| |                   -3.4971,  0.0000]],                                   | |
| |                                                                         | |
| |                            [[-2.3594, -2.3594, -2.3594,  ..., -0.3281,  | |
| |                   -0.2656,  0.0000]],                                   | |
| |                                                                         | |
| |                            ...,                                         | |
| |                                                                         | |
| |                            [[-5.0000, -5.0000, -5.0000,  ..., -2.6094,  | |
| |                   -8.1094,  0.0000]],                                   | |
| |                                                                         | |
| |                            [[-2.4844, -2.4844, -2.4844,  ..., -0.9062,  | |
| |                   -2.3477, -0.4531]],                                   | |
| |                                                                         | |
| |                            [[-3.0469, -3.0469, -3.0469,  ..., -1.9688,  | |
| |                   -4.8750, -1.2812]]]],                                 | |
| |                          grad_fn=<SubBackward0>)                        | |
| |         dropout = 0.0                                                   | |
| |             key = tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             ...,                                        | |
| |                             [-2.1250e+00,  1.0391e+00, -2.9062e+00,     | |
| |                   ..., -7.4158e-03,                                     | |
| |                              -1.6797e+00, -1.0469e+00],                 | |
| |                             [-8.2812e-01,  4.8828e-02, -5.9766e-01,     | |
| |                   ..., -1.2812e+00,                                     | |
| |                               1.0859e+00,  2.2344e+00],                 | |
| |                             [-7.5781e-01,  1.1484e+00, -1.0000e+00,     | |
| |                   ..., -2.1250e+00,                                     | |
| |                              -1.7188e-01, -8.4375e-01]],                | |
| |                                                                         | |
| |                            [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             ...,                                        | |
| |                             [ 1.2500e+01,  2.7188e+00,  4.7070e-01,     | |
| |                   ..., -3.4375e+00,                                     | |
| |                              -7.0312e-01, -1.9297e+00],                 | |
| |                             [-3.9648e-01, -2.2812e+00, -2.9102e-01,     | |
| |                   ..., -1.6875e+00,                                     | |
| |                              -1.0078e+00,  6.2891e-01],                 | |
| |                             [-1.2500e+01, -3.5781e+00, -5.5859e-01,     | |
| |                   ..., -1.8047e+00,                                     | |
| |                               5.2734e-01,  1.1094e+00]],                | |
| |                                                                         | |
| |                            [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             ...,                                        | |
| |                             [-2.6367e-01,  2.1680e-01, -1.1328e+00,     | |
| |                   ..., -8.3594e-01,                                     | |
| |                              -2.5469e+00,  8.5156e-01],                 | |
| |                             [-1.0132e-02,  6.3965e-02, -1.8799e-02,     | |
| |                   ...,  2.5938e+00,                                     | |
| |                               2.1875e+00, -1.6357e-02],                 | |
| |                             [-8.0078e-01, -7.3828e-01, -9.2969e-01,     | |
| |                   ..., -3.8086e-01,                                     | |
| |                               5.2344e-01, -2.9219e+00]],                | |
| |                                                                         | |
| |                            ...,                                         | |
| |                                                                         | |
| |                            [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             ...,                                        | |
| |                             [-1.3672e+00, -1.7969e+00, -2.2344e+00,     | |
| |                   ..., -8.1250e-01,                                     | |
| |                              -1.7734e+00, -5.6641e-01],                 | |
| |                             [-2.0996e-01,  1.1816e-01, -4.7852e-02,     | |
| |                   ...,  6.2109e-01,                                     | |
| |                              -1.5625e+00, -2.7344e+00],                 | |
| |                             [-2.0469e+00,  3.5547e-01, -2.1875e+00,     | |
| |                   ..., -7.0801e-03,                                     | |
| |                               9.0234e-01, -2.5781e+00]],                | |
| |                                                                         | |
| |                            [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             ...,                                        | |
| |                             [ 3.9062e-03, -1.2812e+00, -6.9141e-01,     | |
| |                   ...,  3.9844e+00,                                     | |
| |                              -4.7812e+00,  2.6875e+00],                 | |
| |                             [ 5.6641e-01, -9.4238e-02, -1.4258e-01,     | |
| |                   ..., -1.0000e+00,                                     | |
| |                               5.0000e-01,  1.4609e+00],                 | |
| |                             [ 8.0469e-01,  2.0020e-01,  3.7695e-01,     | |
| |                   ...,  3.4844e+00,                                     | |
| |                              -3.0664e-01, -4.3750e+00]],                | |
| |                                                                         | |
| |                            [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             ...,                                        | |
| |                             [-1.7734e+00,  2.3438e-01,  2.5000e+00,     | |
| |                   ..., -1.2891e+00,                                     | |
| |                              -1.6504e-01,  1.2109e+00],                 | |
| |                             [ 1.2344e+00, -3.6133e-01,  7.0312e-01,     | |
| |                   ...,  6.4844e-01,                                     | |
| |                              -1.1523e-01, -1.6094e+00],                 | |
| |                             [ 2.3906e+00, -2.2656e+00,  8.5547e-01,     | |
| |                   ..., -2.8125e+00,                                     | |
| |                               2.8281e+00, -6.8359e-01]]]],              | |
| |                   dtype=torch.bfloat16,                                 | |
| |                          grad_fn=<CatBackward0>)                        | |
| |      key_states = tensor([[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             ...,                                        | |
| |                             [-2.1250,  1.0391, -2.9062,  ..., -0.0074,  | |
| |                   -1.6797, -1.0469],                                    | |
| |                             [-0.8281,  0.0488, -0.5977,  ..., -1.2812,  | |
| |                   1.0859,  2.2344],                                     | |
| |                             [-0.7578,  1.1484, -1.0000,  ..., -2.1250,  | |
| |                   -0.1719, -0.8438]],                                   | |
| |                                                                         | |
| |                            [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             ...,                                        | |
| |                             [-2.1250,  1.0391, -2.9062,  ..., -0.0074,  | |
| |                   -1.6797, -1.0469],                                    | |
| |                             [-0.8281,  0.0488, -0.5977,  ..., -1.2812,  | |
| |                   1.0859,  2.2344],                                     | |
| |                             [-0.7578,  1.1484, -1.0000,  ..., -2.1250,  | |
| |                   -0.1719, -0.8438]],                                   | |
| |                                                                         | |
| |                            [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             ...,                                        | |
| |                             [-2.1250,  1.0391, -2.9062,  ..., -0.0074,  | |
| |                   -1.6797, -1.0469],                                    | |
| |                             [-0.8281,  0.0488, -0.5977,  ..., -1.2812,  | |
| |                   1.0859,  2.2344],                                     | |
| |                             [-0.7578,  1.1484, -1.0000,  ..., -2.1250,  | |
| |                   -0.1719, -0.8438]],                                   | |
| |                                                                         | |
| |                            ...,                                         | |
| |                                                                         | |
| |                            [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             ...,                                        | |
| |                             [-1.7734,  0.2344,  2.5000,  ..., -1.2891,  | |
| |                   -0.1650,  1.2109],                                    | |
| |                             [ 1.2344, -0.3613,  0.7031,  ...,  0.6484,  | |
| |                   -0.1152, -1.6094],                                    | |
| |                             [ 2.3906, -2.2656,  0.8555,  ..., -2.8125,  | |
| |                   2.8281, -0.6836]],                                    | |
| |                                                                         | |
| |                            [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             ...,                                        | |
| |                             [-1.7734,  0.2344,  2.5000,  ..., -1.2891,  | |
| |                   -0.1650,  1.2109],                                    | |
| |                             [ 1.2344, -0.3613,  0.7031,  ...,  0.6484,  | |
| |                   -0.1152, -1.6094],                                    | |
| |                             [ 2.3906, -2.2656,  0.8555,  ..., -2.8125,  | |
| |                   2.8281, -0.6836]],                                    | |
| |                                                                         | |
| |                            [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             ...,                                        | |
| |                             [-1.7734,  0.2344,  2.5000,  ..., -1.2891,  | |
| |                   -0.1650,  1.2109],                                    | |
| |                             [ 1.2344, -0.3613,  0.7031,  ...,  0.6484,  | |
| |                   -0.1152, -1.6094],                                    | |
| |                             [ 2.3906, -2.2656,  0.8555,  ..., -2.8125,  | |
| |                   2.8281, -0.6836]]]],                                  | |
| |                          dtype=torch.bfloat16,                          | |
| |                   grad_fn=<UnsafeViewBackward0>)                        | |
| |          kwargs = {                                                     | |
| |                       'sliding_window': None,                           | |
| |                       's_aux': Parameter containing:                    | |
| |                   tensor([ 3.9844,  3.2656,  2.3594,  2.2812,  3.0469,  | |
| |                   4.3438,  2.1250,  2.6094,                             | |
| |                            0.9922,  2.0625,  3.3594,  1.8438,  3.4375,  | |
| |                   2.1250,  3.0000,  0.5156,                             | |
| |                            2.5156, -0.3379, -0.4805,  2.6250,  7.8750,  | |
| |                   7.5625,  3.7344,  4.6250,                             | |
| |                            2.6406,  2.7969,  1.5781,  0.9727,  1.2344,  | |
| |                   1.7812,  1.5703,  1.6328,                             | |
| |                            4.6562,  4.4375,  3.6250,  4.0625,  3.0625,  | |
| |                   3.7188,  4.4062,  5.6875,                             | |
| |                            5.0938,  3.9375,  4.3438,  4.1562,  4.4688,  | |
| |                   3.8906,  3.2812,  2.3594,                             | |
| |                            6.7500,  1.8750,  6.5000,  5.8125,  5.0312,  | |
| |                   6.8750,  6.9062,  6.7812,                             | |
| |                            1.3203,  2.2031,  5.5312,  2.2500,  1.3750,  | |
| |                   5.0000,  2.0312,  1.7656],                            | |
| |                          dtype=torch.bfloat16, requires_grad=True),     | |
| |                       'position_ids': tensor([[12]]),                   | |
| |                       'use_cache': True,                                | |
| |                       'output_router_logits': False                     | |
| |                   }                                                     | |
| |          module = GptOssAttention(                                      | |
| |                     (q_proj): Linear(in_features=2880,                  | |
| |                   out_features=4096, bias=True)                         | |
| |                     (k_proj): Linear(in_features=2880,                  | |
| |                   out_features=512, bias=True)                          | |
| |                     (v_proj): Linear(in_features=2880,                  | |
| |                   out_features=512, bias=True)                          | |
| |                     (o_proj): Linear(in_features=4096,                  | |
| |                   out_features=2880, bias=True)                         | |
| |                   )                                                     | |
| |           probs = tensor([[[[8.0390e-03, 8.0390e-03, 8.0390e-03,  ...,  | |
| |                   2.2196e-02,                                           | |
| |                              5.0413e-02, 4.3211e-01]],                  | |
| |                                                                         | |
| |                            [[2.2359e-02, 2.2359e-02, 2.2359e-02,  ...,  | |
| |                   2.9916e-01,                                           | |
| |                              1.7740e-02, 5.8573e-01]],                  | |
| |                                                                         | |
| |                            [[2.3835e-02, 2.3835e-02, 2.3835e-02,  ...,  | |
| |                   1.8171e-01,                                           | |
| |                              1.9342e-01, 2.5227e-01]],                  | |
| |                                                                         | |
| |                            ...,                                         | |
| |                                                                         | |
| |                            [[5.8761e-03, 5.8761e-03, 5.8761e-03,  ...,  | |
| |                   6.4168e-02,                                           | |
| |                              2.6224e-04, 8.7208e-01]],                  | |
| |                                                                         | |
| |                            [[1.3439e-02, 1.3439e-02, 1.3439e-02,  ...,  | |
| |                   6.5124e-02,                                           | |
| |                              1.5408e-02, 1.0245e-01]],                  | |
| |                                                                         | |
| |                            [[2.7297e-02, 2.7297e-02, 2.7297e-02,  ...,  | |
| |                   8.0231e-02,                                           | |
| |                              4.3871e-03, 1.5956e-01]]]],                | |
| |                   grad_fn=<SoftmaxBackward0>)                           | |
| |           query = tensor([[[[-0.1602, -0.1787, -0.2695,  ...,  1.4453,  | |
| |                   0.6602, -0.9023]],                                    | |
| |                                                                         | |
| |                            [[ 0.0107,  0.0908, -0.0137,  ..., -0.0259,  | |
| |                   1.4453,  0.3867]],                                    | |
| |                                                                         | |
| |                            [[-0.0099,  0.0486,  0.0063,  ..., -1.8672,  | |
| |                   0.2949, -0.0635]],                                    | |
| |                                                                         | |
| |                            ...,                                         | |
| |                                                                         | |
| |                            [[-0.0986,  0.1348, -0.0198,  ...,  5.6250,  | |
| |                   0.6836,  1.8047]],                                    | |
| |                                                                         | |
| |                            [[ 0.0811, -0.1250, -0.0508,  ...,  0.5156,  | |
| |                   0.0811,  1.3438]],                                    | |
| |                                                                         | |
| |                            [[ 0.5742, -0.1641, -0.1572,  ...,  2.5312,  | |
| |                   -0.4277,  1.1719]]]],                                 | |
| |                          dtype=torch.bfloat16, grad_fn=<CatBackward0>)  | |
| |         scaling = 0.125                                                 | |
| |          scores = tensor([[[[8.0390e-03, 8.0390e-03, 8.0390e-03,  ...,  | |
| |                   1.9897e-02,                                           | |
| |                              2.2196e-02, 5.0413e-02]],                  | |
| |                                                                         | |
| |                            [[2.2359e-02, 2.2359e-02, 2.2359e-02,  ...,  | |
| |                   1.4602e-04,                                           | |
| |                              2.9916e-01, 1.7740e-02]],                  | |
| |                                                                         | |
| |                            [[2.3835e-02, 2.3835e-02, 2.3835e-02,  ...,  | |
| |                   1.6478e-02,                                           | |
| |                              1.8171e-01, 1.9342e-01]],                  | |
| |                                                                         | |
| |                            ...,                                         | |
| |                                                                         | |
| |                            [[5.8761e-03, 5.8761e-03, 5.8761e-03,  ...,  | |
| |                   1.5906e-04,                                           | |
| |                              6.4168e-02, 2.6224e-04]],                  | |
| |                                                                         | |
| |                            [[1.3439e-02, 1.3439e-02, 1.3439e-02,  ...,  | |
| |                   7.8554e-02,                                           | |
| |                              6.5124e-02, 1.5408e-02]],                  | |
| |                                                                         | |
| |                            [[2.7297e-02, 2.7297e-02, 2.7297e-02,  ...,  | |
| |                   4.3871e-03,                                           | |
| |                              8.0231e-02, 4.3871e-03]]]],                | |
| |                   grad_fn=<SliceBackward0>)                             | |
| |           sinks = tensor([[[[ 3.9844]],                                 | |
| |                                                                         | |
| |                            [[ 3.2656]],                                 | |
| |                                                                         | |
| |                            [[ 2.3594]],                                 | |
| |                                                                         | |
| |                            [[ 2.2812]],                                 | |
| |                                                                         | |
| |                            [[ 3.0469]],                                 | |
| |                                                                         | |
| |                            [[ 4.3438]],                                 | |
| |                                                                         | |
| |                            [[ 2.1250]],                                 | |
| |                                                                         | |
| |                            [[ 2.6094]],                                 | |
| |                                                                         | |
| |                            [[ 0.9922]],                                 | |
| |                                                                         | |
| |                            [[ 2.0625]],                                 | |
| |                                                                         | |
| |                            [[ 3.3594]],                                 | |
| |                                                                         | |
| |                            [[ 1.8438]],                                 | |
| |                                                                         | |
| |                            [[ 3.4375]],                                 | |
| |                                                                         | |
| |                            [[ 2.1250]],                                 | |
| |                                                                         | |
| |                            [[ 3.0000]],                                 | |
| |                                                                         | |
| |                            [[ 0.5156]],                                 | |
| |                                                                         | |
| |                            [[ 2.5156]],                                 | |
| |                                                                         | |
| |                            [[-0.3379]],                                 | |
| |                                                                         | |
| |                            [[-0.4805]],                                 | |
| |                                                                         | |
| |                            [[ 2.6250]],                                 | |
| |                                                                         | |
| |                            [[ 7.8750]],                                 | |
| |                                                                         | |
| |                            [[ 7.5625]],                                 | |
| |                                                                         | |
| |                            [[ 3.7344]],                                 | |
| |                                                                         | |
| |                            [[ 4.6250]],                                 | |
| |                                                                         | |
| |                            [[ 2.6406]],                                 | |
| |                                                                         | |
| |                            [[ 2.7969]],                                 | |
| |                                                                         | |
| |                            [[ 1.5781]],                                 | |
| |                                                                         | |
| |                            [[ 0.9727]],                                 | |
| |                                                                         | |
| |                            [[ 1.2344]],                                 | |
| |                                                                         | |
| |                            [[ 1.7812]],                                 | |
| |                                                                         | |
| |                            [[ 1.5703]],                                 | |
| |                                                                         | |
| |                            [[ 1.6328]],                                 | |
| |                                                                         | |
| |                            [[ 4.6562]],                                 | |
| |                                                                         | |
| |                            [[ 4.4375]],                                 | |
| |                                                                         | |
| |                            [[ 3.6250]],                                 | |
| |                                                                         | |
| |                            [[ 4.0625]],                                 | |
| |                                                                         | |
| |                            [[ 3.0625]],                                 | |
| |                                                                         | |
| |                            [[ 3.7188]],                                 | |
| |                                                                         | |
| |                            [[ 4.4062]],                                 | |
| |                                                                         | |
| |                            [[ 5.6875]],                                 | |
| |                                                                         | |
| |                            [[ 5.0938]],                                 | |
| |                                                                         | |
| |                            [[ 3.9375]],                                 | |
| |                                                                         | |
| |                            [[ 4.3438]],                                 | |
| |                                                                         | |
| |                            [[ 4.1562]],                                 | |
| |                                                                         | |
| |                            [[ 4.4688]],                                 | |
| |                                                                         | |
| |                            [[ 3.8906]],                                 | |
| |                                                                         | |
| |                            [[ 3.2812]],                                 | |
| |                                                                         | |
| |                            [[ 2.3594]],                                 | |
| |                                                                         | |
| |                            [[ 6.7500]],                                 | |
| |                                                                         | |
| |                            [[ 1.8750]],                                 | |
| |                                                                         | |
| |                            [[ 6.5000]],                                 | |
| |                                                                         | |
| |                            [[ 5.8125]],                                 | |
| |                                                                         | |
| |                            [[ 5.0312]],                                 | |
| |                                                                         | |
| |                            [[ 6.8750]],                                 | |
| |                                                                         | |
| |                            [[ 6.9062]],                                 | |
| |                                                                         | |
| |                            [[ 6.7812]],                                 | |
| |                                                                         | |
| |                            [[ 1.3203]],                                 | |
| |                                                                         | |
| |                            [[ 2.2031]],                                 | |
| |                                                                         | |
| |                            [[ 5.5312]],                                 | |
| |                                                                         | |
| |                            [[ 2.2500]],                                 | |
| |                                                                         | |
| |                            [[ 1.3750]],                                 | |
| |                                                                         | |
| |                            [[ 5.0000]],                                 | |
| |                                                                         | |
| |                            [[ 2.0312]],                                 | |
| |                                                                         | |
| |                            [[ 1.7656]]]], dtype=torch.bfloat16,         | |
| |                   grad_fn=<ExpandBackward0>)                            | |
| |           value = tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             ...,                                        | |
| |                             [ 9.2578e-01, -5.0391e-01, -1.3203e+00,     | |
| |                   ...,  2.1562e+00,                                     | |
| |                              -1.1953e+00, -1.7656e+00],                 | |
| |                             [-1.4844e+00, -7.2656e-01,  7.1484e-01,     | |
| |                   ...,  7.1289e-02,                                     | |
| |                              -6.4062e-01, -1.1169e-02],                 | |
| |                             [ 3.5352e-01,  2.9102e-01,  5.5078e-01,     | |
| |                   ..., -6.7578e-01,                                     | |
| |                               1.3867e-01, -7.3438e-01]],                | |
| |                                                                         | |
| |                            [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             ...,                                        | |
| |                             [ 3.3447e-02,  1.7500e+00, -5.7031e-01,     | |
| |                   ..., -3.1250e-01,                                     | |
| |                              -3.3008e-01,  6.6797e-01],                 | |
| |                             [-1.0938e+00,  4.9023e-01, -6.2500e-01,     | |
| |                   ..., -1.2256e-01,                                     | |
| |                              -9.7656e-01,  2.0312e-01],                 | |
| |                             [ 6.7383e-02, -1.1084e-01,  1.3750e+00,     | |
| |                   ..., -1.2891e+00,                                     | |
| |                              -4.0430e-01,  1.6719e+00]],                | |
| |                                                                         | |
| |                            [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             ...,                                        | |
| |                             [ 7.2754e-02, -3.5938e-01,  9.8438e-01,     | |
| |                   ..., -2.0312e+00,                                     | |
| |                              -1.2578e+00, -8.6719e-01],                 | |
| |                             [ 1.2891e+00, -2.5781e-01, -1.7480e-01,     | |
| |                   ..., -6.1719e-01,                                     | |
| |                               4.1797e-01, -3.2617e-01],                 | |
| |                             [-2.4121e-01,  1.3203e+00, -4.1797e-01,     | |
| |                   ..., -1.9297e+00,                                     | |
| |                               1.3379e-01, -2.7734e-01]],                | |
| |                                                                         | |
| |                            ...,                                         | |
| |                                                                         | |
| |                            [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             ...,                                        | |
| |                             [ 5.6250e-01,  1.6406e-01,  2.6855e-03,     | |
| |                   ...,  4.6484e-01,                                     | |
| |                               8.2031e-01, -1.0938e+00],                 | |
| |                             [ 1.0156e-01, -2.2852e-01,  8.9844e-01,     | |
| |                   ..., -2.8711e-01,                                     | |
| |                               1.3770e-01, -8.1250e-01],                 | |
| |                             [-1.7383e-01, -8.8672e-01, -1.0986e-01,     | |
| |                   ..., -8.4766e-01,                                     | |
| |                              -2.8711e-01, -2.3438e-01]],                | |
| |                                                                         | |
| |                            [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             ...,                                        | |
| |                             [ 2.7969e+00,  1.0693e-01, -1.3672e+00,     | |
| |                   ..., -2.8125e+00,                                     | |
| |                               1.5747e-02,  1.7422e+00],                 | |
| |                             [-4.6484e-01,  1.5137e-01, -4.9609e-01,     | |
| |                   ...,  2.8906e-01,                                     | |
| |                               3.3789e-01, -6.8750e-01],                 | |
| |                             [ 1.9062e+00, -4.0039e-02, -1.4609e+00,     | |
| |                   ...,  1.7383e-01,                                     | |
| |                               3.1055e-01,  6.5625e-01]],                | |
| |                                                                         | |
| |                            [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,     | |
| |                   ...,  0.0000e+00,                                     | |
| |                               0.0000e+00,  0.0000e+00],                 | |
| |                             ...,                                        | |
| |                             [ 1.1094e+00, -1.9409e-02, -5.1953e-01,     | |
| |                   ...,  1.6211e-01,                                     | |
| |                               1.2109e+00,  3.8281e-01],                 | |
| |                             [-1.2500e+00, -4.7852e-01,  1.3477e-01,     | |
| |                   ...,  1.3379e-01,                                     | |
| |                              -7.1094e-01,  2.1582e-01],                 | |
| |                             [ 1.3125e+00,  7.5781e-01,  6.7578e-01,     | |
| |                   ..., -3.6377e-02,                                     | |
| |                              -1.2061e-01,  2.6367e-01]]]],              | |
| |                   dtype=torch.bfloat16,                                 | |
| |                          grad_fn=<CatBackward0>)                        | |
| |    value_states = tensor([[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             ...,                                        | |
| |                             [ 0.9258, -0.5039, -1.3203,  ...,  2.1562,  | |
| |                   -1.1953, -1.7656],                                    | |
| |                             [-1.4844, -0.7266,  0.7148,  ...,  0.0713,  | |
| |                   -0.6406, -0.0112],                                    | |
| |                             [ 0.3535,  0.2910,  0.5508,  ..., -0.6758,  | |
| |                   0.1387, -0.7344]],                                    | |
| |                                                                         | |
| |                            [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             ...,                                        | |
| |                             [ 0.9258, -0.5039, -1.3203,  ...,  2.1562,  | |
| |                   -1.1953, -1.7656],                                    | |
| |                             [-1.4844, -0.7266,  0.7148,  ...,  0.0713,  | |
| |                   -0.6406, -0.0112],                                    | |
| |                             [ 0.3535,  0.2910,  0.5508,  ..., -0.6758,  | |
| |                   0.1387, -0.7344]],                                    | |
| |                                                                         | |
| |                            [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             ...,                                        | |
| |                             [ 0.9258, -0.5039, -1.3203,  ...,  2.1562,  | |
| |                   -1.1953, -1.7656],                                    | |
| |                             [-1.4844, -0.7266,  0.7148,  ...,  0.0713,  | |
| |                   -0.6406, -0.0112],                                    | |
| |                             [ 0.3535,  0.2910,  0.5508,  ..., -0.6758,  | |
| |                   0.1387, -0.7344]],                                    | |
| |                                                                         | |
| |                            ...,                                         | |
| |                                                                         | |
| |                            [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             ...,                                        | |
| |                             [ 1.1094, -0.0194, -0.5195,  ...,  0.1621,  | |
| |                   1.2109,  0.3828],                                     | |
| |                             [-1.2500, -0.4785,  0.1348,  ...,  0.1338,  | |
| |                   -0.7109,  0.2158],                                    | |
| |                             [ 1.3125,  0.7578,  0.6758,  ..., -0.0364,  | |
| |                   -0.1206,  0.2637]],                                   | |
| |                                                                         | |
| |                            [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             ...,                                        | |
| |                             [ 1.1094, -0.0194, -0.5195,  ...,  0.1621,  | |
| |                   1.2109,  0.3828],                                     | |
| |                             [-1.2500, -0.4785,  0.1348,  ...,  0.1338,  | |
| |                   -0.7109,  0.2158],                                    | |
| |                             [ 1.3125,  0.7578,  0.6758,  ..., -0.0364,  | |
| |                   -0.1206,  0.2637]],                                   | |
| |                                                                         | |
| |                            [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  | |
| |                   0.0000,  0.0000],                                     | |
| |                             ...,                                        | |
| |                             [ 1.1094, -0.0194, -0.5195,  ...,  0.1621,  | |
| |                   1.2109,  0.3828],                                     | |
| |                             [-1.2500, -0.4785,  0.1348,  ...,  0.1338,  | |
| |                   -0.7109,  0.2158],                                    | |
| |                             [ 1.3125,  0.7578,  0.6758,  ..., -0.0364,  | |
| |                   -0.1206,  0.2637]]]],                                 | |
| |                          dtype=torch.bfloat16,                          | |
| |                   grad_fn=<UnsafeViewBackward0>)                        | |
| +-------------------------------------------------------------------------+ |
+-----------------------------------------------------------------------------+
RuntimeError: expected m1 and m2 to have the same dtype, but got: float != 
struct c10::BFloat16
