PS G:\ossc> py  -m cli.main eval
MXFP4 quantization requires triton >= 3.4.0 and kernels installed, we will default to dequantizing the model to bf16
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.78s/it]
2025-10-29 16:09:05,383 | INFO | gpt_oss_ws | GPT-OSS workspace model initialized
[workspace-debug] layer=1 hidden_states=torch.bfloat16 past=None virtual=torch.bfloat16
[kv-proxy] key_states dtype=torch.bfloat16 value_states dtype=torch.bfloat16 virtual_k dtype=torch.bfloat16 virtual_v dtype=torch.bfloat16
workspace virtual KV dtype alignment (before cast): real_k=torch.bfloat16, real_v=torch.bfloat16, virtual_k=torch.bfloat16, virtual_v=torch.bfloat16, target_key=torch.float32, target_value=torch.float32
workspace virtual KV dtype alignment (after cast): key=torch.float32, value=torch.float32
╭──────────────────────────────────────────────────────────────────────────── Traceback (most recent call last) ────────────────────────────────────────────────────────────────────────────╮
│ G:\ossc\cli\main.py:53 in eval                                                                                                                                                            │
│                                                                                                                                                                                           │
│   50   cfg = load_config(str(cfg_path))                                                                                                                                                   │
│   51   from evals import report                                                                                                                                                           │
│   52                                                                                                                                                                                      │
│ ❱ 53   asyncio.run(report.run(task, cfg))                                                                                                                                                 │
│   54                                                                                                                                                                                      │
│   55                                                                                                                                                                                      │
│   56 @app.command(name="fluency-guard")                                                                                                                                                   │
│                                                                                                                                                                                           │
│ ╭────────────────────────────────────────────────────────────────── locals ───────────────────────────────────────────────────────────────────╮                                           │
│ │      cfg = WorkspaceConfig(                                                                                                                 │                                           │
│ │            │   model_name='openai/gpt-oss-20b',                                                                                             │                                           │
│ │            │   quantization='Mxfp4',                                                                                                        │                                           │
│ │            │   device_map='auto',                                                                                                           │                                           │
│ │            │   hooked_layers=[1, 5, 9, 13, 17, 21],                                                                                         │                                           │
│ │            │   nvirt=4,                                                                                                                     │                                           │
│ │            │   residual_rank=8,                                                                                                             │                                           │
│ │            │   slot_count=6,                                                                                                                │                                           │
│ │            │   slot_dim=256,                                                                                                                │                                           │
│ │            │   slot_iterations=1,                                                                                                           │                                           │
│ │            │   enable_kv_append=True,                                                                                                       │                                           │
│ │            │   enable_residual_delta=True,                                                                                                  │                                           │
│ │            │   enable_read_probes=True,                                                                                                     │                                           │
│ │            │   enable_broadcast=True,                                                                                                       │                                           │
│ │            │   workspace_device='cpu',                                                                                                      │                                           │
│ │            │   retention=RetentionConfig(                                                                                                   │                                           │
│ │            │   │   virt_kv_max_tokens_per_layer=1024,                                                                                       │                                           │
│ │            │   │   virt_kv_ttl_steps=2048,                                                                                                  │                                           │
│ │            │   │   spill_to_cpu=True,                                                                                                       │                                           │
│ │            │   │   prefetch_margin=16                                                                                                       │                                           │
│ │            │   ),                                                                                                                           │                                           │
│ │            │   log_level='INFO',                                                                                                            │                                           │
│ │            │   api_host='0.0.0.0',                                                                                                          │                                           │
│ │            │   api_port=8000,                                                                                                               │                                           │
│ │            │   controller_entropy_floor=2.5,                                                                                                │                                           │
│ │            │   controller_norm_cap=4.5,                                                                                                     │                                           │
│ │            │   sqlite_path='workspace_memory.sqlite',                                                                                       │                                           │
│ │            │   faiss_index_path='workspace_memory.faiss',                                                                                   │                                           │
│ │            │   memory_embedding_dim=384,                                                                                                    │                                           │
│ │            │   max_context_tokens=8192,                                                                                                     │                                           │
│ │            │   bf16_fallback=True                                                                                                           │                                           │
│ │            )                                                                                                                                │                                           │
│ │ cfg_path = WindowsPath('configs/server.yaml')                                                                                               │                                           │
│ │   config = None                                                                                                                             │                                           │
│ │   report = <module 'evals.report' from 'G:\\ossc\\evals\\report.py'>                                                                        │                                           │
│ │     task = 'fluency'                                                                                                                        │                                           │
│ ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯                                           │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\asyncio\runners.py:195 in run                                                                                                   │
│                                                                                                                                                                                           │
│   192 │   │   │   "asyncio.run() cannot be called from a running event loop")                  ╭─────────────────────────────── locals ───────────────────────────────╮                   │
│   193 │                                                                                        │        debug = None                                                  │                   │
│   194 │   with Runner(debug=debug, loop_factory=loop_factory) as runner:                       │ loop_factory = None                                                  │                   │
│ ❱ 195 │   │   return runner.run(main)                                                          │         main = <coroutine object run at 0x0000024126827480>          │                   │
│   196                                                                                          │       runner = <asyncio.runners.Runner object at 0x0000024232B206E0> │                   │
│   197                                                                                          ╰──────────────────────────────────────────────────────────────────────╯                   │
│   198 def _cancel_all_tasks(loop):                                                                                                                                                        │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\asyncio\runners.py:118 in run                                                                                                   │
│                                                                                                                                                                                           │
│   115 │   │                                                                                                                                                                               │
│   116 │   │   self._interrupt_count = 0                                                                                                                                                   │
│   117 │   │   try:                                                                                                                                                                        │
│ ❱ 118 │   │   │   return self._loop.run_until_complete(task)                                                                                                                              │
│   119 │   │   except exceptions.CancelledError:                                                                                                                                           │
│   120 │   │   │   if self._interrupt_count > 0:                                                                                                                                           │
│   121 │   │   │   │   uncancel = getattr(task, "uncancel", None)                                                                                                                          │
│                                                                                                                                                                                           │
│ ╭─────────────────────────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────────────────────────╮ │
│ │        context = <_contextvars.Context object at 0x0000024232B2F100>                                                                                                                  │ │
│ │           coro = <coroutine object run at 0x0000024126827480>                                                                                                                         │ │
│ │           self = <asyncio.runners.Runner object at 0x0000024232B206E0>                                                                                                                │ │
│ │ sigint_handler = functools.partial(<bound method Runner._on_sigint of <asyncio.runners.Runner object at 0x0000024232B206E0>>, main_task=<Task finished name='Task-1' coro=<run()      │ │
│ │                  done, defined at G:\ossc\evals\report.py:17> exception=RuntimeError('expected scalar type Float but found BFloat16')>)                                               │ │
│ │           task = <Task finished name='Task-1' coro=<run() done, defined at G:\ossc\evals\report.py:17> exception=RuntimeError('expected scalar type Float but found BFloat16')>       │ │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\asyncio\base_events.py:725 in run_until_complete                                                                                │
│                                                                                                                                                                                           │
│    722 │   │   if not future.done():                                                                                                                                                      │
│    723 │   │   │   raise RuntimeError('Event loop stopped before Future completed.')                                                                                                      │
│    724 │   │                                                                                                                                                                              │
│ ❱  725 │   │   return future.result()                                                                                                                                                     │
│    726 │                                                                                                                                                                                  │
│    727 │   def stop(self):                                                                                                                                                                │
│    728 │   │   """Stop running the event loop.                                                                                                                                            │
│                                                                                                                                                                                           │
│ ╭───────────────────────────────────────────────────────────────────────────────── locals ──────────────────────────────────────────────────────────────────────────────────╮             │
│ │   future = <Task finished name='Task-1' coro=<run() done, defined at G:\ossc\evals\report.py:17> exception=RuntimeError('expected scalar type Float but found BFloat16')> │             │
│ │ new_task = False                                                                                                                                                          │             │
│ │     self = <ProactorEventLoop running=False closed=True debug=False>                                                                                                      │             │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯             │
│                                                                                                                                                                                           │
│ G:\ossc\evals\report.py:27 in run                                                                                                                                                         │
│                                                                                                                                                                                           │
│   24   request_ctx = GenerationRequestContext(request_id=str(time.time()), toggles=HookToggle                                                                                             │
│   25   for prompt in prompts:                                                                                                                                                             │
│   26 │   input_ids = model.tokenizer_encode(prompt)                                                                                                                                       │
│ ❱ 27 │   outputs = model.generate(request_ctx, input_ids=input_ids, max_new_tokens=128)                                                                                                   │
│   28 │   text = model.tokenizer_decode(outputs[0])                                                                                                                                        │
│   29 │   print(json.dumps({"prompt": prompt, "completion": text}))                                                                                                                        │
│   30   model.close()                                                                                                                                                                      │
│                                                                                                                                                                                           │
│ ╭──────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────╮                                        │
│ │      config = WorkspaceConfig(                                                                                                                 │                                        │
│ │               │   model_name='openai/gpt-oss-20b',                                                                                             │                                        │
│ │               │   quantization='Mxfp4',                                                                                                        │                                        │
│ │               │   device_map='auto',                                                                                                           │                                        │
│ │               │   hooked_layers=[1, 5, 9, 13, 17, 21],                                                                                         │                                        │
│ │               │   nvirt=4,                                                                                                                     │                                        │
│ │               │   residual_rank=8,                                                                                                             │                                        │
│ │               │   slot_count=6,                                                                                                                │                                        │
│ │               │   slot_dim=256,                                                                                                                │                                        │
│ │               │   slot_iterations=1,                                                                                                           │                                        │
│ │               │   enable_kv_append=True,                                                                                                       │                                        │
│ │               │   enable_residual_delta=True,                                                                                                  │                                        │
│ │               │   enable_read_probes=True,                                                                                                     │                                        │
│ │               │   enable_broadcast=True,                                                                                                       │                                        │
│ │               │   workspace_device='cpu',                                                                                                      │                                        │
│ │               │   retention=RetentionConfig(                                                                                                   │                                        │
│ │               │   │   virt_kv_max_tokens_per_layer=1024,                                                                                       │                                        │
│ │               │   │   virt_kv_ttl_steps=2048,                                                                                                  │                                        │
│ │               │   │   spill_to_cpu=True,                                                                                                       │                                        │
│ │               │   │   prefetch_margin=16                                                                                                       │                                        │
│ │               │   ),                                                                                                                           │                                        │
│ │               │   log_level='INFO',                                                                                                            │                                        │
│ │               │   api_host='0.0.0.0',                                                                                                          │                                        │
│ │               │   api_port=8000,                                                                                                               │                                        │
│ │               │   controller_entropy_floor=2.5,                                                                                                │                                        │
│ │               │   controller_norm_cap=4.5,                                                                                                     │                                        │
│ │               │   sqlite_path='workspace_memory.sqlite',                                                                                       │                                        │
│ │               │   faiss_index_path='workspace_memory.faiss',                                                                                   │                                        │
│ │               │   memory_embedding_dim=384,                                                                                                    │                                        │
│ │               │   max_context_tokens=8192,                                                                                                     │                                        │
│ │               │   bf16_fallback=True                                                                                                           │                                        │
│ │               )                                                                                                                                │                                        │
│ │   input_ids = tensor([[64614,   277,   750,   290, 43952, 49803,  2608,  1072,   220,    23,                                                   │                                        │
│ │               │   │    18304,    13]])                                                                                                         │                                        │
│ │       model = <gpt_oss_ws.model_wrapper.GPTOSSHookedModel object at 0x0000024232B20590>                                                        │                                        │
│ │      prompt = 'Summarize the persistent workspace state over 8 turns.'                                                                         │                                        │
│ │     prompts = ['Summarize the persistent workspace state over 8 turns.', 'Describe retention policy effects on current virtual KV.']           │                                        │
│ │ request_ctx = GenerationRequestContext(                                                                                                        │                                        │
│ │               │   request_id='1761768545.3839512',                                                                                             │                                        │
│ │               │   toggles=HookToggles(kv_append=True, residual_delta=True, read_probes=True, broadcast=True),                                  │                                        │
│ │               │   retention_overrides=None                                                                                                     │                                        │
│ │               )                                                                                                                                │                                        │
│ │        task = 'fluency'                                                                                                                        │                                        │
│ ╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯                                        │
│                                                                                                                                                                                           │
│ G:\ossc\gpt_oss_ws\model_wrapper.py:144 in generate                                                                                                                                       │
│                                                                                                                                                                                           │
│   141   def generate(self, request: GenerationRequestContext, **kwargs) -> torch.Tensor:                                                                                                  │
│   142 │   from .generation import generate_with_workspace                                                                                                                                 │
│   143 │                                                                                                                                                                                   │
│ ❱ 144 │   return generate_with_workspace(self, request, **kwargs)                                                                                                                         │
│   145                                                                                                                                                                                     │
│   146   def tokenizer_encode(self, text: str, **kwargs) -> torch.Tensor:                                                                                                                  │
│   147 │   return self.tokenizer(text, return_tensors="pt", **kwargs)["input_ids"]                                                                                                         │
│                                                                                                                                                                                           │
│ ╭───────────────────────────────────────────────── locals ──────────────────────────────────────────────────╮                                                                             │
│ │  kwargs = {                                                                                               │                                                                             │
│ │           │   'input_ids': tensor([[64614,   277,   750,   290, 43952, 49803,  2608,  1072,   220,    23, │                                                                             │
│ │           │   │    18304,    13]]),                                                                       │                                                                             │
│ │           │   'max_new_tokens': 128                                                                       │                                                                             │
│ │           }                                                                                               │                                                                             │
│ │ request = GenerationRequestContext(                                                                       │                                                                             │
│ │           │   request_id='1761768545.3839512',                                                            │                                                                             │
│ │           │   toggles=HookToggles(                                                                        │                                                                             │
│ │           │   │   kv_append=True,                                                                         │                                                                             │
│ │           │   │   residual_delta=True,                                                                    │                                                                             │
│ │           │   │   read_probes=True,                                                                       │                                                                             │
│ │           │   │   broadcast=True                                                                          │                                                                             │
│ │           │   ),                                                                                          │                                                                             │
│ │           │   retention_overrides=None                                                                    │                                                                             │
│ │           )                                                                                               │                                                                             │
│ │    self = <gpt_oss_ws.model_wrapper.GPTOSSHookedModel object at 0x0000024232B20590>                       │                                                                             │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯                                                                             │
│                                                                                                                                                                                           │
│ G:\ossc\gpt_oss_ws\generation.py:63 in generate_with_workspace                                                                                                                            │
│                                                                                                                                                                                           │
│   60 │   if cache is not None and isinstance(cache, tuple):                                                                                                                               │
│   61 │     cache = DynamicCache.from_legacy_cache(cache)                                                                                                                                  │
│   62 │   with model.runtime_context(request.toggles):                                                                                                                                     │
│ ❱ 63 │     outputs = model.model(                                                                                                                                                         │
│   64 │   │   input_ids=step_input,                                                                                                                                                        │
│   65 │   │   attention_mask=attention_mask,                                                                                                                                               │
│   66 │   │   past_key_values=cache,                                                                                                                                                       │
│                                                                                                                                                                                           │
│ ╭─────────────────────────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────────────────────────╮ │
│ │        attention_mask = tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])                                                                                                             │ │
│ │                 cache = DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,               │ │
│ │                         DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,         │ │
│ │                         DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                      │ │
│ │                         DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer])                                                                            │ │
│ │        cache_position = tensor([12])                                                                                                                                                  │ │
│ │              decision = ControllerOutput(broadcast=True, retrieve=False, write_memory=True, halt=False)                                                                               │ │
│ │                device = device(type='cpu')                                                                                                                                            │ │
│ │          eos_token_id = 200002                                                                                                                                                        │ │
│ │            full_input = tensor([[64614,   277,   750,   290, 43952, 49803,  2608,  1072,   220,    23,                                                                                │ │
│ │                         │   │    18304,    13,  1069]])                                                                                                                               │ │
│ │      generated_tokens = [tensor([1069])]                                                                                                                                              │ │
│ │             input_ids = tensor([[64614,   277,   750,   290, 43952, 49803,  2608,  1072,   220,    23,                                                                                │ │
│ │                         │   │    18304,    13]])                                                                                                                                      │ │
│ │                logits = tensor([[ 2.1094,  4.4375,  1.5547,  ..., -0.0679, -0.0757,  0.0767]],                                                                                        │ │
│ │                         │      dtype=torch.bfloat16, grad_fn=<SliceBackward0>)                                                                                                        │ │
│ │        max_new_tokens = 128                                                                                                                                                           │ │
│ │                 model = <gpt_oss_ws.model_wrapper.GPTOSSHookedModel object at 0x0000024232B20590>                                                                                     │ │
│ │             new_cache = DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,               │ │
│ │                         DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,         │ │
│ │                         DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                      │ │
│ │                         DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer])                                                                            │ │
│ │            next_token = tensor([1069])                                                                                                                                                │ │
│ │ next_token_unsqueezed = tensor([[1069]])                                                                                                                                              │ │
│ │               outputs = MoeCausalLMOutputWithPast(                                                                                                                                    │ │
│ │                         │   loss=None,                                                                                                                                                │ │
│ │                         │   aux_loss=None,                                                                                                                                            │ │
│ │                         │   logits=tensor([[[ 5.2188,  8.1875,  6.7500,  ..., -0.0579, -0.0835,  0.0972],                                                                             │ │
│ │                         │   │    [ 5.4688,  7.5000,  4.4375,  ...,  0.0281, -0.0189,  0.0410],                                                                                        │ │
│ │                         │   │    [ 3.9375,  8.5000,  3.3594,  ...,  0.0535, -0.0942,  0.1138],                                                                                        │ │
│ │                         │   │    ...,                                                                                                                                                 │ │
│ │                         │   │    [ 5.0938,  6.4688,  3.7188,  ..., -0.0223,  0.0315, -0.0229],                                                                                        │ │
│ │                         │   │    [ 7.3125, 11.2500,  4.0938,  ..., -0.1221, -0.1060,  0.0986],                                                                                        │ │
│ │                         │   │    [ 2.1094,  4.4375,  1.5547,  ..., -0.0679, -0.0757,  0.0767]]],                                                                                      │ │
│ │                         │      dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>),                                                                                                  │ │
│ │                         │   past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,         │ │
│ │                         DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                      │ │
│ │                         DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,         │ │
│ │                         DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]),                                                             │ │
│ │                         │   hidden_states=None,                                                                                                                                       │ │
│ │                         │   attentions=None,                                                                                                                                          │ │
│ │                         │   router_logits=None                                                                                                                                        │ │
│ │                         )                                                                                                                                                             │ │
│ │        position_start = 12                                                                                                                                                            │ │
│ │               request = GenerationRequestContext(                                                                                                                                     │ │
│ │                         │   request_id='1761768545.3839512',                                                                                                                          │ │
│ │                         │   toggles=HookToggles(kv_append=True, residual_delta=True, read_probes=True, broadcast=True),                                                               │ │
│ │                         │   retention_overrides=None                                                                                                                                  │ │
│ │                         )                                                                                                                                                             │ │
│ │                  step = 1                                                                                                                                                             │ │
│ │            step_input = tensor([[1069]])                                                                                                                                              │ │
│ │           step_length = 1                                                                                                                                                             │ │
│ │       stream_callback = None                                                                                                                                                          │ │
│ │           temperature = 0.8                                                                                                                                                           │ │
│ │                 top_p = 0.95                                                                                                                                                          │ │
│ │          total_length = 13                                                                                                                                                            │ │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\torch\nn\modules\module.py:1773 in _wrapped_call_impl                                                             │
│                                                                                                                                                                                           │
│   1770 │   │   if self._compiled_call_impl is not None:                                                                                                                                   │
│   1771 │   │   │   return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]                                                                                                 │
│   1772 │   │   else:                                                                                                                                                                      │
│ ❱ 1773 │   │   │   return self._call_impl(*args, **kwargs)                                                                                                                                │
│   1774 │                                                                                                                                                                                  │
│   1775 │   # torchrec tests the code consistency with the following code                                                                                                                  │
│   1776 │   # fmt: off                                                                                                                                                                     │
│                                                                                                                                                                                           │
│ ╭─────────────────────────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────────────────────────╮ │
│ │   args = ()                                                                                                                                                                           │ │
│ │ kwargs = {                                                                                                                                                                            │ │
│ │          │   'input_ids': tensor([[1069]]),                                                                                                                                           │ │
│ │          │   'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),                                                                                                     │ │
│ │          │   'past_key_values': DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,       │ │
│ │          DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,          │ │
│ │          DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,          │ │
│ │          DynamicSlidingWindowLayer, DynamicLayer]),                                                                                                                                   │ │
│ │          │   'cache_position': tensor([12]),                                                                                                                                          │ │
│ │          │   'use_cache': True                                                                                                                                                        │ │
│ │          }                                                                                                                                                                            │ │
│ │   self = GptOssForCausalLM(                                                                                                                                                           │ │
│ │            (model): GptOssModel(                                                                                                                                                      │ │
│ │          │   (embed_tokens): Embedding(201088, 2880, padding_idx=199999)                                                                                                              │ │
│ │          │   (layers): ModuleList(                                                                                                                                                    │ │
│ │          │     (0-23): 24 x GptOssDecoderLayer(                                                                                                                                       │ │
│ │          │   │   (self_attn): GptOssAttention(                                                                                                                                        │ │
│ │          │   │     (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                                   │ │
│ │          │   │     (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                    │ │
│ │          │   │     (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                    │ │
│ │          │   │     (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                                   │ │
│ │          │   │   )                                                                                                                                                                    │ │
│ │          │   │   (mlp): GptOssMLP(                                                                                                                                                    │ │
│ │          │   │     (router): GptOssTopKRouter()                                                                                                                                       │ │
│ │          │   │     (experts): GptOssExperts()                                                                                                                                         │ │
│ │          │   │   )                                                                                                                                                                    │ │
│ │          │   │   (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                                 │ │
│ │          │   │   (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                        │ │
│ │          │     )                                                                                                                                                                      │ │
│ │          │   )                                                                                                                                                                        │ │
│ │          │   (norm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                                                │ │
│ │          │   (rotary_emb): GptOssRotaryEmbedding()                                                                                                                                    │ │
│ │            )                                                                                                                                                                          │ │
│ │            (lm_head): Linear(in_features=2880, out_features=201088, bias=False)                                                                                                       │ │
│ │          )                                                                                                                                                                            │ │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\torch\nn\modules\module.py:1784 in _call_impl                                                                     │
│                                                                                                                                                                                           │
│   1781 │   │   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks                                                                                            │
│   1782 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hooks                                                                                                            │
│   1783 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks):                                                                                                            │
│ ❱ 1784 │   │   │   return forward_call(*args, **kwargs)                                                                                                                                   │
│   1785 │   │                                                                                                                                                                              │
│   1786 │   │   result = None                                                                                                                                                              │
│   1787 │   │   called_always_called_hooks = set()                                                                                                                                         │
│                                                                                                                                                                                           │
│ ╭─────────────────────────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────────────────────────╮ │
│ │         args = ()                                                                                                                                                                     │ │
│ │ forward_call = <bound method GptOssForCausalLM.forward of GptOssForCausalLM(                                                                                                          │ │
│ │                  (model): GptOssModel(                                                                                                                                                │ │
│ │                │   (embed_tokens): Embedding(201088, 2880, padding_idx=199999)                                                                                                        │ │
│ │                │   (layers): ModuleList(                                                                                                                                              │ │
│ │                │     (0-23): 24 x GptOssDecoderLayer(                                                                                                                                 │ │
│ │                │   │   (self_attn): GptOssAttention(                                                                                                                                  │ │
│ │                │   │     (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                             │ │
│ │                │   │     (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                              │ │
│ │                │   │     (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                              │ │
│ │                │   │     (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                             │ │
│ │                │   │   )                                                                                                                                                              │ │
│ │                │   │   (mlp): GptOssMLP(                                                                                                                                              │ │
│ │                │   │     (router): GptOssTopKRouter()                                                                                                                                 │ │
│ │                │   │     (experts): GptOssExperts()                                                                                                                                   │ │
│ │                │   │   )                                                                                                                                                              │ │
│ │                │   │   (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                           │ │
│ │                │   │   (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                  │ │
│ │                │     )                                                                                                                                                                │ │
│ │                │   )                                                                                                                                                                  │ │
│ │                │   (norm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                                          │ │
│ │                │   (rotary_emb): GptOssRotaryEmbedding()                                                                                                                              │ │
│ │                  )                                                                                                                                                                    │ │
│ │                  (lm_head): Linear(in_features=2880, out_features=201088, bias=False)                                                                                                 │ │
│ │                )>                                                                                                                                                                     │ │
│ │       kwargs = {                                                                                                                                                                      │ │
│ │                │   'input_ids': tensor([[1069]]),                                                                                                                                     │ │
│ │                │   'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),                                                                                               │ │
│ │                │   'past_key_values': DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, │ │
│ │                DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,    │ │
│ │                DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,    │ │
│ │                DynamicSlidingWindowLayer, DynamicLayer]),                                                                                                                             │ │
│ │                │   'cache_position': tensor([12]),                                                                                                                                    │ │
│ │                │   'use_cache': True                                                                                                                                                  │ │
│ │                }                                                                                                                                                                      │ │
│ │         self = GptOssForCausalLM(                                                                                                                                                     │ │
│ │                  (model): GptOssModel(                                                                                                                                                │ │
│ │                │   (embed_tokens): Embedding(201088, 2880, padding_idx=199999)                                                                                                        │ │
│ │                │   (layers): ModuleList(                                                                                                                                              │ │
│ │                │     (0-23): 24 x GptOssDecoderLayer(                                                                                                                                 │ │
│ │                │   │   (self_attn): GptOssAttention(                                                                                                                                  │ │
│ │                │   │     (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                             │ │
│ │                │   │     (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                              │ │
│ │                │   │     (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                              │ │
│ │                │   │     (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                             │ │
│ │                │   │   )                                                                                                                                                              │ │
│ │                │   │   (mlp): GptOssMLP(                                                                                                                                              │ │
│ │                │   │     (router): GptOssTopKRouter()                                                                                                                                 │ │
│ │                │   │     (experts): GptOssExperts()                                                                                                                                   │ │
│ │                │   │   )                                                                                                                                                              │ │
│ │                │   │   (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                           │ │
│ │                │   │   (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                  │ │
│ │                │     )                                                                                                                                                                │ │
│ │                │   )                                                                                                                                                                  │ │
│ │                │   (norm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                                          │ │
│ │                │   (rotary_emb): GptOssRotaryEmbedding()                                                                                                                              │ │
│ │                  )                                                                                                                                                                    │ │
│ │                  (lm_head): Linear(in_features=2880, out_features=201088, bias=False)                                                                                                 │ │
│ │                )                                                                                                                                                                      │ │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\transformers\utils\generic.py:940 in wrapper                                                                      │
│                                                                                                                                                                                           │
│    937 │   │   return_dict_passed = kwargs.pop("return_dict", return_dict)                                                                                                                │
│    938 │   │   if return_dict_passed is not None:                                                                                                                                         │
│    939 │   │   │   return_dict = return_dict_passed                                                                                                                                       │
│ ❱  940 │   │   output = func(self, *args, **kwargs)                                                                                                                                       │
│    941 │   │   if not return_dict and not isinstance(output, tuple):                                                                                                                      │
│    942 │   │   │   output = output.to_tuple()                                                                                                                                             │
│    943 │   │   return output                                                                                                                                                              │
│                                                                                                                                                                                           │
│ ╭─────────────────────────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────────────────────────╮ │
│ │               args = ()                                                                                                                                                               │ │
│ │             kwargs = {                                                                                                                                                                │ │
│ │                      │   'input_ids': tensor([[1069]]),                                                                                                                               │ │
│ │                      │   'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),                                                                                         │ │
│ │                      │   'past_key_values': DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,         │ │
│ │                      DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                         │ │
│ │                      DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,            │ │
│ │                      DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]),                                                                │ │
│ │                      │   'cache_position': tensor([12]),                                                                                                                              │ │
│ │                      │   'use_cache': True                                                                                                                                            │ │
│ │                      }                                                                                                                                                                │ │
│ │        return_dict = True                                                                                                                                                             │ │
│ │ return_dict_passed = True                                                                                                                                                             │ │
│ │               self = GptOssForCausalLM(                                                                                                                                               │ │
│ │                        (model): GptOssModel(                                                                                                                                          │ │
│ │                      │   (embed_tokens): Embedding(201088, 2880, padding_idx=199999)                                                                                                  │ │
│ │                      │   (layers): ModuleList(                                                                                                                                        │ │
│ │                      │     (0-23): 24 x GptOssDecoderLayer(                                                                                                                           │ │
│ │                      │   │   (self_attn): GptOssAttention(                                                                                                                            │ │
│ │                      │   │     (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                       │ │
│ │                      │   │     (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                        │ │
│ │                      │   │     (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                        │ │
│ │                      │   │     (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                       │ │
│ │                      │   │   )                                                                                                                                                        │ │
│ │                      │   │   (mlp): GptOssMLP(                                                                                                                                        │ │
│ │                      │   │     (router): GptOssTopKRouter()                                                                                                                           │ │
│ │                      │   │     (experts): GptOssExperts()                                                                                                                             │ │
│ │                      │   │   )                                                                                                                                                        │ │
│ │                      │   │   (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                     │ │
│ │                      │   │   (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                            │ │
│ │                      │     )                                                                                                                                                          │ │
│ │                      │   )                                                                                                                                                            │ │
│ │                      │   (norm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                                    │ │
│ │                      │   (rotary_emb): GptOssRotaryEmbedding()                                                                                                                        │ │
│ │                        )                                                                                                                                                              │ │
│ │                        (lm_head): Linear(in_features=2880, out_features=201088, bias=False)                                                                                           │ │
│ │                      )                                                                                                                                                                │ │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\transformers\models\gpt_oss\modeling_gpt_oss.py:663 in forward                                                    │
│                                                                                                                                                                                           │
│   660 │   │   )                                                                                                                                                                           │
│   661 │   │                                                                                                                                                                               │
│   662 │   │   # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)                                                                                             │
│ ❱ 663 │   │   outputs: MoeModelOutputWithPast = self.model(                                                                                                                               │
│   664 │   │   │   input_ids=input_ids,                                                                                                                                                    │
│   665 │   │   │   attention_mask=attention_mask,                                                                                                                                          │
│   666 │   │   │   position_ids=position_ids,                                                                                                                                              │
│                                                                                                                                                                                           │
│ ╭─────────────────────────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────────────────────────╮ │
│ │       attention_mask = tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])                                                                                                              │ │
│ │       cache_position = tensor([12])                                                                                                                                                   │ │
│ │            input_ids = tensor([[1069]])                                                                                                                                               │ │
│ │        inputs_embeds = None                                                                                                                                                           │ │
│ │               kwargs = {}                                                                                                                                                             │ │
│ │               labels = None                                                                                                                                                           │ │
│ │       logits_to_keep = 0                                                                                                                                                              │ │
│ │ output_router_logits = False                                                                                                                                                          │ │
│ │      past_key_values = DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                │ │
│ │                        DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,          │ │
│ │                        DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                       │ │
│ │                        DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer])                                                                             │ │
│ │         position_ids = None                                                                                                                                                           │ │
│ │                 self = GptOssForCausalLM(                                                                                                                                             │ │
│ │                          (model): GptOssModel(                                                                                                                                        │ │
│ │                        │   (embed_tokens): Embedding(201088, 2880, padding_idx=199999)                                                                                                │ │
│ │                        │   (layers): ModuleList(                                                                                                                                      │ │
│ │                        │     (0-23): 24 x GptOssDecoderLayer(                                                                                                                         │ │
│ │                        │   │   (self_attn): GptOssAttention(                                                                                                                          │ │
│ │                        │   │     (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                     │ │
│ │                        │   │     (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                      │ │
│ │                        │   │     (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                      │ │
│ │                        │   │     (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                     │ │
│ │                        │   │   )                                                                                                                                                      │ │
│ │                        │   │   (mlp): GptOssMLP(                                                                                                                                      │ │
│ │                        │   │     (router): GptOssTopKRouter()                                                                                                                         │ │
│ │                        │   │     (experts): GptOssExperts()                                                                                                                           │ │
│ │                        │   │   )                                                                                                                                                      │ │
│ │                        │   │   (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                   │ │
│ │                        │   │   (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                          │ │
│ │                        │     )                                                                                                                                                        │ │
│ │                        │   )                                                                                                                                                          │ │
│ │                        │   (norm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                                  │ │
│ │                        │   (rotary_emb): GptOssRotaryEmbedding()                                                                                                                      │ │
│ │                          )                                                                                                                                                            │ │
│ │                          (lm_head): Linear(in_features=2880, out_features=201088, bias=False)                                                                                         │ │
│ │                        )                                                                                                                                                              │ │
│ │            use_cache = True                                                                                                                                                           │ │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\torch\nn\modules\module.py:1773 in _wrapped_call_impl                                                             │
│                                                                                                                                                                                           │
│   1770 │   │   if self._compiled_call_impl is not None:                                                                                                                                   │
│   1771 │   │   │   return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]                                                                                                 │
│   1772 │   │   else:                                                                                                                                                                      │
│ ❱ 1773 │   │   │   return self._call_impl(*args, **kwargs)                                                                                                                                │
│   1774 │                                                                                                                                                                                  │
│   1775 │   # torchrec tests the code consistency with the following code                                                                                                                  │
│   1776 │   # fmt: off                                                                                                                                                                     │
│                                                                                                                                                                                           │
│ ╭─────────────────────────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────────────────────────╮ │
│ │   args = ()                                                                                                                                                                           │ │
│ │ kwargs = {                                                                                                                                                                            │ │
│ │          │   'input_ids': tensor([[1069]]),                                                                                                                                           │ │
│ │          │   'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),                                                                                                     │ │
│ │          │   'position_ids': None,                                                                                                                                                    │ │
│ │          │   'past_key_values': DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,       │ │
│ │          DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,          │ │
│ │          DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,          │ │
│ │          DynamicSlidingWindowLayer, DynamicLayer]),                                                                                                                                   │ │
│ │          │   'inputs_embeds': None,                                                                                                                                                   │ │
│ │          │   'use_cache': True,                                                                                                                                                       │ │
│ │          │   'output_router_logits': False,                                                                                                                                           │ │
│ │          │   'cache_position': tensor([12])                                                                                                                                           │ │
│ │          }                                                                                                                                                                            │ │
│ │   self = GptOssModel(                                                                                                                                                                 │ │
│ │            (embed_tokens): Embedding(201088, 2880, padding_idx=199999)                                                                                                                │ │
│ │            (layers): ModuleList(                                                                                                                                                      │ │
│ │          │   (0-23): 24 x GptOssDecoderLayer(                                                                                                                                         │ │
│ │          │     (self_attn): GptOssAttention(                                                                                                                                          │ │
│ │          │   │   (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                                     │ │
│ │          │   │   (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                      │ │
│ │          │   │   (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                      │ │
│ │          │   │   (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                                     │ │
│ │          │     )                                                                                                                                                                      │ │
│ │          │     (mlp): GptOssMLP(                                                                                                                                                      │ │
│ │          │   │   (router): GptOssTopKRouter()                                                                                                                                         │ │
│ │          │   │   (experts): GptOssExperts()                                                                                                                                           │ │
│ │          │     )                                                                                                                                                                      │ │
│ │          │     (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                                   │ │
│ │          │     (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                          │ │
│ │          │   )                                                                                                                                                                        │ │
│ │            )                                                                                                                                                                          │ │
│ │            (norm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                                                  │ │
│ │            (rotary_emb): GptOssRotaryEmbedding()                                                                                                                                      │ │
│ │          )                                                                                                                                                                            │ │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\torch\nn\modules\module.py:1784 in _call_impl                                                                     │
│                                                                                                                                                                                           │
│   1781 │   │   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks                                                                                            │
│   1782 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hooks                                                                                                            │
│   1783 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks):                                                                                                            │
│ ❱ 1784 │   │   │   return forward_call(*args, **kwargs)                                                                                                                                   │
│   1785 │   │                                                                                                                                                                              │
│   1786 │   │   result = None                                                                                                                                                              │
│   1787 │   │   called_always_called_hooks = set()                                                                                                                                         │
│                                                                                                                                                                                           │
│ ╭─────────────────────────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────────────────────────╮ │
│ │         args = ()                                                                                                                                                                     │ │
│ │ forward_call = <bound method GptOssModel.forward of GptOssModel(                                                                                                                      │ │
│ │                  (embed_tokens): Embedding(201088, 2880, padding_idx=199999)                                                                                                          │ │
│ │                  (layers): ModuleList(                                                                                                                                                │ │
│ │                │   (0-23): 24 x GptOssDecoderLayer(                                                                                                                                   │ │
│ │                │     (self_attn): GptOssAttention(                                                                                                                                    │ │
│ │                │   │   (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                               │ │
│ │                │   │   (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                │ │
│ │                │   │   (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                │ │
│ │                │   │   (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                               │ │
│ │                │     )                                                                                                                                                                │ │
│ │                │     (mlp): GptOssMLP(                                                                                                                                                │ │
│ │                │   │   (router): GptOssTopKRouter()                                                                                                                                   │ │
│ │                │   │   (experts): GptOssExperts()                                                                                                                                     │ │
│ │                │     )                                                                                                                                                                │ │
│ │                │     (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                             │ │
│ │                │     (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                    │ │
│ │                │   )                                                                                                                                                                  │ │
│ │                  )                                                                                                                                                                    │ │
│ │                  (norm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                                            │ │
│ │                  (rotary_emb): GptOssRotaryEmbedding()                                                                                                                                │ │
│ │                )>                                                                                                                                                                     │ │
│ │       kwargs = {                                                                                                                                                                      │ │
│ │                │   'input_ids': tensor([[1069]]),                                                                                                                                     │ │
│ │                │   'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),                                                                                               │ │
│ │                │   'position_ids': None,                                                                                                                                              │ │
│ │                │   'past_key_values': DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, │ │
│ │                DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,    │ │
│ │                DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,    │ │
│ │                DynamicSlidingWindowLayer, DynamicLayer]),                                                                                                                             │ │
│ │                │   'inputs_embeds': None,                                                                                                                                             │ │
│ │                │   'use_cache': True,                                                                                                                                                 │ │
│ │                │   'output_router_logits': False,                                                                                                                                     │ │
│ │                │   'cache_position': tensor([12])                                                                                                                                     │ │
│ │                }                                                                                                                                                                      │ │
│ │         self = GptOssModel(                                                                                                                                                           │ │
│ │                  (embed_tokens): Embedding(201088, 2880, padding_idx=199999)                                                                                                          │ │
│ │                  (layers): ModuleList(                                                                                                                                                │ │
│ │                │   (0-23): 24 x GptOssDecoderLayer(                                                                                                                                   │ │
│ │                │     (self_attn): GptOssAttention(                                                                                                                                    │ │
│ │                │   │   (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                               │ │
│ │                │   │   (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                │ │
│ │                │   │   (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                │ │
│ │                │   │   (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                               │ │
│ │                │     )                                                                                                                                                                │ │
│ │                │     (mlp): GptOssMLP(                                                                                                                                                │ │
│ │                │   │   (router): GptOssTopKRouter()                                                                                                                                   │ │
│ │                │   │   (experts): GptOssExperts()                                                                                                                                     │ │
│ │                │     )                                                                                                                                                                │ │
│ │                │     (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                             │ │
│ │                │     (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                    │ │
│ │                │   )                                                                                                                                                                  │ │
│ │                  )                                                                                                                                                                    │ │
│ │                  (norm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                                            │ │
│ │                  (rotary_emb): GptOssRotaryEmbedding()                                                                                                                                │ │
│ │                )                                                                                                                                                                      │ │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\transformers\utils\generic.py:1064 in wrapper                                                                     │
│                                                                                                                                                                                           │
│   1061 │   │   │   │   │   │   module.forward = make_capture_wrapper(module, original_forward,                                                                                            │
│   1062 │   │   │   │   │   │   monkey_patched_layers.append((module, original_forward))                                                                                                   │
│   1063 │   │                                                                                                                                                                              │
│ ❱ 1064 │   │   outputs = func(self, *args, **kwargs)                                                                                                                                      │
│   1065 │   │   # Restore original forward methods                                                                                                                                         │
│   1066 │   │   for module, original_forward in monkey_patched_layers:                                                                                                                     │
│   1067 │   │   │   module.forward = original_forward                                                                                                                                      │
│                                                                                                                                                                                           │
│ ╭─────────────────────────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────────────────────────╮ │
│ │              all_args = {                                                                                                                                                             │ │
│ │                         │   'input_ids': tensor([[1069]]),                                                                                                                            │ │
│ │                         │   'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),                                                                                      │ │
│ │                         │   'position_ids': None,                                                                                                                                     │ │
│ │                         │   'past_key_values': DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,      │ │
│ │                         DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                      │ │
│ │                         DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,         │ │
│ │                         DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]),                                                             │ │
│ │                         │   'inputs_embeds': None,                                                                                                                                    │ │
│ │                         │   'use_cache': True,                                                                                                                                        │ │
│ │                         │   'output_router_logits': False,                                                                                                                            │ │
│ │                         │   'cache_position': tensor([12])                                                                                                                            │ │
│ │                         }                                                                                                                                                             │ │
│ │                  args = ()                                                                                                                                                            │ │
│ │         capture_flags = {                                                                                                                                                             │ │
│ │                         │   'router_logits': OutputRecorder(                                                                                                                          │ │
│ │                         │   │   target_class=<class 'transformers.models.gpt_oss.modeling_gpt_oss.GptOssTopKRouter'>,                                                                 │ │
│ │                         │   │   index=0,                                                                                                                                              │ │
│ │                         │   │   layer_name=None,                                                                                                                                      │ │
│ │                         │   │   class_name=None                                                                                                                                       │ │
│ │                         │   ),                                                                                                                                                        │ │
│ │                         │   'hidden_states': <class 'transformers.models.gpt_oss.modeling_gpt_oss.GptOssDecoderLayer'>,                                                               │ │
│ │                         │   'attentions': <class 'transformers.models.gpt_oss.modeling_gpt_oss.GptOssAttention'>                                                                      │ │
│ │                         }                                                                                                                                                             │ │
│ │     collected_outputs = defaultdict(<class 'tuple'>, {})                                                                                                                              │ │
│ │                kwargs = {                                                                                                                                                             │ │
│ │                         │   'input_ids': tensor([[1069]]),                                                                                                                            │ │
│ │                         │   'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),                                                                                      │ │
│ │                         │   'position_ids': None,                                                                                                                                     │ │
│ │                         │   'past_key_values': DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,      │ │
│ │                         DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                      │ │
│ │                         DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,         │ │
│ │                         DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]),                                                             │ │
│ │                         │   'inputs_embeds': None,                                                                                                                                    │ │
│ │                         │   'use_cache': True,                                                                                                                                        │ │
│ │                         │   'output_router_logits': False,                                                                                                                            │ │
│ │                         │   'cache_position': tensor([12])                                                                                                                            │ │
│ │                         }                                                                                                                                                             │ │
│ │ monkey_patched_layers = []                                                                                                                                                            │ │
│ │       recordable_keys = {'output_router_logits': False, 'output_hidden_states': False, 'output_attentions': False}                                                                    │ │
│ │           return_dict = True                                                                                                                                                          │ │
│ │                  self = GptOssModel(                                                                                                                                                  │ │
│ │                           (embed_tokens): Embedding(201088, 2880, padding_idx=199999)                                                                                                 │ │
│ │                           (layers): ModuleList(                                                                                                                                       │ │
│ │                         │   (0-23): 24 x GptOssDecoderLayer(                                                                                                                          │ │
│ │                         │     (self_attn): GptOssAttention(                                                                                                                           │ │
│ │                         │   │   (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                      │ │
│ │                         │   │   (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                       │ │
│ │                         │   │   (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                       │ │
│ │                         │   │   (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                      │ │
│ │                         │     )                                                                                                                                                       │ │
│ │                         │     (mlp): GptOssMLP(                                                                                                                                       │ │
│ │                         │   │   (router): GptOssTopKRouter()                                                                                                                          │ │
│ │                         │   │   (experts): GptOssExperts()                                                                                                                            │ │
│ │                         │     )                                                                                                                                                       │ │
│ │                         │     (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                    │ │
│ │                         │     (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                           │ │
│ │                         │   )                                                                                                                                                         │ │
│ │                           )                                                                                                                                                           │ │
│ │                           (norm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                                   │ │
│ │                           (rotary_emb): GptOssRotaryEmbedding()                                                                                                                       │ │
│ │                         )                                                                                                                                                             │ │
│ │             use_cache = True                                                                                                                                                          │ │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\transformers\models\gpt_oss\modeling_gpt_oss.py:502 in forward                                                    │
│                                                                                                                                                                                           │
│   499 │   │   position_embeddings = self.rotary_emb(hidden_states, position_ids)                                                                                                          │
│   500 │   │                                                                                                                                                                               │
│   501 │   │   for decoder_layer in self.layers:                                                                                                                                           │
│ ❱ 502 │   │   │   hidden_states = decoder_layer(                                                                                                                                          │
│   503 │   │   │   │   hidden_states,                                                                                                                                                      │
│   504 │   │   │   │   attention_mask=causal_mask_mapping[decoder_layer.attention_type],                                                                                                   │
│   505 │   │   │   │   position_ids=position_ids,                                                                                                                                          │
│                                                                                                                                                                                           │
│ ╭─────────────────────────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────────────────────────╮ │
│ │      attention_mask = tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])                                                                                                               │ │
│ │      cache_position = tensor([12])                                                                                                                                                    │ │
│ │ causal_mask_mapping = {                                                                                                                                                               │ │
│ │                       │   'full_attention': tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],                                                                        │ │
│ │                       │      dtype=torch.bfloat16),                                                                                                                                   │ │
│ │                       │   'sliding_attention': tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],                                                                     │ │
│ │                       │      dtype=torch.bfloat16)                                                                                                                                    │ │
│ │                       }                                                                                                                                                               │ │
│ │       decoder_layer = GptOssDecoderLayer(                                                                                                                                             │ │
│ │                         (self_attn): GptOssAttention(                                                                                                                                 │ │
│ │                       │   (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                            │ │
│ │                       │   (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                             │ │
│ │                       │   (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                             │ │
│ │                       │   (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                            │ │
│ │                         )                                                                                                                                                             │ │
│ │                         (mlp): GptOssMLP(                                                                                                                                             │ │
│ │                       │   (router): GptOssTopKRouter()                                                                                                                                │ │
│ │                       │   (experts): GptOssExperts()                                                                                                                                  │ │
│ │                         )                                                                                                                                                             │ │
│ │                         (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                          │ │
│ │                         (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                 │ │
│ │                       )                                                                                                                                                               │ │
│ │       hidden_states = tensor([[[ 0.3711, -0.0996,  0.6172,  ...,  0.0026, -0.0210,  0.4883]]],                                                                                        │ │
│ │                       │      dtype=torch.bfloat16, grad_fn=<AddBackward0>)                                                                                                            │ │
│ │           input_ids = tensor([[1069]])                                                                                                                                                │ │
│ │       inputs_embeds = tensor([[[0.2178, 0.8984, 0.9688,  ..., 0.0442, 0.0147, 0.3379]]],                                                                                              │ │
│ │                       │      dtype=torch.bfloat16, grad_fn=<EmbeddingBackward0>)                                                                                                      │ │
│ │              kwargs = {'output_router_logits': False}                                                                                                                                 │ │
│ │         mask_kwargs = {                                                                                                                                                               │ │
│ │                       │   'config': GptOssConfig {                                                                                                                                    │ │
│ │                         "architectures": [                                                                                                                                            │ │
│ │                       │   "GptOssForCausalLM"                                                                                                                                         │ │
│ │                         ],                                                                                                                                                            │ │
│ │                         "attention_bias": true,                                                                                                                                       │ │
│ │                         "attention_dropout": 0.0,                                                                                                                                     │ │
│ │                         "dtype": "bfloat16",                                                                                                                                          │ │
│ │                         "eos_token_id": 200002,                                                                                                                                       │ │
│ │                         "experts_per_token": 4,                                                                                                                                       │ │
│ │                         "head_dim": 64,                                                                                                                                               │ │
│ │                         "hidden_act": "silu",                                                                                                                                         │ │
│ │                         "hidden_size": 2880,                                                                                                                                          │ │
│ │                         "initial_context_length": 4096,                                                                                                                               │ │
│ │                         "initializer_range": 0.02,                                                                                                                                    │ │
│ │                         "intermediate_size": 2880,                                                                                                                                    │ │
│ │                         "layer_types": [                                                                                                                                              │ │
│ │                       │   "sliding_attention",                                                                                                                                        │ │
│ │                       │   "full_attention",                                                                                                                                           │ │
│ │                       │   "sliding_attention",                                                                                                                                        │ │
│ │                       │   "full_attention",                                                                                                                                           │ │
│ │                       │   "sliding_attention",                                                                                                                                        │ │
│ │                       │   "full_attention",                                                                                                                                           │ │
│ │                       │   "sliding_attention",                                                                                                                                        │ │
│ │                       │   "full_attention",                                                                                                                                           │ │
│ │                       │   "sliding_attention",                                                                                                                                        │ │
│ │                       │   "full_attention",                                                                                                                                           │ │
│ │                       │   "sliding_attention",                                                                                                                                        │ │
│ │                       │   "full_attention",                                                                                                                                           │ │
│ │                       │   "sliding_attention",                                                                                                                                        │ │
│ │                       │   "full_attention",                                                                                                                                           │ │
│ │                       │   "sliding_attention",                                                                                                                                        │ │
│ │                       │   "full_attention",                                                                                                                                           │ │
│ │                       │   "sliding_attention",                                                                                                                                        │ │
│ │                       │   "full_attention",                                                                                                                                           │ │
│ │                       │   "sliding_attention",                                                                                                                                        │ │
│ │                       │   "full_attention",                                                                                                                                           │ │
│ │                       │   "sliding_attention",                                                                                                                                        │ │
│ │                       │   "full_attention",                                                                                                                                           │ │
│ │                       │   "sliding_attention",                                                                                                                                        │ │
│ │                       │   "full_attention"                                                                                                                                            │ │
│ │                         ],                                                                                                                                                            │ │
│ │                         "max_position_embeddings": 131072,                                                                                                                            │ │
│ │                         "model_type": "gpt_oss",                                                                                                                                      │ │
│ │                         "num_attention_heads": 64,                                                                                                                                    │ │
│ │                         "num_experts_per_tok": 4,                                                                                                                                     │ │
│ │                         "num_hidden_layers": 24,                                                                                                                                      │ │
│ │                         "num_key_value_heads": 8,                                                                                                                                     │ │
│ │                         "num_local_experts": 32,                                                                                                                                      │ │
│ │                         "output_router_logits": false,                                                                                                                                │ │
│ │                         "pad_token_id": 199999,                                                                                                                                       │ │
│ │                         "rms_norm_eps": 1e-05,                                                                                                                                        │ │
│ │                         "rope_scaling": {                                                                                                                                             │ │
│ │                       │   "beta_fast": 32.0,                                                                                                                                          │ │
│ │                       │   "beta_slow": 1.0,                                                                                                                                           │ │
│ │                       │   "factor": 32.0,                                                                                                                                             │ │
│ │                       │   "original_max_position_embeddings": 4096,                                                                                                                   │ │
│ │                       │   "rope_type": "yarn",                                                                                                                                        │ │
│ │                       │   "truncate": false                                                                                                                                           │ │
│ │                         },                                                                                                                                                            │ │
│ │                         "rope_theta": 150000,                                                                                                                                         │ │
│ │                         "router_aux_loss_coef": 0.9,                                                                                                                                  │ │
│ │                         "sliding_window": 128,                                                                                                                                        │ │
│ │                         "swiglu_limit": 7.0,                                                                                                                                          │ │
│ │                         "tie_word_embeddings": false,                                                                                                                                 │ │
│ │                         "transformers_version": "4.56.2",                                                                                                                             │ │
│ │                         "use_cache": true,                                                                                                                                            │ │
│ │                         "vocab_size": 201088                                                                                                                                          │ │
│ │                       }                                                                                                                                                               │ │
│ │                       ,                                                                                                                                                               │ │
│ │                       │   'input_embeds': tensor([[[0.2178, 0.8984, 0.9688,  ..., 0.0442, 0.0147, 0.3379]]],                                                                          │ │
│ │                       │      dtype=torch.bfloat16, grad_fn=<EmbeddingBackward0>),                                                                                                     │ │
│ │                       │   'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),                                                                                        │ │
│ │                       │   'cache_position': tensor([12]),                                                                                                                             │ │
│ │                       │   'past_key_values': DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,        │ │
│ │                       DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                        │ │
│ │                       DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,           │ │
│ │                       DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer])                                                                │ │
│ │                       }                                                                                                                                                               │ │
│ │     past_key_values = DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                 │ │
│ │                       DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,           │ │
│ │                       DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                        │ │
│ │                       DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer])                                                                              │ │
│ │ position_embeddings = (                                                                                                                                                               │ │
│ │                       │   tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, -1.2188, -0.3887,  0.3809,                                                                                    │ │
│ │                       │   │      0.8516,  1.1016,  1.2500,  1.3125,  1.3359,  1.3438,  1.3438,                                                                                        │ │
│ │                       │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                                        │ │
│ │                       │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                                        │ │
│ │                       │   │      1.3438,  1.3438,  1.3438,  1.3438]]], dtype=torch.bfloat16),                                                                                         │ │
│ │                       │   tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01, -9.4922e-01,  5.7031e-01,                                                                                  │ │
│ │                       │   │      1.2891e+00,  1.2891e+00,  1.0391e+00,  7.6953e-01,  5.0000e-01,                                                                                      │ │
│ │                       │   │      3.1055e-01,  1.8652e-01,  1.0986e-01,  6.2256e-02,  3.3936e-02,                                                                                      │ │
│ │                       │   │      1.6968e-02,  7.3853e-03,  2.0905e-03,  6.1798e-04,  4.2725e-04,                                                                                      │ │
│ │                       │   │      2.9373e-04,  2.0218e-04,  1.3924e-04,  9.6321e-05,  6.6280e-05,                                                                                      │ │
│ │                       │   │      4.5538e-05,  3.1471e-05,  2.1696e-05,  1.4961e-05,  1.0312e-05,                                                                                      │ │
│ │                       │   │      7.0930e-06,  4.8876e-06]]], dtype=torch.bfloat16)                                                                                                    │ │
│ │                       )                                                                                                                                                               │ │
│ │        position_ids = tensor([[12]])                                                                                                                                                  │ │
│ │                self = GptOssModel(                                                                                                                                                    │ │
│ │                         (embed_tokens): Embedding(201088, 2880, padding_idx=199999)                                                                                                   │ │
│ │                         (layers): ModuleList(                                                                                                                                         │ │
│ │                       │   (0-23): 24 x GptOssDecoderLayer(                                                                                                                            │ │
│ │                       │     (self_attn): GptOssAttention(                                                                                                                             │ │
│ │                       │   │   (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                        │ │
│ │                       │   │   (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                         │ │
│ │                       │   │   (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                         │ │
│ │                       │   │   (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                        │ │
│ │                       │     )                                                                                                                                                         │ │
│ │                       │     (mlp): GptOssMLP(                                                                                                                                         │ │
│ │                       │   │   (router): GptOssTopKRouter()                                                                                                                            │ │
│ │                       │   │   (experts): GptOssExperts()                                                                                                                              │ │
│ │                       │     )                                                                                                                                                         │ │
│ │                       │     (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                      │ │
│ │                       │     (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                             │ │
│ │                       │   )                                                                                                                                                           │ │
│ │                         )                                                                                                                                                             │ │
│ │                         (norm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                                     │ │
│ │                         (rotary_emb): GptOssRotaryEmbedding()                                                                                                                         │ │
│ │                       )                                                                                                                                                               │ │
│ │           use_cache = True                                                                                                                                                            │ │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\transformers\modeling_layers.py:94 in __call__                                                                    │
│                                                                                                                                                                                           │
│    91 │   │   │   │   logger.warning_once(message)                                                                                                                                        │
│    92 │   │   │                                                                                                                                                                           │
│    93 │   │   │   return self._gradient_checkpointing_func(partial(super().__call__, **kwargs)                                                                                            │
│ ❱  94 │   │   return super().__call__(*args, **kwargs)                                                                                                                                    │
│    95                                                                                                                                                                                     │
│    96                                                                                                                                                                                     │
│    97 @auto_docstring                                                                                                                                                                     │
│                                                                                                                                                                                           │
│ ╭─────────────────────────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────────────────────────╮ │
│ │   args = (tensor([[[ 0.3711, -0.0996,  0.6172,  ...,  0.0026, -0.0210,  0.4883]]],                                                                                                    │ │
│ │          │      dtype=torch.bfloat16, grad_fn=<AddBackward0>),)                                                                                                                       │ │
│ │ kwargs = {                                                                                                                                                                            │ │
│ │          │   'attention_mask': tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],                                                                                     │ │
│ │          │      dtype=torch.bfloat16),                                                                                                                                                │ │
│ │          │   'position_ids': tensor([[12]]),                                                                                                                                          │ │
│ │          │   'past_key_values': DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,       │ │
│ │          DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,          │ │
│ │          DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,          │ │
│ │          DynamicSlidingWindowLayer, DynamicLayer]),                                                                                                                                   │ │
│ │          │   'use_cache': True,                                                                                                                                                       │ │
│ │          │   'cache_position': tensor([12]),                                                                                                                                          │ │
│ │          │   'position_embeddings': (                                                                                                                                                 │ │
│ │          │   │   tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, -1.2188, -0.3887,  0.3809,                                                                                             │ │
│ │          │   │      0.8516,  1.1016,  1.2500,  1.3125,  1.3359,  1.3438,  1.3438,                                                                                                     │ │
│ │          │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                                                     │ │
│ │          │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                                                     │ │
│ │          │   │      1.3438,  1.3438,  1.3438,  1.3438]]], dtype=torch.bfloat16),                                                                                                      │ │
│ │          │   │   tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01, -9.4922e-01,  5.7031e-01,                                                                                           │ │
│ │          │   │      1.2891e+00,  1.2891e+00,  1.0391e+00,  7.6953e-01,  5.0000e-01,                                                                                                   │ │
│ │          │   │      3.1055e-01,  1.8652e-01,  1.0986e-01,  6.2256e-02,  3.3936e-02,                                                                                                   │ │
│ │          │   │      1.6968e-02,  7.3853e-03,  2.0905e-03,  6.1798e-04,  4.2725e-04,                                                                                                   │ │
│ │          │   │      2.9373e-04,  2.0218e-04,  1.3924e-04,  9.6321e-05,  6.6280e-05,                                                                                                   │ │
│ │          │   │      4.5538e-05,  3.1471e-05,  2.1696e-05,  1.4961e-05,  1.0312e-05,                                                                                                   │ │
│ │          │   │      7.0930e-06,  4.8876e-06]]], dtype=torch.bfloat16)                                                                                                                 │ │
│ │          │   ),                                                                                                                                                                       │ │
│ │          │   'output_router_logits': False                                                                                                                                            │ │
│ │          }                                                                                                                                                                            │ │
│ │   self = GptOssDecoderLayer(                                                                                                                                                          │ │
│ │            (self_attn): GptOssAttention(                                                                                                                                              │ │
│ │          │   (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                                         │ │
│ │          │   (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                          │ │
│ │          │   (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                          │ │
│ │          │   (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                                         │ │
│ │            )                                                                                                                                                                          │ │
│ │            (mlp): GptOssMLP(                                                                                                                                                          │ │
│ │          │   (router): GptOssTopKRouter()                                                                                                                                             │ │
│ │          │   (experts): GptOssExperts()                                                                                                                                               │ │
│ │            )                                                                                                                                                                          │ │
│ │            (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                                       │ │
│ │            (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                              │ │
│ │          )                                                                                                                                                                            │ │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\torch\nn\modules\module.py:1773 in _wrapped_call_impl                                                             │
│                                                                                                                                                                                           │
│   1770 │   │   if self._compiled_call_impl is not None:                                                                                                                                   │
│   1771 │   │   │   return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]                                                                                                 │
│   1772 │   │   else:                                                                                                                                                                      │
│ ❱ 1773 │   │   │   return self._call_impl(*args, **kwargs)                                                                                                                                │
│   1774 │                                                                                                                                                                                  │
│   1775 │   # torchrec tests the code consistency with the following code                                                                                                                  │
│   1776 │   # fmt: off                                                                                                                                                                     │
│                                                                                                                                                                                           │
│ ╭─────────────────────────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────────────────────────╮ │
│ │   args = (tensor([[[ 0.3711, -0.0996,  0.6172,  ...,  0.0026, -0.0210,  0.4883]]],                                                                                                    │ │
│ │          │      dtype=torch.bfloat16, grad_fn=<AddBackward0>),)                                                                                                                       │ │
│ │ kwargs = {                                                                                                                                                                            │ │
│ │          │   'attention_mask': tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],                                                                                     │ │
│ │          │      dtype=torch.bfloat16),                                                                                                                                                │ │
│ │          │   'position_ids': tensor([[12]]),                                                                                                                                          │ │
│ │          │   'past_key_values': DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,       │ │
│ │          DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,          │ │
│ │          DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,          │ │
│ │          DynamicSlidingWindowLayer, DynamicLayer]),                                                                                                                                   │ │
│ │          │   'use_cache': True,                                                                                                                                                       │ │
│ │          │   'cache_position': tensor([12]),                                                                                                                                          │ │
│ │          │   'position_embeddings': (                                                                                                                                                 │ │
│ │          │   │   tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, -1.2188, -0.3887,  0.3809,                                                                                             │ │
│ │          │   │      0.8516,  1.1016,  1.2500,  1.3125,  1.3359,  1.3438,  1.3438,                                                                                                     │ │
│ │          │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                                                     │ │
│ │          │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                                                     │ │
│ │          │   │      1.3438,  1.3438,  1.3438,  1.3438]]], dtype=torch.bfloat16),                                                                                                      │ │
│ │          │   │   tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01, -9.4922e-01,  5.7031e-01,                                                                                           │ │
│ │          │   │      1.2891e+00,  1.2891e+00,  1.0391e+00,  7.6953e-01,  5.0000e-01,                                                                                                   │ │
│ │          │   │      3.1055e-01,  1.8652e-01,  1.0986e-01,  6.2256e-02,  3.3936e-02,                                                                                                   │ │
│ │          │   │      1.6968e-02,  7.3853e-03,  2.0905e-03,  6.1798e-04,  4.2725e-04,                                                                                                   │ │
│ │          │   │      2.9373e-04,  2.0218e-04,  1.3924e-04,  9.6321e-05,  6.6280e-05,                                                                                                   │ │
│ │          │   │      4.5538e-05,  3.1471e-05,  2.1696e-05,  1.4961e-05,  1.0312e-05,                                                                                                   │ │
│ │          │   │      7.0930e-06,  4.8876e-06]]], dtype=torch.bfloat16)                                                                                                                 │ │
│ │          │   ),                                                                                                                                                                       │ │
│ │          │   'output_router_logits': False                                                                                                                                            │ │
│ │          }                                                                                                                                                                            │ │
│ │   self = GptOssDecoderLayer(                                                                                                                                                          │ │
│ │            (self_attn): GptOssAttention(                                                                                                                                              │ │
│ │          │   (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                                         │ │
│ │          │   (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                          │ │
│ │          │   (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                          │ │
│ │          │   (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                                         │ │
│ │            )                                                                                                                                                                          │ │
│ │            (mlp): GptOssMLP(                                                                                                                                                          │ │
│ │          │   (router): GptOssTopKRouter()                                                                                                                                             │ │
│ │          │   (experts): GptOssExperts()                                                                                                                                               │ │
│ │            )                                                                                                                                                                          │ │
│ │            (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                                       │ │
│ │            (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                              │ │
│ │          )                                                                                                                                                                            │ │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\torch\nn\modules\module.py:1784 in _call_impl                                                                     │
│                                                                                                                                                                                           │
│   1781 │   │   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks                                                                                            │
│   1782 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hooks                                                                                                            │
│   1783 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks):                                                                                                            │
│ ❱ 1784 │   │   │   return forward_call(*args, **kwargs)                                                                                                                                   │
│   1785 │   │                                                                                                                                                                              │
│   1786 │   │   result = None                                                                                                                                                              │
│   1787 │   │   called_always_called_hooks = set()                                                                                                                                         │
│                                                                                                                                                                                           │
│ ╭─────────────────────────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────────────────────────╮ │
│ │         args = (tensor([[[ 0.3711, -0.0996,  0.6172,  ...,  0.0026, -0.0210,  0.4883]]],                                                                                              │ │
│ │                │      dtype=torch.bfloat16, grad_fn=<AddBackward0>),)                                                                                                                 │ │
│ │ forward_call = <bound method GptOssDecoderLayer.forward of GptOssDecoderLayer(                                                                                                        │ │
│ │                  (self_attn): GptOssAttention(                                                                                                                                        │ │
│ │                │   (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                                   │ │
│ │                │   (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                    │ │
│ │                │   (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                    │ │
│ │                │   (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                                   │ │
│ │                  )                                                                                                                                                                    │ │
│ │                  (mlp): GptOssMLP(                                                                                                                                                    │ │
│ │                │   (router): GptOssTopKRouter()                                                                                                                                       │ │
│ │                │   (experts): GptOssExperts()                                                                                                                                         │ │
│ │                  )                                                                                                                                                                    │ │
│ │                  (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                                 │ │
│ │                  (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                        │ │
│ │                )>                                                                                                                                                                     │ │
│ │       kwargs = {                                                                                                                                                                      │ │
│ │                │   'attention_mask': tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],                                                                               │ │
│ │                │      dtype=torch.bfloat16),                                                                                                                                          │ │
│ │                │   'position_ids': tensor([[12]]),                                                                                                                                    │ │
│ │                │   'past_key_values': DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, │ │
│ │                DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,    │ │
│ │                DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,    │ │
│ │                DynamicSlidingWindowLayer, DynamicLayer]),                                                                                                                             │ │
│ │                │   'use_cache': True,                                                                                                                                                 │ │
│ │                │   'cache_position': tensor([12]),                                                                                                                                    │ │
│ │                │   'position_embeddings': (                                                                                                                                           │ │
│ │                │   │   tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, -1.2188, -0.3887,  0.3809,                                                                                       │ │
│ │                │   │      0.8516,  1.1016,  1.2500,  1.3125,  1.3359,  1.3438,  1.3438,                                                                                               │ │
│ │                │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                                               │ │
│ │                │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                                               │ │
│ │                │   │      1.3438,  1.3438,  1.3438,  1.3438]]], dtype=torch.bfloat16),                                                                                                │ │
│ │                │   │   tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01, -9.4922e-01,  5.7031e-01,                                                                                     │ │
│ │                │   │      1.2891e+00,  1.2891e+00,  1.0391e+00,  7.6953e-01,  5.0000e-01,                                                                                             │ │
│ │                │   │      3.1055e-01,  1.8652e-01,  1.0986e-01,  6.2256e-02,  3.3936e-02,                                                                                             │ │
│ │                │   │      1.6968e-02,  7.3853e-03,  2.0905e-03,  6.1798e-04,  4.2725e-04,                                                                                             │ │
│ │                │   │      2.9373e-04,  2.0218e-04,  1.3924e-04,  9.6321e-05,  6.6280e-05,                                                                                             │ │
│ │                │   │      4.5538e-05,  3.1471e-05,  2.1696e-05,  1.4961e-05,  1.0312e-05,                                                                                             │ │
│ │                │   │      7.0930e-06,  4.8876e-06]]], dtype=torch.bfloat16)                                                                                                           │ │
│ │                │   ),                                                                                                                                                                 │ │
│ │                │   'output_router_logits': False                                                                                                                                      │ │
│ │                }                                                                                                                                                                      │ │
│ │         self = GptOssDecoderLayer(                                                                                                                                                    │ │
│ │                  (self_attn): GptOssAttention(                                                                                                                                        │ │
│ │                │   (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                                   │ │
│ │                │   (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                    │ │
│ │                │   (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                    │ │
│ │                │   (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                                   │ │
│ │                  )                                                                                                                                                                    │ │
│ │                  (mlp): GptOssMLP(                                                                                                                                                    │ │
│ │                │   (router): GptOssTopKRouter()                                                                                                                                       │ │
│ │                │   (experts): GptOssExperts()                                                                                                                                         │ │
│ │                  )                                                                                                                                                                    │ │
│ │                  (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                                 │ │
│ │                  (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                        │ │
│ │                )                                                                                                                                                                      │ │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\transformers\utils\deprecation.py:172 in wrapped_func                                                             │
│                                                                                                                                                                                           │
│   169 │   │   │   │   # DeprecationWarning is ignored by default, so we use FutureWarning inst                                                                                            │
│   170 │   │   │   │   warnings.warn(message, FutureWarning, stacklevel=2)                                                                                                                 │
│   171 │   │   │                                                                                                                                                                           │
│ ❱ 172 │   │   │   return func(*args, **kwargs)                                                                                                                                            │
│   173 │   │                                                                                                                                                                               │
│   174 │   │   return wrapped_func                                                                                                                                                         │
│   175                                                                                                                                                                                     │
│                                                                                                                                                                                           │
│ ╭─────────────────────────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────────────────────────╮ │
│ │                additional_message = None                                                                                                                                              │ │
│ │                              args = (                                                                                                                                                 │ │
│ │                                     │   GptOssDecoderLayer(                                                                                                                           │ │
│ │                                       (self_attn): GptOssAttention(                                                                                                                   │ │
│ │                                     │   (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                              │ │
│ │                                     │   (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                               │ │
│ │                                     │   (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                               │ │
│ │                                     │   (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                              │ │
│ │                                       )                                                                                                                                               │ │
│ │                                       (mlp): GptOssMLP(                                                                                                                               │ │
│ │                                     │   (router): GptOssTopKRouter()                                                                                                                  │ │
│ │                                     │   (experts): GptOssExperts()                                                                                                                    │ │
│ │                                       )                                                                                                                                               │ │
│ │                                       (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                            │ │
│ │                                       (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                   │ │
│ │                                     ),                                                                                                                                                │ │
│ │                                     │   tensor([[[ 0.3711, -0.0996,  0.6172,  ...,  0.0026, -0.0210,  0.4883]]],                                                                      │ │
│ │                                     │      dtype=torch.bfloat16, grad_fn=<AddBackward0>)                                                                                              │ │
│ │                                     )                                                                                                                                                 │ │
│ │                         func_name = 'GptOssDecoderLayer.forward'                                                                                                                      │ │
│ │                   is_class_method = False                                                                                                                                             │ │
│ │       is_greater_or_equal_version = False                                                                                                                                             │ │
│ │                is_instance_method = True                                                                                                                                              │ │
│ │                            kwargs = {                                                                                                                                                 │ │
│ │                                     │   'attention_mask': tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],                                                          │ │
│ │                                     │      dtype=torch.bfloat16),                                                                                                                     │ │
│ │                                     │   'position_ids': tensor([[12]]),                                                                                                               │ │
│ │                                     │   'past_key_values': DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                     │ │
│ │                                     DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                        │ │
│ │                                     DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                        │ │
│ │                                     DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                        │ │
│ │                                     DynamicSlidingWindowLayer, DynamicLayer]),                                                                                                        │ │
│ │                                     │   'use_cache': True,                                                                                                                            │ │
│ │                                     │   'cache_position': tensor([12]),                                                                                                               │ │
│ │                                     │   'position_embeddings': (                                                                                                                      │ │
│ │                                     │   │   tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, -1.2188, -0.3887,  0.3809,                                                                  │ │
│ │                                     │   │      0.8516,  1.1016,  1.2500,  1.3125,  1.3359,  1.3438,  1.3438,                                                                          │ │
│ │                                     │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                          │ │
│ │                                     │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                          │ │
│ │                                     │   │      1.3438,  1.3438,  1.3438,  1.3438]]], dtype=torch.bfloat16),                                                                           │ │
│ │                                     │   │   tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01, -9.4922e-01,  5.7031e-01,                                                                │ │
│ │                                     │   │      1.2891e+00,  1.2891e+00,  1.0391e+00,  7.6953e-01,  5.0000e-01,                                                                        │ │
│ │                                     │   │      3.1055e-01,  1.8652e-01,  1.0986e-01,  6.2256e-02,  3.3936e-02,                                                                        │ │
│ │                                     │   │      1.6968e-02,  7.3853e-03,  2.0905e-03,  6.1798e-04,  4.2725e-04,                                                                        │ │
│ │                                     │   │      2.9373e-04,  2.0218e-04,  1.3924e-04,  9.6321e-05,  6.6280e-05,                                                                        │ │
│ │                                     │   │      4.5538e-05,  3.1471e-05,  2.1696e-05,  1.4961e-05,  1.0312e-05,                                                                        │ │
│ │                                     │   │      7.0930e-06,  4.8876e-06]]], dtype=torch.bfloat16)                                                                                      │ │
│ │                                     │   ),                                                                                                                                            │ │
│ │                                     │   'output_router_logits': False                                                                                                                 │ │
│ │                                     }                                                                                                                                                 │ │
│ │                           message = None                                                                                                                                              │ │
│ │                    minimum_action = <Action.NONE: 'none'>                                                                                                                             │ │
│ │                          new_name = 'past_key_values'                                                                                                                                 │ │
│ │                          old_name = 'past_key_value'                                                                                                                                  │ │
│ │               raise_if_both_names = False                                                                                                                                             │ │
│ │ raise_if_greater_or_equal_version = False                                                                                                                                             │ │
│ │                   version_message = 'and will be removed in version 4.58'                                                                                                             │ │
│ │  warn_if_greater_or_equal_version = False                                                                                                                                             │ │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\transformers\models\gpt_oss\modeling_gpt_oss.py:366 in forward                                                    │
│                                                                                                                                                                                           │
│   363 │   │   residual = hidden_states                                                                                                                                                    │
│   364 │   │   hidden_states = self.input_layernorm(hidden_states)                                                                                                                         │
│   365 │   │   # Self Attention                                                                                                                                                            │
│ ❱ 366 │   │   hidden_states, _ = self.self_attn(                                                                                                                                          │
│   367 │   │   │   hidden_states=hidden_states,                                                                                                                                            │
│   368 │   │   │   attention_mask=attention_mask,                                                                                                                                          │
│   369 │   │   │   position_ids=position_ids,                                                                                                                                              │
│                                                                                                                                                                                           │
│ ╭─────────────────────────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────────────────────────╮ │
│ │      attention_mask = tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],                                                                                              │ │
│ │                       │      dtype=torch.bfloat16)                                                                                                                                    │ │
│ │      cache_position = tensor([12])                                                                                                                                                    │ │
│ │       hidden_states = tensor([[[ 0.2090, -0.0486,  0.2520,  ...,  0.0030, -0.0250,  0.3516]]],                                                                                        │ │
│ │                       │      dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)                                                                                                         │ │
│ │              kwargs = {'output_router_logits': False}                                                                                                                                 │ │
│ │     past_key_values = DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                 │ │
│ │                       DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,           │ │
│ │                       DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                        │ │
│ │                       DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer])                                                                              │ │
│ │ position_embeddings = (                                                                                                                                                               │ │
│ │                       │   tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, -1.2188, -0.3887,  0.3809,                                                                                    │ │
│ │                       │   │      0.8516,  1.1016,  1.2500,  1.3125,  1.3359,  1.3438,  1.3438,                                                                                        │ │
│ │                       │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                                        │ │
│ │                       │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                                        │ │
│ │                       │   │      1.3438,  1.3438,  1.3438,  1.3438]]], dtype=torch.bfloat16),                                                                                         │ │
│ │                       │   tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01, -9.4922e-01,  5.7031e-01,                                                                                  │ │
│ │                       │   │      1.2891e+00,  1.2891e+00,  1.0391e+00,  7.6953e-01,  5.0000e-01,                                                                                      │ │
│ │                       │   │      3.1055e-01,  1.8652e-01,  1.0986e-01,  6.2256e-02,  3.3936e-02,                                                                                      │ │
│ │                       │   │      1.6968e-02,  7.3853e-03,  2.0905e-03,  6.1798e-04,  4.2725e-04,                                                                                      │ │
│ │                       │   │      2.9373e-04,  2.0218e-04,  1.3924e-04,  9.6321e-05,  6.6280e-05,                                                                                      │ │
│ │                       │   │      4.5538e-05,  3.1471e-05,  2.1696e-05,  1.4961e-05,  1.0312e-05,                                                                                      │ │
│ │                       │   │      7.0930e-06,  4.8876e-06]]], dtype=torch.bfloat16)                                                                                                    │ │
│ │                       )                                                                                                                                                               │ │
│ │        position_ids = tensor([[12]])                                                                                                                                                  │ │
│ │            residual = tensor([[[ 0.3711, -0.0996,  0.6172,  ...,  0.0026, -0.0210,  0.4883]]],                                                                                        │ │
│ │                       │      dtype=torch.bfloat16, grad_fn=<AddBackward0>)                                                                                                            │ │
│ │                self = GptOssDecoderLayer(                                                                                                                                             │ │
│ │                         (self_attn): GptOssAttention(                                                                                                                                 │ │
│ │                       │   (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                            │ │
│ │                       │   (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                             │ │
│ │                       │   (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                             │ │
│ │                       │   (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                            │ │
│ │                         )                                                                                                                                                             │ │
│ │                         (mlp): GptOssMLP(                                                                                                                                             │ │
│ │                       │   (router): GptOssTopKRouter()                                                                                                                                │ │
│ │                       │   (experts): GptOssExperts()                                                                                                                                  │ │
│ │                         )                                                                                                                                                             │ │
│ │                         (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                          │ │
│ │                         (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)                                                                                                 │ │
│ │                       )                                                                                                                                                               │ │
│ │           use_cache = True                                                                                                                                                            │ │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\torch\nn\modules\module.py:1773 in _wrapped_call_impl                                                             │
│                                                                                                                                                                                           │
│   1770 │   │   if self._compiled_call_impl is not None:                                                                                                                                   │
│   1771 │   │   │   return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]                                                                                                 │
│   1772 │   │   else:                                                                                                                                                                      │
│ ❱ 1773 │   │   │   return self._call_impl(*args, **kwargs)                                                                                                                                │
│   1774 │                                                                                                                                                                                  │
│   1775 │   # torchrec tests the code consistency with the following code                                                                                                                  │
│   1776 │   # fmt: off                                                                                                                                                                     │
│                                                                                                                                                                                           │
│ ╭─────────────────────────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────────────────────────╮ │
│ │   args = ()                                                                                                                                                                           │ │
│ │ kwargs = {                                                                                                                                                                            │ │
│ │          │   'hidden_states': tensor([[[ 0.2090, -0.0486,  0.2520,  ...,  0.0030, -0.0250,  0.3516]]],                                                                                │ │
│ │          │      dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>),                                                                                                                     │ │
│ │          │   'attention_mask': tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],                                                                                     │ │
│ │          │      dtype=torch.bfloat16),                                                                                                                                                │ │
│ │          │   'position_ids': tensor([[12]]),                                                                                                                                          │ │
│ │          │   'past_key_values': DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,       │ │
│ │          DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,          │ │
│ │          DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,          │ │
│ │          DynamicSlidingWindowLayer, DynamicLayer]),                                                                                                                                   │ │
│ │          │   'use_cache': True,                                                                                                                                                       │ │
│ │          │   'cache_position': tensor([12]),                                                                                                                                          │ │
│ │          │   'position_embeddings': (                                                                                                                                                 │ │
│ │          │   │   tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, -1.2188, -0.3887,  0.3809,                                                                                             │ │
│ │          │   │      0.8516,  1.1016,  1.2500,  1.3125,  1.3359,  1.3438,  1.3438,                                                                                                     │ │
│ │          │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                                                     │ │
│ │          │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                                                     │ │
│ │          │   │      1.3438,  1.3438,  1.3438,  1.3438]]], dtype=torch.bfloat16),                                                                                                      │ │
│ │          │   │   tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01, -9.4922e-01,  5.7031e-01,                                                                                           │ │
│ │          │   │      1.2891e+00,  1.2891e+00,  1.0391e+00,  7.6953e-01,  5.0000e-01,                                                                                                   │ │
│ │          │   │      3.1055e-01,  1.8652e-01,  1.0986e-01,  6.2256e-02,  3.3936e-02,                                                                                                   │ │
│ │          │   │      1.6968e-02,  7.3853e-03,  2.0905e-03,  6.1798e-04,  4.2725e-04,                                                                                                   │ │
│ │          │   │      2.9373e-04,  2.0218e-04,  1.3924e-04,  9.6321e-05,  6.6280e-05,                                                                                                   │ │
│ │          │   │      4.5538e-05,  3.1471e-05,  2.1696e-05,  1.4961e-05,  1.0312e-05,                                                                                                   │ │
│ │          │   │      7.0930e-06,  4.8876e-06]]], dtype=torch.bfloat16)                                                                                                                 │ │
│ │          │   ),                                                                                                                                                                       │ │
│ │          │   'output_router_logits': False                                                                                                                                            │ │
│ │          }                                                                                                                                                                            │ │
│ │   self = GptOssAttention(                                                                                                                                                             │ │
│ │            (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                                           │ │
│ │            (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                            │ │
│ │            (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                            │ │
│ │            (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                                           │ │
│ │          )                                                                                                                                                                            │ │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\torch\nn\modules\module.py:1784 in _call_impl                                                                     │
│                                                                                                                                                                                           │
│   1781 │   │   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks                                                                                            │
│   1782 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hooks                                                                                                            │
│   1783 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks):                                                                                                            │
│ ❱ 1784 │   │   │   return forward_call(*args, **kwargs)                                                                                                                                   │
│   1785 │   │                                                                                                                                                                              │
│   1786 │   │   result = None                                                                                                                                                              │
│   1787 │   │   called_always_called_hooks = set()                                                                                                                                         │
│                                                                                                                                                                                           │
│ ╭─────────────────────────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────────────────────────╮ │
│ │   args = ()                                                                                                                                                                           │ │
│ │ kwargs = {                                                                                                                                                                            │ │
│ │          │   'hidden_states': tensor([[[ 0.2090, -0.0486,  0.2520,  ...,  0.0030, -0.0250,  0.3516]]],                                                                                │ │
│ │          │      dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>),                                                                                                                     │ │
│ │          │   'attention_mask': tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],                                                                                     │ │
│ │          │      dtype=torch.bfloat16),                                                                                                                                                │ │
│ │          │   'position_ids': tensor([[12]]),                                                                                                                                          │ │
│ │          │   'past_key_values': DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,       │ │
│ │          DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,          │ │
│ │          DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,          │ │
│ │          DynamicSlidingWindowLayer, DynamicLayer]),                                                                                                                                   │ │
│ │          │   'use_cache': True,                                                                                                                                                       │ │
│ │          │   'cache_position': tensor([12]),                                                                                                                                          │ │
│ │          │   'position_embeddings': (                                                                                                                                                 │ │
│ │          │   │   tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, -1.2188, -0.3887,  0.3809,                                                                                             │ │
│ │          │   │      0.8516,  1.1016,  1.2500,  1.3125,  1.3359,  1.3438,  1.3438,                                                                                                     │ │
│ │          │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                                                     │ │
│ │          │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                                                     │ │
│ │          │   │      1.3438,  1.3438,  1.3438,  1.3438]]], dtype=torch.bfloat16),                                                                                                      │ │
│ │          │   │   tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01, -9.4922e-01,  5.7031e-01,                                                                                           │ │
│ │          │   │      1.2891e+00,  1.2891e+00,  1.0391e+00,  7.6953e-01,  5.0000e-01,                                                                                                   │ │
│ │          │   │      3.1055e-01,  1.8652e-01,  1.0986e-01,  6.2256e-02,  3.3936e-02,                                                                                                   │ │
│ │          │   │      1.6968e-02,  7.3853e-03,  2.0905e-03,  6.1798e-04,  4.2725e-04,                                                                                                   │ │
│ │          │   │      2.9373e-04,  2.0218e-04,  1.3924e-04,  9.6321e-05,  6.6280e-05,                                                                                                   │ │
│ │          │   │      4.5538e-05,  3.1471e-05,  2.1696e-05,  1.4961e-05,  1.0312e-05,                                                                                                   │ │
│ │          │   │      7.0930e-06,  4.8876e-06]]], dtype=torch.bfloat16)                                                                                                                 │ │
│ │          │   ),                                                                                                                                                                       │ │
│ │          │   'output_router_logits': False                                                                                                                                            │ │
│ │          }                                                                                                                                                                            │ │
│ │   self = GptOssAttention(                                                                                                                                                             │ │
│ │            (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                                           │ │
│ │            (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                            │ │
│ │            (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                                            │ │
│ │            (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                                           │ │
│ │          )                                                                                                                                                                            │ │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │
│                                                                                                                                                                                           │
│ G:\ossc\gpt_oss_ws\attention_patch.py:216 in patched_forward                                                                                                                              │
│                                                                                                                                                                                           │
│   213 │   │   attention_mask = bound.arguments.get("attention_mask")                                                                                                                      │
│   214 │   │   bound.arguments["attention_mask"] = _extend_mask(attention_mask, virtual_k)                                                                                                 │
│   215 │                                                                                                                                                                                   │
│ ❱ 216 │     outputs = original_forward(*bound.args, **bound.kwargs)                                                                                                                       │
│   217 │     model_output = HFModelOutput is not None and isinstance(outputs, HFModelOutput)                                                                                               │
│   218 │     if not model_output and not isinstance(outputs, tuple):                                                                                                                       │
│   219 │   │   return outputs                                                                                                                                                              │
│                                                                                                                                                                                           │
│ ╭─────────────────────────────────────────────────────────────────────────────────────── locals ────────────────────────────────────────────────────────────────────────────────────────╮ │
│ │ accepts_attention_mask = True                                                                                                                                                         │ │
│ │     accepts_past_value = False                                                                                                                                                        │ │
│ │    accepts_past_values = True                                                                                                                                                         │ │
│ │                   args = ()                                                                                                                                                           │ │
│ │         attention_mask = tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],                                                                                           │ │
│ │                          │      dtype=torch.bfloat16)                                                                                                                                 │ │
│ │             base_cache = DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,              │ │
│ │                          DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,        │ │
│ │                          DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                     │ │
│ │                          DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer])                                                                           │ │
│ │                  bound = <BoundArguments (hidden_states=tensor([[[ 0.2090, -0.0486,  0.2520,  ...,  0.0030, -0.0250,  0.3516]]],                                                      │ │
│ │                          │      dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>), position_embeddings=(tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, -1.2188, -0.3887,  0.3809,       │ │
│ │                          │   │      0.8516,  1.1016,  1.2500,  1.3125,  1.3359,  1.3438,  1.3438,                                                                                     │ │
│ │                          │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                                     │ │
│ │                          │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                                     │ │
│ │                          │   │      1.3438,  1.3438,  1.3438,  1.3438]]], dtype=torch.bfloat16), tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01, -9.4922e-01,  5.7031e-01,           │ │
│ │                          │   │      1.2891e+00,  1.2891e+00,  1.0391e+00,  7.6953e-01,  5.0000e-01,                                                                                   │ │
│ │                          │   │      3.1055e-01,  1.8652e-01,  1.0986e-01,  6.2256e-02,  3.3936e-02,                                                                                   │ │
│ │                          │   │      1.6968e-02,  7.3853e-03,  2.0905e-03,  6.1798e-04,  4.2725e-04,                                                                                   │ │
│ │                          │   │      2.9373e-04,  2.0218e-04,  1.3924e-04,  9.6321e-05,  6.6280e-05,                                                                                   │ │
│ │                          │   │      4.5538e-05,  3.1471e-05,  2.1696e-05,  1.4961e-05,  1.0312e-05,                                                                                   │ │
│ │                          │   │      7.0930e-06,  4.8876e-06]]], dtype=torch.bfloat16)), attention_mask=tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,     │ │
│ │                          0.]]]]), past_key_values=<gpt_oss_ws.attention_patch._VirtualCacheProxy object at 0x0000024437B95E80>, cache_position=tensor([12]), kwargs={'position_ids':  │ │
│ │                          tensor([[12]]), 'use_cache': True, 'output_router_logits': False})>                                                                                          │ │
│ │                 device = 'cpu'                                                                                                                                                        │ │
│ │         expected_dtype = torch.bfloat16                                                                                                                                               │ │
│ │                fetched = (                                                                                                                                                            │ │
│ │                          │   tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    ...,                                                                                                                                                │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]]]], dtype=torch.bfloat16,                                                                                           │ │
│ │                          │      grad_fn=<CatBackward0>),                                                                                                                              │ │
│ │                          │   tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    ...,                                                                                                                                                │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]]]], dtype=torch.bfloat16,                                                                                           │ │
│ │                          │      grad_fn=<CatBackward0>)                                                                                                                               │ │
│ │                          )                                                                                                                                                            │ │
│ │      hidden_states_arg = tensor([[[ 0.2090, -0.0486,  0.2520,  ...,  0.0030, -0.0250,  0.3516]]],                                                                                     │ │
│ │                          │      dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)                                                                                                      │ │
│ │               hs_dtype = torch.bfloat16                                                                                                                                               │ │
│ │                 kwargs = {                                                                                                                                                            │ │
│ │                          │   'hidden_states': tensor([[[ 0.2090, -0.0486,  0.2520,  ...,  0.0030, -0.0250,  0.3516]]],                                                                │ │
│ │                          │      dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>),                                                                                                     │ │
│ │                          │   'attention_mask': tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],                                                                     │ │
│ │                          │      dtype=torch.bfloat16),                                                                                                                                │ │
│ │                          │   'position_ids': tensor([[12]]),                                                                                                                          │ │
│ │                          │   'past_key_values': DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,     │ │
│ │                          DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                     │ │
│ │                          DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,        │ │
│ │                          DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]),                                                            │ │
│ │                          │   'use_cache': True,                                                                                                                                       │ │
│ │                          │   'cache_position': tensor([12]),                                                                                                                          │ │
│ │                          │   'position_embeddings': (                                                                                                                                 │ │
│ │                          │   │   tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, -1.2188, -0.3887,  0.3809,                                                                             │ │
│ │                          │   │      0.8516,  1.1016,  1.2500,  1.3125,  1.3359,  1.3438,  1.3438,                                                                                     │ │
│ │                          │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                                     │ │
│ │                          │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                                                                                     │ │
│ │                          │   │      1.3438,  1.3438,  1.3438,  1.3438]]], dtype=torch.bfloat16),                                                                                      │ │
│ │                          │   │   tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01, -9.4922e-01,  5.7031e-01,                                                                           │ │
│ │                          │   │      1.2891e+00,  1.2891e+00,  1.0391e+00,  7.6953e-01,  5.0000e-01,                                                                                   │ │
│ │                          │   │      3.1055e-01,  1.8652e-01,  1.0986e-01,  6.2256e-02,  3.3936e-02,                                                                                   │ │
│ │                          │   │      1.6968e-02,  7.3853e-03,  2.0905e-03,  6.1798e-04,  4.2725e-04,                                                                                   │ │
│ │                          │   │      2.9373e-04,  2.0218e-04,  1.3924e-04,  9.6321e-05,  6.6280e-05,                                                                                   │ │
│ │                          │   │      4.5538e-05,  3.1471e-05,  2.1696e-05,  1.4961e-05,  1.0312e-05,                                                                                   │ │
│ │                          │   │      7.0930e-06,  4.8876e-06]]], dtype=torch.bfloat16)                                                                                                 │ │
│ │                          │   ),                                                                                                                                                       │ │
│ │                          │   'output_router_logits': False                                                                                                                            │ │
│ │                          }                                                                                                                                                            │ │
│ │       original_forward = <bound method GptOssAttention.forward of GptOssAttention(                                                                                                    │ │
│ │                            (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                                                                                           │ │
│ │                            (k_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                            │ │
│ │                            (v_proj): Linear(in_features=2880, out_features=512, bias=True)                                                                                            │ │
│ │                            (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                                                                                           │ │
│ │                          )>                                                                                                                                                           │ │
│ │                   past = DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,              │ │
│ │                          DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer,        │ │
│ │                          DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer,                     │ │
│ │                          DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer])                                                                           │ │
│ │          past_arg_name = 'past_key_values'                                                                                                                                            │ │
│ │             past_dtype = None                                                                                                                                                         │ │
│ │                runtime = WorkspaceRuntimeState(                                                                                                                                       │ │
│ │                          │   toggles=HookToggles(kv_append=True, residual_delta=True, read_probes=True, broadcast=True),                                                              │ │
│ │                          │   layer_map={},                                                                                                                                            │ │
│ │                          │   kv_fetch=<bound method GPTOSSHookedModel._kv_fetch of <gpt_oss_ws.model_wrapper.GPTOSSHookedModel object at 0x0000024232B20590>>,                        │ │
│ │                          │   residual_delta=<bound method GPTOSSHookedModel._residual_delta of <gpt_oss_ws.model_wrapper.GPTOSSHookedModel object at 0x0000024232B20590>>,            │ │
│ │                          │   record_residual=<bound method GPTOSSHookedModel._record_residual of <gpt_oss_ws.model_wrapper.GPTOSSHookedModel object at 0x0000024232B20590>>,          │ │
│ │                          │   post_attention_hook=None,                                                                                                                                │ │
│ │                          │   device='cpu',                                                                                                                                            │ │
│ │                          │   slots=tensor([[[ 0.2381,  0.5548, -1.4226,  ...,  1.1380, -0.1747,  1.0149],                                                                             │ │
│ │                          │   │    [ 1.8067,  0.4476,  0.5067,  ...,  0.7321,  0.9700,  0.9564],                                                                                       │ │
│ │                          │   │    [ 0.1783,  0.2941, -0.5310,  ...,  0.0573, -0.5329, -0.1938],                                                                                       │ │
│ │                          │   │    [ 0.6669, -0.1690,  0.4875,  ..., -0.1974, -0.8195, -0.8909],                                                                                       │ │
│ │                          │   │    [ 0.4446,  0.5020, -0.4695,  ..., -0.6009,  0.3695, -0.1493],                                                                                       │ │
│ │                          │   │    [ 1.0253,  1.0542, -0.1753,  ...,  1.0288, -0.4843,  0.4651]]],                                                                                     │ │
│ │                          │      grad_fn=<AddBackward0>),                                                                                                                              │ │
│ │                          │   entropy=5.34375                                                                                                                                          │ │
│ │                          )                                                                                                                                                            │ │
│ │                   self = <gpt_oss_ws.attention_patch.AttentionPatcher object at 0x0000024437B95400>                                                                                   │ │
│ │              signature = <Signature (hidden_states: torch.Tensor, position_embeddings: tuple[torch.Tensor, torch.Tensor], attention_mask: Optional[torch.Tensor], past_key_values:    │ │
│ │                          Optional[transformers.cache_utils.Cache] = None, cache_position: Optional[torch.LongTensor] = None, **kwargs:                                                │ │
│ │                          Unpack[transformers.utils.generic.TransformersKwargs]) -> tuple[torch.Tensor, torch.Tensor]>                                                                 │ │
│ │             target_key = 'past_key_values'                                                                                                                                            │ │
│ │                toggles = HookToggles(kv_append=True, residual_delta=True, read_probes=True, broadcast=True)                                                                           │ │
│ │              virtual_k = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    ...,                                                                                                                                                │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]]]], dtype=torch.bfloat16,                                                                                           │ │
│ │                          │      grad_fn=<CatBackward0>)                                                                                                                               │ │
│ │              virtual_v = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    ...,                                                                                                                                                │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]],                                                                                                                   │ │
│ │                          │   │                                                                                                                                                        │ │
│ │                          │   │    [[0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.],                                                                                                                    │ │
│ │                          │   │     [0., 0., 0.,  ..., 0., 0., 0.]]]], dtype=torch.bfloat16,                                                                                           │ │
│ │                          │      grad_fn=<CatBackward0>)                                                                                                                               │ │
│ ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\transformers\utils\deprecation.py:172 in wrapped_func                                                             │
│                                                                                                                                                                                           │
│   169 │   │   │   │   # DeprecationWarning is ignored by default, so we use FutureWarning inst                                                                                            │
│   170 │   │   │   │   warnings.warn(message, FutureWarning, stacklevel=2)                                                                                                                 │
│   171 │   │   │                                                                                                                                                                           │
│ ❱ 172 │   │   │   return func(*args, **kwargs)                                                                                                                                            │
│   173 │   │                                                                                                                                                                               │
│   174 │   │   return wrapped_func                                                                                                                                                         │
│   175                                                                                                                                                                                     │
│                                                                                                                                                                                           │
│ ╭────────────────────────────────────────────────────────── locals ───────────────────────────────────────────────────────────╮                                                           │
│ │                additional_message = None                                                                                    │                                                           │
│ │                              args = (                                                                                       │                                                           │
│ │                                     │   GptOssAttention(                                                                    │                                                           │
│ │                                       (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                      │                                                           │
│ │                                       (k_proj): Linear(in_features=2880, out_features=512, bias=True)                       │                                                           │
│ │                                       (v_proj): Linear(in_features=2880, out_features=512, bias=True)                       │                                                           │
│ │                                       (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                      │                                                           │
│ │                                     ),                                                                                      │                                                           │
│ │                                     │   tensor([[[ 0.2090, -0.0486,  0.2520,  ...,  0.0030, -0.0250,  0.3516]]],            │                                                           │
│ │                                     │      dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>),                                │                                                           │
│ │                                     │   (                                                                                   │                                                           │
│ │                                     │   │   tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, -1.2188, -0.3887,  0.3809,        │                                                           │
│ │                                     │   │      0.8516,  1.1016,  1.2500,  1.3125,  1.3359,  1.3438,  1.3438,                │                                                           │
│ │                                     │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                │                                                           │
│ │                                     │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,                │                                                           │
│ │                                     │   │      1.3438,  1.3438,  1.3438,  1.3438]]], dtype=torch.bfloat16),                 │                                                           │
│ │                                     │   │   tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01, -9.4922e-01,  5.7031e-01,      │                                                           │
│ │                                     │   │      1.2891e+00,  1.2891e+00,  1.0391e+00,  7.6953e-01,  5.0000e-01,              │                                                           │
│ │                                     │   │      3.1055e-01,  1.8652e-01,  1.0986e-01,  6.2256e-02,  3.3936e-02,              │                                                           │
│ │                                     │   │      1.6968e-02,  7.3853e-03,  2.0905e-03,  6.1798e-04,  4.2725e-04,              │                                                           │
│ │                                     │   │      2.9373e-04,  2.0218e-04,  1.3924e-04,  9.6321e-05,  6.6280e-05,              │                                                           │
│ │                                     │   │      4.5538e-05,  3.1471e-05,  2.1696e-05,  1.4961e-05,  1.0312e-05,              │                                                           │
│ │                                     │   │      7.0930e-06,  4.8876e-06]]], dtype=torch.bfloat16)                            │                                                           │
│ │                                     │   ),                                                                                  │                                                           │
│ │                                     │   tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]]), │                                                           │
│ │                                     │   <gpt_oss_ws.attention_patch._VirtualCacheProxy object at 0x0000024437B95E80>,       │                                                           │
│ │                                     │   tensor([12])                                                                        │                                                           │
│ │                                     )                                                                                       │                                                           │
│ │                         func_name = 'GptOssAttention.forward'                                                               │                                                           │
│ │                   is_class_method = False                                                                                   │                                                           │
│ │       is_greater_or_equal_version = False                                                                                   │                                                           │
│ │                is_instance_method = True                                                                                    │                                                           │
│ │                            kwargs = {'position_ids': tensor([[12]]), 'use_cache': True, 'output_router_logits': False}      │                                                           │
│ │                           message = None                                                                                    │                                                           │
│ │                    minimum_action = <Action.NONE: 'none'>                                                                   │                                                           │
│ │                          new_name = 'past_key_values'                                                                       │                                                           │
│ │                          old_name = 'past_key_value'                                                                        │                                                           │
│ │               raise_if_both_names = False                                                                                   │                                                           │
│ │ raise_if_greater_or_equal_version = False                                                                                   │                                                           │
│ │                   version_message = 'and will be removed in version 4.58'                                                   │                                                           │
│ │  warn_if_greater_or_equal_version = False                                                                                   │                                                           │
│ ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯                                                           │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\transformers\models\gpt_oss\modeling_gpt_oss.py:323 in forward                                                    │
│                                                                                                                                                                                           │
│   320 │   │   if self.config._attn_implementation != "eager":                                                                                                                             │
│   321 │   │   │   attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementati                                                                                            │
│   322 │   │                                                                                                                                                                               │
│ ❱ 323 │   │   attn_output, attn_weights = attention_interface(                                                                                                                            │
│   324 │   │   │   self,                                                                                                                                                                   │
│   325 │   │   │   query_states,                                                                                                                                                           │
│   326 │   │   │   key_states,                                                                                                                                                             │
│                                                                                                                                                                                           │
│ ╭───────────────────────────────────────────────── locals ─────────────────────────────────────────────────╮                                                                              │
│ │      attention_mask = tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]]) │                                                                              │
│ │        cache_kwargs = {'cache_position': tensor([12])}                                                   │                                                                              │
│ │      cache_position = tensor([12])                                                                       │                                                                              │
│ │                 cos = tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, -1.2188, -0.3887,  0.3809,           │                                                                              │
│ │                       │   │      0.8516,  1.1016,  1.2500,  1.3125,  1.3359,  1.3438,  1.3438,           │                                                                              │
│ │                       │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,           │                                                                              │
│ │                       │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,           │                                                                              │
│ │                       │   │      1.3438,  1.3438,  1.3438,  1.3438]]], dtype=torch.bfloat16)             │                                                                              │
│ │        hidden_shape = (1, 1, -1, 64)                                                                     │                                                                              │
│ │       hidden_states = tensor([[[ 0.2090, -0.0486,  0.2520,  ...,  0.0030, -0.0250,  0.3516]]],           │                                                                              │
│ │                       │      dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)                            │                                                                              │
│ │         input_shape = torch.Size([1, 1])                                                                 │                                                                              │
│ │          key_states = tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     ...,                                                                     │                                                                              │
│ │                       │   │     [-2.1250e+00,  1.0391e+00, -2.9062e+00,  ..., -7.4158e-03,               │                                                                              │
│ │                       │   │      -1.6797e+00, -1.0469e+00],                                              │                                                                              │
│ │                       │   │     [-8.2812e-01,  4.8828e-02, -5.9766e-01,  ..., -1.2812e+00,               │                                                                              │
│ │                       │   │   │   1.0859e+00,  2.2344e+00],                                              │                                                                              │
│ │                       │   │     [-1.5078e+00,  1.8359e+00, -1.7344e+00,  ..., -3.4688e+00,               │                                                                              │
│ │                       │   │   │   1.0391e+00, -5.7031e-01]],                                             │                                                                              │
│ │                       │   │                                                                              │                                                                              │
│ │                       │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     ...,                                                                     │                                                                              │
│ │                       │   │     [ 1.2500e+01,  2.7188e+00,  4.7070e-01,  ..., -3.4375e+00,               │                                                                              │
│ │                       │   │      -7.0312e-01, -1.9297e+00],                                              │                                                                              │
│ │                       │   │     [-3.9648e-01, -2.2812e+00, -2.9102e-01,  ..., -1.6875e+00,               │                                                                              │
│ │                       │   │      -1.0078e+00,  6.2891e-01],                                              │                                                                              │
│ │                       │   │     [-1.1438e+01, -4.7500e+00, -1.8984e+00,  ..., -1.1328e+00,               │                                                                              │
│ │                       │   │      -9.8047e-01,  1.5000e+00]],                                             │                                                                              │
│ │                       │   │                                                                              │                                                                              │
│ │                       │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     ...,                                                                     │                                                                              │
│ │                       │   │     [-2.6367e-01,  2.1680e-01, -1.1328e+00,  ..., -8.3594e-01,               │                                                                              │
│ │                       │   │      -2.5469e+00,  8.5156e-01],                                              │                                                                              │
│ │                       │   │     [-1.0132e-02,  6.3965e-02, -1.8799e-02,  ...,  2.5938e+00,               │                                                                              │
│ │                       │   │   │   2.1875e+00, -1.6357e-02],                                              │                                                                              │
│ │                       │   │     [-7.4609e-01, -1.0156e+00, -6.3281e-01,  ..., -1.5234e+00,               │                                                                              │
│ │                       │   │      -3.2812e-01, -2.2344e+00]],                                             │                                                                              │
│ │                       │   │                                                                              │                                                                              │
│ │                       │   │    ...,                                                                      │                                                                              │
│ │                       │   │                                                                              │                                                                              │
│ │                       │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     ...,                                                                     │                                                                              │
│ │                       │   │     [-1.3672e+00, -1.7969e+00, -2.2344e+00,  ..., -8.1250e-01,               │                                                                              │
│ │                       │   │      -1.7734e+00, -5.6641e-01],                                              │                                                                              │
│ │                       │   │     [-2.0996e-01,  1.1816e-01, -4.7852e-02,  ...,  6.2109e-01,               │                                                                              │
│ │                       │   │      -1.5625e+00, -2.7344e+00],                                              │                                                                              │
│ │                       │   │     [-1.9453e+00,  9.2578e-01, -2.6562e+00,  ...,  1.0469e+00,               │                                                                              │
│ │                       │   │      -7.5781e-01,  1.2451e-01]],                                             │                                                                              │
│ │                       │   │                                                                              │                                                                              │
│ │                       │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     ...,                                                                     │                                                                              │
│ │                       │   │     [ 3.9062e-03, -1.2812e+00, -6.9141e-01,  ...,  3.9844e+00,               │                                                                              │
│ │                       │   │      -4.7812e+00,  2.6875e+00],                                              │                                                                              │
│ │                       │   │     [ 5.6641e-01, -9.4238e-02, -1.4258e-01,  ..., -1.0000e+00,               │                                                                              │
│ │                       │   │   │   5.0000e-01,  1.4609e+00],                                              │                                                                              │
│ │                       │   │     [ 1.2188e+00,  9.7656e-02,  5.1172e-01,  ...,  1.6875e+00,               │                                                                              │
│ │                       │   │      -2.2812e+00, -3.7656e+00]],                                             │                                                                              │
│ │                       │   │                                                                              │                                                                              │
│ │                       │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     ...,                                                                     │                                                                              │
│ │                       │   │     [-1.7734e+00,  2.3438e-01,  2.5000e+00,  ..., -1.2891e+00,               │                                                                              │
│ │                       │   │      -1.6504e-01,  1.2109e+00],                                              │                                                                              │
│ │                       │   │     [ 1.2344e+00, -3.6133e-01,  7.0312e-01,  ...,  6.4844e-01,               │                                                                              │
│ │                       │   │      -1.1523e-01, -1.6094e+00],                                              │                                                                              │
│ │                       │   │     [ 3.7188e+00, -2.7812e+00,  8.9453e-01,  ..., -3.1094e+00,               │                                                                              │
│ │                       │   │   │   1.4766e+00, -1.3047e+00]]]], grad_fn=<CatBackward0>)                   │                                                                              │
│ │              kwargs = {'position_ids': tensor([[12]]), 'use_cache': True, 'output_router_logits': False} │                                                                              │
│ │     past_key_values = <gpt_oss_ws.attention_patch._VirtualCacheProxy object at 0x0000024437B95E80>       │                                                                              │
│ │ position_embeddings = (                                                                                  │                                                                              │
│ │                       │   tensor([[[ 1.1328, -0.5430,  1.1250, -0.9531, -1.2188, -0.3887,  0.3809,       │                                                                              │
│ │                       │   │      0.8516,  1.1016,  1.2500,  1.3125,  1.3359,  1.3438,  1.3438,           │                                                                              │
│ │                       │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,           │                                                                              │
│ │                       │   │      1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,  1.3438,           │                                                                              │
│ │                       │   │      1.3438,  1.3438,  1.3438,  1.3438]]], dtype=torch.bfloat16),            │                                                                              │
│ │                       │   tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01, -9.4922e-01,  5.7031e-01,     │                                                                              │
│ │                       │   │      1.2891e+00,  1.2891e+00,  1.0391e+00,  7.6953e-01,  5.0000e-01,         │                                                                              │
│ │                       │   │      3.1055e-01,  1.8652e-01,  1.0986e-01,  6.2256e-02,  3.3936e-02,         │                                                                              │
│ │                       │   │      1.6968e-02,  7.3853e-03,  2.0905e-03,  6.1798e-04,  4.2725e-04,         │                                                                              │
│ │                       │   │      2.9373e-04,  2.0218e-04,  1.3924e-04,  9.6321e-05,  6.6280e-05,         │                                                                              │
│ │                       │   │      4.5538e-05,  3.1471e-05,  2.1696e-05,  1.4961e-05,  1.0312e-05,         │                                                                              │
│ │                       │   │      7.0930e-06,  4.8876e-06]]], dtype=torch.bfloat16)                       │                                                                              │
│ │                       )                                                                                  │                                                                              │
│ │        query_states = tensor([[[[-0.1582,  0.0083, -0.0530,  ...,  1.8438,  1.2422, -0.2578]],           │                                                                              │
│ │                       │   │                                                                              │                                                                              │
│ │                       │   │    [[ 0.0305,  0.1709,  0.0098,  ...,  0.6445,  0.4961,  0.3887]],           │                                                                              │
│ │                       │   │                                                                              │                                                                              │
│ │                       │   │    [[-0.0200,  0.0503, -0.0415,  ..., -2.0469,  0.3652,  0.6523]],           │                                                                              │
│ │                       │   │                                                                              │                                                                              │
│ │                       │   │    ...,                                                                      │                                                                              │
│ │                       │   │                                                                              │                                                                              │
│ │                       │   │    [[-0.1885,  0.1240,  0.0444,  ...,  4.8438,  2.8906,  0.2451]],           │                                                                              │
│ │                       │   │                                                                              │                                                                              │
│ │                       │   │    [[ 0.1553, -0.0684,  0.1211,  ...,  0.3320, -0.0884,  0.6523]],           │                                                                              │
│ │                       │   │                                                                              │                                                                              │
│ │                       │   │    [[-0.1025, -0.8242,  0.0591,  ...,  2.2812, -0.1338,  1.1484]]]],         │                                                                              │
│ │                       │      dtype=torch.bfloat16, grad_fn=<CatBackward0>)                               │                                                                              │
│ │                self = GptOssAttention(                                                                   │                                                                              │
│ │                         (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                 │                                                                              │
│ │                         (k_proj): Linear(in_features=2880, out_features=512, bias=True)                  │                                                                              │
│ │                         (v_proj): Linear(in_features=2880, out_features=512, bias=True)                  │                                                                              │
│ │                         (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                 │                                                                              │
│ │                       )                                                                                  │                                                                              │
│ │                 sin = tensor([[[-7.2266e-01,  1.2344e+00, -7.4609e-01, -9.4922e-01,  5.7031e-01,         │                                                                              │
│ │                       │   │      1.2891e+00,  1.2891e+00,  1.0391e+00,  7.6953e-01,  5.0000e-01,         │                                                                              │
│ │                       │   │      3.1055e-01,  1.8652e-01,  1.0986e-01,  6.2256e-02,  3.3936e-02,         │                                                                              │
│ │                       │   │      1.6968e-02,  7.3853e-03,  2.0905e-03,  6.1798e-04,  4.2725e-04,         │                                                                              │
│ │                       │   │      2.9373e-04,  2.0218e-04,  1.3924e-04,  9.6321e-05,  6.6280e-05,         │                                                                              │
│ │                       │   │      4.5538e-05,  3.1471e-05,  2.1696e-05,  1.4961e-05,  1.0312e-05,         │                                                                              │
│ │                       │   │      7.0930e-06,  4.8876e-06]]], dtype=torch.bfloat16)                       │                                                                              │
│ │        value_states = tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     ...,                                                                     │                                                                              │
│ │                       │   │     [ 9.2578e-01, -5.0391e-01, -1.3203e+00,  ...,  2.1562e+00,               │                                                                              │
│ │                       │   │      -1.1953e+00, -1.7656e+00],                                              │                                                                              │
│ │                       │   │     [-1.4844e+00, -7.2656e-01,  7.1484e-01,  ...,  7.1289e-02,               │                                                                              │
│ │                       │   │      -6.4062e-01, -1.1169e-02],                                              │                                                                              │
│ │                       │   │     [-1.0156e+00, -3.7305e-01, -4.9414e-01,  ...,  6.9531e-01,               │                                                                              │
│ │                       │   │   │   8.7500e-01, -1.4355e-01]],                                             │                                                                              │
│ │                       │   │                                                                              │                                                                              │
│ │                       │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     ...,                                                                     │                                                                              │
│ │                       │   │     [ 3.3447e-02,  1.7500e+00, -5.7031e-01,  ..., -3.1250e-01,               │                                                                              │
│ │                       │   │      -3.3008e-01,  6.6797e-01],                                              │                                                                              │
│ │                       │   │     [-1.0938e+00,  4.9023e-01, -6.2500e-01,  ..., -1.2256e-01,               │                                                                              │
│ │                       │   │      -9.7656e-01,  2.0312e-01],                                              │                                                                              │
│ │                       │   │     [ 1.6094e+00, -1.4648e-01,  8.0078e-01,  ...,  3.6523e-01,               │                                                                              │
│ │                       │   │      -3.0312e+00,  5.5859e-01]],                                             │                                                                              │
│ │                       │   │                                                                              │                                                                              │
│ │                       │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     ...,                                                                     │                                                                              │
│ │                       │   │     [ 7.2754e-02, -3.5938e-01,  9.8438e-01,  ..., -2.0312e+00,               │                                                                              │
│ │                       │   │      -1.2578e+00, -8.6719e-01],                                              │                                                                              │
│ │                       │   │     [ 1.2891e+00, -2.5781e-01, -1.7480e-01,  ..., -6.1719e-01,               │                                                                              │
│ │                       │   │   │   4.1797e-01, -3.2617e-01],                                              │                                                                              │
│ │                       │   │     [ 7.7344e-01,  8.7500e-01,  5.6641e-01,  ..., -1.8359e+00,               │                                                                              │
│ │                       │   │      -6.2500e-01,  2.7148e-01]],                                             │                                                                              │
│ │                       │   │                                                                              │                                                                              │
│ │                       │   │    ...,                                                                      │                                                                              │
│ │                       │   │                                                                              │                                                                              │
│ │                       │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     ...,                                                                     │                                                                              │
│ │                       │   │     [ 5.6250e-01,  1.6406e-01,  2.6855e-03,  ...,  4.6484e-01,               │                                                                              │
│ │                       │   │   │   8.2031e-01, -1.0938e+00],                                              │                                                                              │
│ │                       │   │     [ 1.0156e-01, -2.2852e-01,  8.9844e-01,  ..., -2.8711e-01,               │                                                                              │
│ │                       │   │   │   1.3770e-01, -8.1250e-01],                                              │                                                                              │
│ │                       │   │     [-5.5469e-01,  3.6328e-01, -3.4766e-01,  ...,  8.8281e-01,               │                                                                              │
│ │                       │   │   │   6.7188e-01, -7.3047e-01]],                                             │                                                                              │
│ │                       │   │                                                                              │                                                                              │
│ │                       │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     ...,                                                                     │                                                                              │
│ │                       │   │     [ 2.7969e+00,  1.0693e-01, -1.3672e+00,  ..., -2.8125e+00,               │                                                                              │
│ │                       │   │   │   1.5747e-02,  1.7422e+00],                                              │                                                                              │
│ │                       │   │     [-4.6484e-01,  1.5137e-01, -4.9609e-01,  ...,  2.8906e-01,               │                                                                              │
│ │                       │   │   │   3.3789e-01, -6.8750e-01],                                              │                                                                              │
│ │                       │   │     [ 3.1250e+00,  3.2227e-02, -1.8984e+00,  ..., -8.0859e-01,               │                                                                              │
│ │                       │   │      -3.0469e-01,  5.0000e-01]],                                             │                                                                              │
│ │                       │   │                                                                              │                                                                              │
│ │                       │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                              │
│ │                       │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                              │
│ │                       │   │     ...,                                                                     │                                                                              │
│ │                       │   │     [ 1.1094e+00, -1.9409e-02, -5.1953e-01,  ...,  1.6211e-01,               │                                                                              │
│ │                       │   │   │   1.2109e+00,  3.8281e-01],                                              │                                                                              │
│ │                       │   │     [-1.2500e+00, -4.7852e-01,  1.3477e-01,  ...,  1.3379e-01,               │                                                                              │
│ │                       │   │      -7.1094e-01,  2.1582e-01],                                              │                                                                              │
│ │                       │   │     [ 8.8672e-01,  3.0859e-01,  8.9844e-01,  ...,  4.4141e-01,               │                                                                              │
│ │                       │   │      -4.0625e-01, -3.3008e-01]]]], grad_fn=<CatBackward0>)                   │                                                                              │
│ ╰──────────────────────────────────────────────────────────────────────────────────────────────────────────╯                                                                              │
│                                                                                                                                                                                           │
│ C:\Users\user\AppData\Local\Programs\Python\Python313\Lib\site-packages\transformers\models\gpt_oss\modeling_gpt_oss.py:248 in eager_attention_forward                                    │
│                                                                                                                                                                                           │
│   245 ):                                                                                                                                                                                  │
│   246 │   key_states = repeat_kv(key, module.num_key_value_groups)                                                                                                                        │
│   247 │   value_states = repeat_kv(value, module.num_key_value_groups)                                                                                                                    │
│ ❱ 248 │   attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling                                                                                                        │
│   249 │   if attention_mask is not None:                                                                                                                                                  │
│   250 │   │   causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]                                                                                                               │
│   251 │   │   attn_weights = attn_weights + causal_mask                                                                                                                                   │
│                                                                                                                                                                                           │
│ ╭────────────────────────────────────────────── locals ───────────────────────────────────────────────╮                                                                                   │
│ │ attention_mask = tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]]) │                                                                                   │
│ │        dropout = 0.0                                                                                │                                                                                   │
│ │            key = tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [-2.1250e+00,  1.0391e+00, -2.9062e+00,  ..., -7.4158e-03,               │                                                                                   │
│ │                  │   │      -1.6797e+00, -1.0469e+00],                                              │                                                                                   │
│ │                  │   │     [-8.2812e-01,  4.8828e-02, -5.9766e-01,  ..., -1.2812e+00,               │                                                                                   │
│ │                  │   │   │   1.0859e+00,  2.2344e+00],                                              │                                                                                   │
│ │                  │   │     [-1.5078e+00,  1.8359e+00, -1.7344e+00,  ..., -3.4688e+00,               │                                                                                   │
│ │                  │   │   │   1.0391e+00, -5.7031e-01]],                                             │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [ 1.2500e+01,  2.7188e+00,  4.7070e-01,  ..., -3.4375e+00,               │                                                                                   │
│ │                  │   │      -7.0312e-01, -1.9297e+00],                                              │                                                                                   │
│ │                  │   │     [-3.9648e-01, -2.2812e+00, -2.9102e-01,  ..., -1.6875e+00,               │                                                                                   │
│ │                  │   │      -1.0078e+00,  6.2891e-01],                                              │                                                                                   │
│ │                  │   │     [-1.1438e+01, -4.7500e+00, -1.8984e+00,  ..., -1.1328e+00,               │                                                                                   │
│ │                  │   │      -9.8047e-01,  1.5000e+00]],                                             │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [-2.6367e-01,  2.1680e-01, -1.1328e+00,  ..., -8.3594e-01,               │                                                                                   │
│ │                  │   │      -2.5469e+00,  8.5156e-01],                                              │                                                                                   │
│ │                  │   │     [-1.0132e-02,  6.3965e-02, -1.8799e-02,  ...,  2.5938e+00,               │                                                                                   │
│ │                  │   │   │   2.1875e+00, -1.6357e-02],                                              │                                                                                   │
│ │                  │   │     [-7.4609e-01, -1.0156e+00, -6.3281e-01,  ..., -1.5234e+00,               │                                                                                   │
│ │                  │   │      -3.2812e-01, -2.2344e+00]],                                             │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    ...,                                                                      │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [-1.3672e+00, -1.7969e+00, -2.2344e+00,  ..., -8.1250e-01,               │                                                                                   │
│ │                  │   │      -1.7734e+00, -5.6641e-01],                                              │                                                                                   │
│ │                  │   │     [-2.0996e-01,  1.1816e-01, -4.7852e-02,  ...,  6.2109e-01,               │                                                                                   │
│ │                  │   │      -1.5625e+00, -2.7344e+00],                                              │                                                                                   │
│ │                  │   │     [-1.9453e+00,  9.2578e-01, -2.6562e+00,  ...,  1.0469e+00,               │                                                                                   │
│ │                  │   │      -7.5781e-01,  1.2451e-01]],                                             │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [ 3.9062e-03, -1.2812e+00, -6.9141e-01,  ...,  3.9844e+00,               │                                                                                   │
│ │                  │   │      -4.7812e+00,  2.6875e+00],                                              │                                                                                   │
│ │                  │   │     [ 5.6641e-01, -9.4238e-02, -1.4258e-01,  ..., -1.0000e+00,               │                                                                                   │
│ │                  │   │   │   5.0000e-01,  1.4609e+00],                                              │                                                                                   │
│ │                  │   │     [ 1.2188e+00,  9.7656e-02,  5.1172e-01,  ...,  1.6875e+00,               │                                                                                   │
│ │                  │   │      -2.2812e+00, -3.7656e+00]],                                             │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [-1.7734e+00,  2.3438e-01,  2.5000e+00,  ..., -1.2891e+00,               │                                                                                   │
│ │                  │   │      -1.6504e-01,  1.2109e+00],                                              │                                                                                   │
│ │                  │   │     [ 1.2344e+00, -3.6133e-01,  7.0312e-01,  ...,  6.4844e-01,               │                                                                                   │
│ │                  │   │      -1.1523e-01, -1.6094e+00],                                              │                                                                                   │
│ │                  │   │     [ 3.7188e+00, -2.7812e+00,  8.9453e-01,  ..., -3.1094e+00,               │                                                                                   │
│ │                  │   │   │   1.4766e+00, -1.3047e+00]]]], grad_fn=<CatBackward0>)                   │                                                                                   │
│ │     key_states = tensor([[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [-2.1250,  1.0391, -2.9062,  ..., -0.0074, -1.6797, -1.0469],            │                                                                                   │
│ │                  │   │     [-0.8281,  0.0488, -0.5977,  ..., -1.2812,  1.0859,  2.2344],            │                                                                                   │
│ │                  │   │     [-1.5078,  1.8359, -1.7344,  ..., -3.4688,  1.0391, -0.5703]],           │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [-2.1250,  1.0391, -2.9062,  ..., -0.0074, -1.6797, -1.0469],            │                                                                                   │
│ │                  │   │     [-0.8281,  0.0488, -0.5977,  ..., -1.2812,  1.0859,  2.2344],            │                                                                                   │
│ │                  │   │     [-1.5078,  1.8359, -1.7344,  ..., -3.4688,  1.0391, -0.5703]],           │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [-2.1250,  1.0391, -2.9062,  ..., -0.0074, -1.6797, -1.0469],            │                                                                                   │
│ │                  │   │     [-0.8281,  0.0488, -0.5977,  ..., -1.2812,  1.0859,  2.2344],            │                                                                                   │
│ │                  │   │     [-1.5078,  1.8359, -1.7344,  ..., -3.4688,  1.0391, -0.5703]],           │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    ...,                                                                      │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [-1.7734,  0.2344,  2.5000,  ..., -1.2891, -0.1650,  1.2109],            │                                                                                   │
│ │                  │   │     [ 1.2344, -0.3613,  0.7031,  ...,  0.6484, -0.1152, -1.6094],            │                                                                                   │
│ │                  │   │     [ 3.7188, -2.7812,  0.8945,  ..., -3.1094,  1.4766, -1.3047]],           │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [-1.7734,  0.2344,  2.5000,  ..., -1.2891, -0.1650,  1.2109],            │                                                                                   │
│ │                  │   │     [ 1.2344, -0.3613,  0.7031,  ...,  0.6484, -0.1152, -1.6094],            │                                                                                   │
│ │                  │   │     [ 3.7188, -2.7812,  0.8945,  ..., -3.1094,  1.4766, -1.3047]],           │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [-1.7734,  0.2344,  2.5000,  ..., -1.2891, -0.1650,  1.2109],            │                                                                                   │
│ │                  │   │     [ 1.2344, -0.3613,  0.7031,  ...,  0.6484, -0.1152, -1.6094],            │                                                                                   │
│ │                  │   │     [ 3.7188, -2.7812,  0.8945,  ..., -3.1094,  1.4766, -1.3047]]]],         │                                                                                   │
│ │                  │      grad_fn=<UnsafeViewBackward0>)                                              │                                                                                   │
│ │         kwargs = {                                                                                  │                                                                                   │
│ │                  │   'sliding_window': None,                                                        │                                                                                   │
│ │                  │   's_aux': Parameter containing:                                                 │                                                                                   │
│ │                  tensor([ 3.9844,  3.2656,  2.3594,  2.2812,  3.0469,  4.3438,  2.1250,  2.6094,    │                                                                                   │
│ │                  │   │    0.9922,  2.0625,  3.3594,  1.8438,  3.4375,  2.1250,  3.0000,  0.5156,    │                                                                                   │
│ │                  │   │    2.5156, -0.3379, -0.4805,  2.6250,  7.8750,  7.5625,  3.7344,  4.6250,    │                                                                                   │
│ │                  │   │    2.6406,  2.7969,  1.5781,  0.9727,  1.2344,  1.7812,  1.5703,  1.6328,    │                                                                                   │
│ │                  │   │    4.6562,  4.4375,  3.6250,  4.0625,  3.0625,  3.7188,  4.4062,  5.6875,    │                                                                                   │
│ │                  │   │    5.0938,  3.9375,  4.3438,  4.1562,  4.4688,  3.8906,  3.2812,  2.3594,    │                                                                                   │
│ │                  │   │    6.7500,  1.8750,  6.5000,  5.8125,  5.0312,  6.8750,  6.9062,  6.7812,    │                                                                                   │
│ │                  │   │    1.3203,  2.2031,  5.5312,  2.2500,  1.3750,  5.0000,  2.0312,  1.7656],   │                                                                                   │
│ │                  │      dtype=torch.bfloat16, requires_grad=True),                                  │                                                                                   │
│ │                  │   'position_ids': tensor([[12]]),                                                │                                                                                   │
│ │                  │   'use_cache': True,                                                             │                                                                                   │
│ │                  │   'output_router_logits': False                                                  │                                                                                   │
│ │                  }                                                                                  │                                                                                   │
│ │         module = GptOssAttention(                                                                   │                                                                                   │
│ │                    (q_proj): Linear(in_features=2880, out_features=4096, bias=True)                 │                                                                                   │
│ │                    (k_proj): Linear(in_features=2880, out_features=512, bias=True)                  │                                                                                   │
│ │                    (v_proj): Linear(in_features=2880, out_features=512, bias=True)                  │                                                                                   │
│ │                    (o_proj): Linear(in_features=4096, out_features=2880, bias=True)                 │                                                                                   │
│ │                  )                                                                                  │                                                                                   │
│ │          query = tensor([[[[-0.1582,  0.0083, -0.0530,  ...,  1.8438,  1.2422, -0.2578]],           │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0305,  0.1709,  0.0098,  ...,  0.6445,  0.4961,  0.3887]],           │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[-0.0200,  0.0503, -0.0415,  ..., -2.0469,  0.3652,  0.6523]],           │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    ...,                                                                      │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[-0.1885,  0.1240,  0.0444,  ...,  4.8438,  2.8906,  0.2451]],           │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.1553, -0.0684,  0.1211,  ...,  0.3320, -0.0884,  0.6523]],           │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[-0.1025, -0.8242,  0.0591,  ...,  2.2812, -0.1338,  1.1484]]]],         │                                                                                   │
│ │                  │      dtype=torch.bfloat16, grad_fn=<CatBackward0>)                               │                                                                                   │
│ │        scaling = 0.125                                                                              │                                                                                   │
│ │          value = tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [ 9.2578e-01, -5.0391e-01, -1.3203e+00,  ...,  2.1562e+00,               │                                                                                   │
│ │                  │   │      -1.1953e+00, -1.7656e+00],                                              │                                                                                   │
│ │                  │   │     [-1.4844e+00, -7.2656e-01,  7.1484e-01,  ...,  7.1289e-02,               │                                                                                   │
│ │                  │   │      -6.4062e-01, -1.1169e-02],                                              │                                                                                   │
│ │                  │   │     [-1.0156e+00, -3.7305e-01, -4.9414e-01,  ...,  6.9531e-01,               │                                                                                   │
│ │                  │   │   │   8.7500e-01, -1.4355e-01]],                                             │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [ 3.3447e-02,  1.7500e+00, -5.7031e-01,  ..., -3.1250e-01,               │                                                                                   │
│ │                  │   │      -3.3008e-01,  6.6797e-01],                                              │                                                                                   │
│ │                  │   │     [-1.0938e+00,  4.9023e-01, -6.2500e-01,  ..., -1.2256e-01,               │                                                                                   │
│ │                  │   │      -9.7656e-01,  2.0312e-01],                                              │                                                                                   │
│ │                  │   │     [ 1.6094e+00, -1.4648e-01,  8.0078e-01,  ...,  3.6523e-01,               │                                                                                   │
│ │                  │   │      -3.0312e+00,  5.5859e-01]],                                             │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [ 7.2754e-02, -3.5938e-01,  9.8438e-01,  ..., -2.0312e+00,               │                                                                                   │
│ │                  │   │      -1.2578e+00, -8.6719e-01],                                              │                                                                                   │
│ │                  │   │     [ 1.2891e+00, -2.5781e-01, -1.7480e-01,  ..., -6.1719e-01,               │                                                                                   │
│ │                  │   │   │   4.1797e-01, -3.2617e-01],                                              │                                                                                   │
│ │                  │   │     [ 7.7344e-01,  8.7500e-01,  5.6641e-01,  ..., -1.8359e+00,               │                                                                                   │
│ │                  │   │      -6.2500e-01,  2.7148e-01]],                                             │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    ...,                                                                      │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [ 5.6250e-01,  1.6406e-01,  2.6855e-03,  ...,  4.6484e-01,               │                                                                                   │
│ │                  │   │   │   8.2031e-01, -1.0938e+00],                                              │                                                                                   │
│ │                  │   │     [ 1.0156e-01, -2.2852e-01,  8.9844e-01,  ..., -2.8711e-01,               │                                                                                   │
│ │                  │   │   │   1.3770e-01, -8.1250e-01],                                              │                                                                                   │
│ │                  │   │     [-5.5469e-01,  3.6328e-01, -3.4766e-01,  ...,  8.8281e-01,               │                                                                                   │
│ │                  │   │   │   6.7188e-01, -7.3047e-01]],                                             │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [ 2.7969e+00,  1.0693e-01, -1.3672e+00,  ..., -2.8125e+00,               │                                                                                   │
│ │                  │   │   │   1.5747e-02,  1.7422e+00],                                              │                                                                                   │
│ │                  │   │     [-4.6484e-01,  1.5137e-01, -4.9609e-01,  ...,  2.8906e-01,               │                                                                                   │
│ │                  │   │   │   3.3789e-01, -6.8750e-01],                                              │                                                                                   │
│ │                  │   │     [ 3.1250e+00,  3.2227e-02, -1.8984e+00,  ..., -8.0859e-01,               │                                                                                   │
│ │                  │   │      -3.0469e-01,  5.0000e-01]],                                             │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,               │                                                                                   │
│ │                  │   │   │   0.0000e+00,  0.0000e+00],                                              │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [ 1.1094e+00, -1.9409e-02, -5.1953e-01,  ...,  1.6211e-01,               │                                                                                   │
│ │                  │   │   │   1.2109e+00,  3.8281e-01],                                              │                                                                                   │
│ │                  │   │     [-1.2500e+00, -4.7852e-01,  1.3477e-01,  ...,  1.3379e-01,               │                                                                                   │
│ │                  │   │      -7.1094e-01,  2.1582e-01],                                              │                                                                                   │
│ │                  │   │     [ 8.8672e-01,  3.0859e-01,  8.9844e-01,  ...,  4.4141e-01,               │                                                                                   │
│ │                  │   │      -4.0625e-01, -3.3008e-01]]]], grad_fn=<CatBackward0>)                   │                                                                                   │
│ │   value_states = tensor([[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [ 0.9258, -0.5039, -1.3203,  ...,  2.1562, -1.1953, -1.7656],            │                                                                                   │
│ │                  │   │     [-1.4844, -0.7266,  0.7148,  ...,  0.0713, -0.6406, -0.0112],            │                                                                                   │
│ │                  │   │     [-1.0156, -0.3730, -0.4941,  ...,  0.6953,  0.8750, -0.1436]],           │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [ 0.9258, -0.5039, -1.3203,  ...,  2.1562, -1.1953, -1.7656],            │                                                                                   │
│ │                  │   │     [-1.4844, -0.7266,  0.7148,  ...,  0.0713, -0.6406, -0.0112],            │                                                                                   │
│ │                  │   │     [-1.0156, -0.3730, -0.4941,  ...,  0.6953,  0.8750, -0.1436]],           │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [ 0.9258, -0.5039, -1.3203,  ...,  2.1562, -1.1953, -1.7656],            │                                                                                   │
│ │                  │   │     [-1.4844, -0.7266,  0.7148,  ...,  0.0713, -0.6406, -0.0112],            │                                                                                   │
│ │                  │   │     [-1.0156, -0.3730, -0.4941,  ...,  0.6953,  0.8750, -0.1436]],           │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    ...,                                                                      │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [ 1.1094, -0.0194, -0.5195,  ...,  0.1621,  1.2109,  0.3828],            │                                                                                   │
│ │                  │   │     [-1.2500, -0.4785,  0.1348,  ...,  0.1338, -0.7109,  0.2158],            │                                                                                   │
│ │                  │   │     [ 0.8867,  0.3086,  0.8984,  ...,  0.4414, -0.4062, -0.3301]],           │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [ 1.1094, -0.0194, -0.5195,  ...,  0.1621,  1.2109,  0.3828],            │                                                                                   │
│ │                  │   │     [-1.2500, -0.4785,  0.1348,  ...,  0.1338, -0.7109,  0.2158],            │                                                                                   │
│ │                  │   │     [ 0.8867,  0.3086,  0.8984,  ...,  0.4414, -0.4062, -0.3301]],           │                                                                                   │
│ │                  │   │                                                                              │                                                                                   │
│ │                  │   │    [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],            │                                                                                   │
│ │                  │   │     ...,                                                                     │                                                                                   │
│ │                  │   │     [ 1.1094, -0.0194, -0.5195,  ...,  0.1621,  1.2109,  0.3828],            │                                                                                   │
│ │                  │   │     [-1.2500, -0.4785,  0.1348,  ...,  0.1338, -0.7109,  0.2158],            │                                                                                   │
│ │                  │   │     [ 0.8867,  0.3086,  0.8984,  ...,  0.4414, -0.4062, -0.3301]]]],         │                                                                                   │
│ │                  │      grad_fn=<UnsafeViewBackward0>)                                              │                                                                                   │
│ ╰─────────────────────────────────────────────────────────────────────────────────────────────────────╯                                                                                   │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
RuntimeError: expected scalar type Float but found BFloat16